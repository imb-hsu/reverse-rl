Logging to ./Logging/PPO_Forward_Baseline_10_300
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.7      |
| time/              |          |
|    fps             | 84       |
|    iterations      | 1        |
|    time_elapsed    | 24       |
|    total_timesteps | 2048     |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | 0            |
| time/                   |              |
|    total_timesteps      | 3000         |
| train/                  |              |
|    approx_kl            | 0.0145066045 |
|    clip_fraction        | 0.125        |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.54        |
|    explained_variance   | 0.864        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0448      |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.0231      |
|    value_loss           | 0.00896      |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.675    |
| time/              |          |
|    fps             | 82       |
|    iterations      | 2        |
|    time_elapsed    | 49       |
|    total_timesteps | 4096     |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.014497065 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.53       |
|    explained_variance   | 0.646       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0331     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0222     |
|    value_loss           | 0.00415     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.717    |
| time/              |          |
|    fps             | 82       |
|    iterations      | 3        |
|    time_elapsed    | 74       |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 7000        |
| train/                  |             |
|    approx_kl            | 0.015417937 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.52       |
|    explained_variance   | 0.73        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0327     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0252     |
|    value_loss           | 0.00377     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 968      |
|    ep_rew_mean     | 0.738    |
| time/              |          |
|    fps             | 81       |
|    iterations      | 4        |
|    time_elapsed    | 100      |
|    total_timesteps | 8192     |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 9000        |
| train/                  |             |
|    approx_kl            | 0.017995372 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.5        |
|    explained_variance   | 0.852       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0267     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.035      |
|    value_loss           | 0.00569     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.75     |
| time/              |          |
|    fps             | 81       |
|    iterations      | 5        |
|    time_elapsed    | 125      |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 11000       |
| train/                  |             |
|    approx_kl            | 0.016680604 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.49       |
|    explained_variance   | 0.671       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0332     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0282     |
|    value_loss           | 0.00355     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 998      |
|    ep_rew_mean     | 0.758    |
| time/              |          |
|    fps             | 81       |
|    iterations      | 6        |
|    time_elapsed    | 150      |
|    total_timesteps | 12288    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 13000       |
| train/                  |             |
|    approx_kl            | 0.016688928 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.47       |
|    explained_variance   | 0.858       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0148     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0268     |
|    value_loss           | 0.00211     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.8      |
| time/              |          |
|    fps             | 81       |
|    iterations      | 7        |
|    time_elapsed    | 175      |
|    total_timesteps | 14336    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.016850453 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.44       |
|    explained_variance   | 0.729       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0408     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0286     |
|    value_loss           | 0.00366     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.794    |
| time/              |          |
|    fps             | 81       |
|    iterations      | 8        |
|    time_elapsed    | 201      |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 17000       |
| train/                  |             |
|    approx_kl            | 0.020180551 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.44       |
|    explained_variance   | 0.868       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0116     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0349     |
|    value_loss           | 0.00215     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 982      |
|    ep_rew_mean     | 0.789    |
| time/              |          |
|    fps             | 81       |
|    iterations      | 9        |
|    time_elapsed    | 226      |
|    total_timesteps | 18432    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 19000      |
| train/                  |            |
|    approx_kl            | 0.01743522 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.43      |
|    explained_variance   | 0.902      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0319    |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0311    |
|    value_loss           | 0.00221    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 984      |
|    ep_rew_mean     | 0.81     |
| time/              |          |
|    fps             | 81       |
|    iterations      | 10       |
|    time_elapsed    | 252      |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 21000       |
| train/                  |             |
|    approx_kl            | 0.019837495 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.43       |
|    explained_variance   | 0.643       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0652     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0315     |
|    value_loss           | 0.00168     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 991      |
|    ep_rew_mean     | 0.827    |
| time/              |          |
|    fps             | 81       |
|    iterations      | 11       |
|    time_elapsed    | 277      |
|    total_timesteps | 22528    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 23000      |
| train/                  |            |
|    approx_kl            | 0.01884795 |
|    clip_fraction        | 0.176      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.36      |
|    explained_variance   | 0.92       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0239    |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0232    |
|    value_loss           | 0.00232    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.817    |
| time/              |          |
|    fps             | 81       |
|    iterations      | 12       |
|    time_elapsed    | 302      |
|    total_timesteps | 24576    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.013657356 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.39       |
|    explained_variance   | 0.645       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0177     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0229     |
|    value_loss           | 0.00181     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.808    |
| time/              |          |
|    fps             | 81       |
|    iterations      | 13       |
|    time_elapsed    | 328      |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 27000       |
| train/                  |             |
|    approx_kl            | 0.019999627 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.39       |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0455     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0271     |
|    value_loss           | 0.00121     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.804    |
| time/              |          |
|    fps             | 81       |
|    iterations      | 14       |
|    time_elapsed    | 353      |
|    total_timesteps | 28672    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 29000       |
| train/                  |             |
|    approx_kl            | 0.014960035 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.37       |
|    explained_variance   | 0.802       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0514     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0278     |
|    value_loss           | 0.000717    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.79     |
| time/              |          |
|    fps             | 81       |
|    iterations      | 15       |
|    time_elapsed    | 378      |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 31000       |
| train/                  |             |
|    approx_kl            | 0.016240487 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.37       |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0837     |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.027      |
|    value_loss           | 0.000525    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.781    |
| time/              |          |
|    fps             | 81       |
|    iterations      | 16       |
|    time_elapsed    | 403      |
|    total_timesteps | 32768    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 33000       |
| train/                  |             |
|    approx_kl            | 0.016393375 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.32       |
|    explained_variance   | 0.848       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0259     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0262     |
|    value_loss           | 0.00102     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.776    |
| time/              |          |
|    fps             | 81       |
|    iterations      | 17       |
|    time_elapsed    | 429      |
|    total_timesteps | 34816    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.013529874 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.35       |
|    explained_variance   | 0.86        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0209     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0239     |
|    value_loss           | 0.000585    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.775    |
| time/              |          |
|    fps             | 81       |
|    iterations      | 18       |
|    time_elapsed    | 454      |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 37000       |
| train/                  |             |
|    approx_kl            | 0.013590911 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.34       |
|    explained_variance   | 0.804       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0239     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0263     |
|    value_loss           | 0.00167     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.766    |
| time/              |          |
|    fps             | 81       |
|    iterations      | 19       |
|    time_elapsed    | 479      |
|    total_timesteps | 38912    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 39000      |
| train/                  |            |
|    approx_kl            | 0.01719841 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.32      |
|    explained_variance   | 0.75       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0106    |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.0265    |
|    value_loss           | 0.00194    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.76     |
| time/              |          |
|    fps             | 81       |
|    iterations      | 20       |
|    time_elapsed    | 505      |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 41000      |
| train/                  |            |
|    approx_kl            | 0.01589159 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.28      |
|    explained_variance   | 0.806      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0345    |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0231    |
|    value_loss           | 0.0013     |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.753    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 21       |
|    time_elapsed    | 540      |
|    total_timesteps | 43008    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 44000       |
| train/                  |             |
|    approx_kl            | 0.018671976 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.32       |
|    explained_variance   | 0.751       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0618     |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0277     |
|    value_loss           | 0.00194     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.744    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 22       |
|    time_elapsed    | 565      |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 46000       |
| train/                  |             |
|    approx_kl            | 0.018098446 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.31       |
|    explained_variance   | 0.892       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0429     |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0236     |
|    value_loss           | 0.000354    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.743    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 23       |
|    time_elapsed    | 591      |
|    total_timesteps | 47104    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 48000       |
| train/                  |             |
|    approx_kl            | 0.015638176 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.25       |
|    explained_variance   | 0.899       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0361     |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0303     |
|    value_loss           | 0.000844    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.745    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 24       |
|    time_elapsed    | 616      |
|    total_timesteps | 49152    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.016739171 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.27       |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0314     |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0278     |
|    value_loss           | 0.000695    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 988      |
|    ep_rew_mean     | 0.749    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 25       |
|    time_elapsed    | 641      |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 52000       |
| train/                  |             |
|    approx_kl            | 0.018519249 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.21       |
|    explained_variance   | 0.8         |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0524     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0346     |
|    value_loss           | 0.00142     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.757    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 26       |
|    time_elapsed    | 667      |
|    total_timesteps | 53248    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 54000       |
| train/                  |             |
|    approx_kl            | 0.014880909 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.11       |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0248     |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0248     |
|    value_loss           | 0.000329    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.755    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 27       |
|    time_elapsed    | 692      |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 56000       |
| train/                  |             |
|    approx_kl            | 0.019984009 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.25       |
|    explained_variance   | 0.675       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0533     |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0259     |
|    value_loss           | 0.000907    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.749    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 28       |
|    time_elapsed    | 717      |
|    total_timesteps | 57344    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 58000       |
| train/                  |             |
|    approx_kl            | 0.024747685 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.15       |
|    explained_variance   | 0.911       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0181     |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0305     |
|    value_loss           | 0.000271    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 998      |
|    ep_rew_mean     | 0.751    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 29       |
|    time_elapsed    | 743      |
|    total_timesteps | 59392    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.018782351 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.19       |
|    explained_variance   | 0.883       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0245     |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0258     |
|    value_loss           | 0.000992    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.749    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 30       |
|    time_elapsed    | 768      |
|    total_timesteps | 61440    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 62000       |
| train/                  |             |
|    approx_kl            | 0.015351433 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.21       |
|    explained_variance   | 0.768       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0339     |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.0193     |
|    value_loss           | 0.00254     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.748    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 31       |
|    time_elapsed    | 793      |
|    total_timesteps | 63488    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 64000       |
| train/                  |             |
|    approx_kl            | 0.016066393 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.26       |
|    explained_variance   | 0.674       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0443     |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.0227     |
|    value_loss           | 0.00172     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.745    |
| time/              |          |
|    fps             | 80       |
|    iterations      | 32       |
|    time_elapsed    | 818      |
|    total_timesteps | 65536    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 66000       |
| train/                  |             |
|    approx_kl            | 0.014108503 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.1        |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0416     |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0254     |
|    value_loss           | 0.000321    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.743    |
| time/              |          |
|    fps             | 80       |
|    iterations      | 33       |
|    time_elapsed    | 844      |
|    total_timesteps | 67584    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 68000      |
| train/                  |            |
|    approx_kl            | 0.01584064 |
|    clip_fraction        | 0.205      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.18      |
|    explained_variance   | 0.673      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0649    |
|    n_updates            | 330        |
|    policy_gradient_loss | -0.0255    |
|    value_loss           | 0.000427   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.742    |
| time/              |          |
|    fps             | 80       |
|    iterations      | 34       |
|    time_elapsed    | 869      |
|    total_timesteps | 69632    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.017463712 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.12       |
|    explained_variance   | 0.893       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0154     |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.0209     |
|    value_loss           | 0.000356    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.741    |
| time/              |          |
|    fps             | 80       |
|    iterations      | 35       |
|    time_elapsed    | 895      |
|    total_timesteps | 71680    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 72000       |
| train/                  |             |
|    approx_kl            | 0.016378399 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.18       |
|    explained_variance   | 0.872       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0699     |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.0274     |
|    value_loss           | 0.000261    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.738    |
| time/              |          |
|    fps             | 80       |
|    iterations      | 36       |
|    time_elapsed    | 920      |
|    total_timesteps | 73728    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 74000       |
| train/                  |             |
|    approx_kl            | 0.015321538 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.1        |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0187     |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.0187     |
|    value_loss           | 0.00018     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.739    |
| time/              |          |
|    fps             | 80       |
|    iterations      | 37       |
|    time_elapsed    | 945      |
|    total_timesteps | 75776    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 76000       |
| train/                  |             |
|    approx_kl            | 0.018738626 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.12       |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0312     |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.0265     |
|    value_loss           | 0.000395    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.738    |
| time/              |          |
|    fps             | 80       |
|    iterations      | 38       |
|    time_elapsed    | 971      |
|    total_timesteps | 77824    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 78000      |
| train/                  |            |
|    approx_kl            | 0.01933585 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.08      |
|    explained_variance   | 0.786      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0416    |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.0246    |
|    value_loss           | 0.000474   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.739    |
| time/              |          |
|    fps             | 80       |
|    iterations      | 39       |
|    time_elapsed    | 996      |
|    total_timesteps | 79872    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.016968276 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.08       |
|    explained_variance   | 0.75        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0102     |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.025      |
|    value_loss           | 0.000669    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.738    |
| time/              |          |
|    fps             | 80       |
|    iterations      | 40       |
|    time_elapsed    | 1021     |
|    total_timesteps | 81920    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 82000       |
| train/                  |             |
|    approx_kl            | 0.016242297 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.05       |
|    explained_variance   | 0.809       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0312     |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.0258     |
|    value_loss           | 0.000637    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.74     |
| time/              |          |
|    fps             | 80       |
|    iterations      | 41       |
|    time_elapsed    | 1047     |
|    total_timesteps | 83968    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 84000       |
| train/                  |             |
|    approx_kl            | 0.016230432 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.99       |
|    explained_variance   | 0.805       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.033      |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.0252     |
|    value_loss           | 0.000502    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.74     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 42       |
|    time_elapsed    | 1082     |
|    total_timesteps | 86016    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 87000       |
| train/                  |             |
|    approx_kl            | 0.014071572 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.08       |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0522     |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0221     |
|    value_loss           | 0.000518    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 999      |
|    ep_rew_mean     | 0.741    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 43       |
|    time_elapsed    | 1107     |
|    total_timesteps | 88064    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 89000       |
| train/                  |             |
|    approx_kl            | 0.014696719 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.01       |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00726    |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.0243     |
|    value_loss           | 0.00032     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.756    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 44       |
|    time_elapsed    | 1132     |
|    total_timesteps | 90112    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 91000       |
| train/                  |             |
|    approx_kl            | 0.013993654 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.01       |
|    explained_variance   | 0.891       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0317     |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.0211     |
|    value_loss           | 0.000673    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 996      |
|    ep_rew_mean     | 0.758    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 45       |
|    time_elapsed    | 1158     |
|    total_timesteps | 92160    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 93000       |
| train/                  |             |
|    approx_kl            | 0.017426625 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.02       |
|    explained_variance   | 0.894       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0479     |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.0263     |
|    value_loss           | 0.000602    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.762    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 46       |
|    time_elapsed    | 1183     |
|    total_timesteps | 94208    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 95000       |
| train/                  |             |
|    approx_kl            | 0.014177873 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.97       |
|    explained_variance   | 0.894       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0293     |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.0183     |
|    value_loss           | 0.000244    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 993      |
|    ep_rew_mean     | 0.764    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 47       |
|    time_elapsed    | 1208     |
|    total_timesteps | 96256    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 97000       |
| train/                  |             |
|    approx_kl            | 0.015106982 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.93       |
|    explained_variance   | 0.894       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0101     |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.0218     |
|    value_loss           | 0.000486    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.768    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 48       |
|    time_elapsed    | 1234     |
|    total_timesteps | 98304    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 99000       |
| train/                  |             |
|    approx_kl            | 0.022798523 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.97       |
|    explained_variance   | 0.861       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0331     |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.0276     |
|    value_loss           | 0.000479    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 999      |
|    ep_rew_mean     | 0.77     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 49       |
|    time_elapsed    | 1259     |
|    total_timesteps | 100352   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 101000      |
| train/                  |             |
|    approx_kl            | 0.015204431 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.96       |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0165     |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.0197     |
|    value_loss           | 0.000427    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 997      |
|    ep_rew_mean     | 0.775    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 50       |
|    time_elapsed    | 1284     |
|    total_timesteps | 102400   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 103000      |
| train/                  |             |
|    approx_kl            | 0.017418046 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.89       |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0174     |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.0271     |
|    value_loss           | 0.000524    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.781    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 51       |
|    time_elapsed    | 1310     |
|    total_timesteps | 104448   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 105000      |
| train/                  |             |
|    approx_kl            | 0.019916553 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.04       |
|    explained_variance   | 0.911       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0324     |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.0255     |
|    value_loss           | 0.000375    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.781    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 52       |
|    time_elapsed    | 1335     |
|    total_timesteps | 106496   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 107000      |
| train/                  |             |
|    approx_kl            | 0.020552173 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.93       |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0334     |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.0205     |
|    value_loss           | 0.000212    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 998      |
|    ep_rew_mean     | 0.788    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 53       |
|    time_elapsed    | 1360     |
|    total_timesteps | 108544   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 109000      |
| train/                  |             |
|    approx_kl            | 0.017018944 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.89       |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0512     |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.0191     |
|    value_loss           | 0.00145     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 996      |
|    ep_rew_mean     | 0.81     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 54       |
|    time_elapsed    | 1386     |
|    total_timesteps | 110592   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 111000      |
| train/                  |             |
|    approx_kl            | 0.016261462 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.88       |
|    explained_variance   | 0.796       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00505    |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.0205     |
|    value_loss           | 0.00145     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 992      |
|    ep_rew_mean     | 0.81     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 55       |
|    time_elapsed    | 1411     |
|    total_timesteps | 112640   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 113000      |
| train/                  |             |
|    approx_kl            | 0.015857395 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.82       |
|    explained_variance   | 0.899       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0218     |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.0238     |
|    value_loss           | 0.000384    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 983      |
|    ep_rew_mean     | 0.807    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 56       |
|    time_elapsed    | 1436     |
|    total_timesteps | 114688   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 115000      |
| train/                  |             |
|    approx_kl            | 0.016180329 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.77       |
|    explained_variance   | 0.736       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.047      |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.000617    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 987      |
|    ep_rew_mean     | 0.817    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 57       |
|    time_elapsed    | 1461     |
|    total_timesteps | 116736   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 117000      |
| train/                  |             |
|    approx_kl            | 0.014730681 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.91       |
|    explained_variance   | 0.663       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0386     |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 0.00127     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 989      |
|    ep_rew_mean     | 0.827    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 58       |
|    time_elapsed    | 1487     |
|    total_timesteps | 118784   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 119000      |
| train/                  |             |
|    approx_kl            | 0.014868916 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.84       |
|    explained_variance   | 0.691       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0197     |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.0203     |
|    value_loss           | 0.00111     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 990      |
|    ep_rew_mean     | 0.828    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 59       |
|    time_elapsed    | 1512     |
|    total_timesteps | 120832   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 121000      |
| train/                  |             |
|    approx_kl            | 0.017720273 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.86       |
|    explained_variance   | 0.803       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0246     |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.0193     |
|    value_loss           | 0.000753    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 977      |
|    ep_rew_mean     | 0.83     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 60       |
|    time_elapsed    | 1537     |
|    total_timesteps | 122880   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 123000      |
| train/                  |             |
|    approx_kl            | 0.014928633 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.68       |
|    explained_variance   | 0.721       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0138     |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.0202     |
|    value_loss           | 0.00117     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 974      |
|    ep_rew_mean     | 0.855    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 61       |
|    time_elapsed    | 1563     |
|    total_timesteps | 124928   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 125000      |
| train/                  |             |
|    approx_kl            | 0.019592572 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.67       |
|    explained_variance   | 0.807       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0413     |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.0219     |
|    value_loss           | 0.00173     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 969      |
|    ep_rew_mean     | 0.875    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 62       |
|    time_elapsed    | 1588     |
|    total_timesteps | 126976   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 127000      |
| train/                  |             |
|    approx_kl            | 0.016101668 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.68       |
|    explained_variance   | 0.733       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0354     |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.0222     |
|    value_loss           | 0.00142     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 967      |
|    ep_rew_mean     | 0.894    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 63       |
|    time_elapsed    | 1623     |
|    total_timesteps | 129024   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 130000      |
| train/                  |             |
|    approx_kl            | 0.019869667 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.64       |
|    explained_variance   | 0.594       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0266     |
|    n_updates            | 630         |
|    policy_gradient_loss | -0.0213     |
|    value_loss           | 0.00197     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 958      |
|    ep_rew_mean     | 0.904    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 64       |
|    time_elapsed    | 1649     |
|    total_timesteps | 131072   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 132000      |
| train/                  |             |
|    approx_kl            | 0.019929836 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.63       |
|    explained_variance   | 0.723       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0447     |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.0258     |
|    value_loss           | 0.00114     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 947      |
|    ep_rew_mean     | 0.925    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 65       |
|    time_elapsed    | 1674     |
|    total_timesteps | 133120   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 134000      |
| train/                  |             |
|    approx_kl            | 0.015131448 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.64       |
|    explained_variance   | 0.675       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.012      |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.00167     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 936      |
|    ep_rew_mean     | 0.945    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 66       |
|    time_elapsed    | 1699     |
|    total_timesteps | 135168   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 136000      |
| train/                  |             |
|    approx_kl            | 0.017275026 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.63       |
|    explained_variance   | 0.695       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0361     |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.0247     |
|    value_loss           | 0.00205     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 926      |
|    ep_rew_mean     | 0.964    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 67       |
|    time_elapsed    | 1725     |
|    total_timesteps | 137216   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.3        |
| time/                   |            |
|    total_timesteps      | 138000     |
| train/                  |            |
|    approx_kl            | 0.01730697 |
|    clip_fraction        | 0.2        |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.45      |
|    explained_variance   | 0.775      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0413    |
|    n_updates            | 670        |
|    policy_gradient_loss | -0.0246    |
|    value_loss           | 0.00102    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 892      |
|    ep_rew_mean     | 0.988    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 68       |
|    time_elapsed    | 1750     |
|    total_timesteps | 139264   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 140000      |
| train/                  |             |
|    approx_kl            | 0.018250342 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.39       |
|    explained_variance   | 0.846       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0452     |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.0226     |
|    value_loss           | 0.00164     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 863      |
|    ep_rew_mean     | 0.993    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 69       |
|    time_elapsed    | 1775     |
|    total_timesteps | 141312   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 142000      |
| train/                  |             |
|    approx_kl            | 0.015901547 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.47       |
|    explained_variance   | 0.783       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00388    |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.00169     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 848      |
|    ep_rew_mean     | 1.02     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 70       |
|    time_elapsed    | 1801     |
|    total_timesteps | 143360   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 144000      |
| train/                  |             |
|    approx_kl            | 0.016513597 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.49       |
|    explained_variance   | 0.582       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00764    |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.0185     |
|    value_loss           | 0.00243     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 818      |
|    ep_rew_mean     | 1.04     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 71       |
|    time_elapsed    | 1826     |
|    total_timesteps | 145408   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 146000      |
| train/                  |             |
|    approx_kl            | 0.017827243 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.42       |
|    explained_variance   | 0.642       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.041      |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.0227     |
|    value_loss           | 0.002       |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 802      |
|    ep_rew_mean     | 1.07     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 72       |
|    time_elapsed    | 1851     |
|    total_timesteps | 147456   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 148000      |
| train/                  |             |
|    approx_kl            | 0.014182635 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.48       |
|    explained_variance   | 0.675       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.03       |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 0.00183     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 775      |
|    ep_rew_mean     | 1.09     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 73       |
|    time_elapsed    | 1877     |
|    total_timesteps | 149504   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.021596052 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.26       |
|    explained_variance   | 0.712       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0254     |
|    n_updates            | 730         |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.00196     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 151000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 742      |
|    ep_rew_mean     | 1.11     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 74       |
|    time_elapsed    | 1902     |
|    total_timesteps | 151552   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 152000      |
| train/                  |             |
|    approx_kl            | 0.014607659 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.4        |
|    explained_variance   | 0.664       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0452     |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.00234     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 153000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 665      |
|    ep_rew_mean     | 1.14     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 75       |
|    time_elapsed    | 1927     |
|    total_timesteps | 153600   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 154000      |
| train/                  |             |
|    approx_kl            | 0.015603867 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.21       |
|    explained_variance   | 0.684       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0403     |
|    n_updates            | 750         |
|    policy_gradient_loss | -0.0205     |
|    value_loss           | 0.00306     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 601      |
|    ep_rew_mean     | 1.15     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 76       |
|    time_elapsed    | 1953     |
|    total_timesteps | 155648   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 156000      |
| train/                  |             |
|    approx_kl            | 0.015565542 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.19       |
|    explained_variance   | 0.711       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000207   |
|    n_updates            | 760         |
|    policy_gradient_loss | -0.0154     |
|    value_loss           | 0.00201     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 510      |
|    ep_rew_mean     | 1.16     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 77       |
|    time_elapsed    | 1978     |
|    total_timesteps | 157696   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 158000      |
| train/                  |             |
|    approx_kl            | 0.017645283 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.02       |
|    explained_variance   | 0.761       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0474     |
|    n_updates            | 770         |
|    policy_gradient_loss | -0.0218     |
|    value_loss           | 0.00267     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 159000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 397      |
|    ep_rew_mean     | 1.13     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 78       |
|    time_elapsed    | 2003     |
|    total_timesteps | 159744   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 160000      |
| train/                  |             |
|    approx_kl            | 0.021139909 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.9        |
|    explained_variance   | 0.733       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0282     |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.0249     |
|    value_loss           | 0.00333     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 161000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 1.1      |
| time/              |          |
|    fps             | 79       |
|    iterations      | 79       |
|    time_elapsed    | 2029     |
|    total_timesteps | 161792   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 162000      |
| train/                  |             |
|    approx_kl            | 0.015152764 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.94       |
|    explained_variance   | 0.84        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0319     |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.0189     |
|    value_loss           | 0.00209     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 163000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 246      |
|    ep_rew_mean     | 1.06     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 80       |
|    time_elapsed    | 2054     |
|    total_timesteps | 163840   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 164000      |
| train/                  |             |
|    approx_kl            | 0.015763318 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.91       |
|    explained_variance   | 0.728       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0239     |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.019      |
|    value_loss           | 0.00363     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 165000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 204      |
|    ep_rew_mean     | 1.04     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 81       |
|    time_elapsed    | 2079     |
|    total_timesteps | 165888   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 166000      |
| train/                  |             |
|    approx_kl            | 0.019630827 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.86       |
|    explained_variance   | 0.81        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00502    |
|    n_updates            | 810         |
|    policy_gradient_loss | -0.0187     |
|    value_loss           | 0.00202     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 167000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 147      |
|    ep_rew_mean     | 0.986    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 82       |
|    time_elapsed    | 2104     |
|    total_timesteps | 167936   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 168000      |
| train/                  |             |
|    approx_kl            | 0.023592701 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0468     |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.0278     |
|    value_loss           | 0.000682    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 169000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | 0.964    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 83       |
|    time_elapsed    | 2130     |
|    total_timesteps | 169984   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 170000      |
| train/                  |             |
|    approx_kl            | 0.015172035 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.52       |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00663    |
|    n_updates            | 830         |
|    policy_gradient_loss | -0.0154     |
|    value_loss           | 0.00288     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.1     |
|    ep_rew_mean     | 0.946    |
| time/              |          |
|    fps             | 80       |
|    iterations      | 84       |
|    time_elapsed    | 2135     |
|    total_timesteps | 172032   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 173000      |
| train/                  |             |
|    approx_kl            | 0.020954926 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0112     |
|    n_updates            | 840         |
|    policy_gradient_loss | -0.0202     |
|    value_loss           | 0.00246     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.7     |
|    ep_rew_mean     | 0.938    |
| time/              |          |
|    fps             | 81       |
|    iterations      | 85       |
|    time_elapsed    | 2141     |
|    total_timesteps | 174080   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 175000      |
| train/                  |             |
|    approx_kl            | 0.019347735 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00976    |
|    n_updates            | 850         |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.00262     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.7     |
|    ep_rew_mean     | 0.936    |
| time/              |          |
|    fps             | 82       |
|    iterations      | 86       |
|    time_elapsed    | 2147     |
|    total_timesteps | 176128   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 9          |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 177000     |
| train/                  |            |
|    approx_kl            | 0.01651559 |
|    clip_fraction        | 0.192      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.95      |
|    explained_variance   | 0.929      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0119    |
|    n_updates            | 860        |
|    policy_gradient_loss | -0.0181    |
|    value_loss           | 0.00244    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.8     |
|    ep_rew_mean     | 0.932    |
| time/              |          |
|    fps             | 82       |
|    iterations      | 87       |
|    time_elapsed    | 2152     |
|    total_timesteps | 178176   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 179000      |
| train/                  |             |
|    approx_kl            | 0.015587157 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.82       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0287     |
|    n_updates            | 870         |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 0.00234     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 0.919    |
| time/              |          |
|    fps             | 83       |
|    iterations      | 88       |
|    time_elapsed    | 2158     |
|    total_timesteps | 180224   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 181000      |
| train/                  |             |
|    approx_kl            | 0.018289167 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.49       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000408    |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.0285     |
|    value_loss           | 0.00107     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.8     |
|    ep_rew_mean     | 0.915    |
| time/              |          |
|    fps             | 84       |
|    iterations      | 89       |
|    time_elapsed    | 2164     |
|    total_timesteps | 182272   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 183000      |
| train/                  |             |
|    approx_kl            | 0.017049406 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.38       |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0329     |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 0.00325     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.8     |
|    ep_rew_mean     | 0.919    |
| time/              |          |
|    fps             | 84       |
|    iterations      | 90       |
|    time_elapsed    | 2169     |
|    total_timesteps | 184320   |
---------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 9         |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 185000    |
| train/                  |           |
|    approx_kl            | 0.0219648 |
|    clip_fraction        | 0.243     |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.12     |
|    explained_variance   | 0.948     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0509   |
|    n_updates            | 900       |
|    policy_gradient_loss | -0.0198   |
|    value_loss           | 0.00252   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.7     |
|    ep_rew_mean     | 0.923    |
| time/              |          |
|    fps             | 85       |
|    iterations      | 91       |
|    time_elapsed    | 2175     |
|    total_timesteps | 186368   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 187000      |
| train/                  |             |
|    approx_kl            | 0.021225568 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.26       |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00504    |
|    n_updates            | 910         |
|    policy_gradient_loss | -0.0139     |
|    value_loss           | 0.00313     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.7     |
|    ep_rew_mean     | 0.922    |
| time/              |          |
|    fps             | 86       |
|    iterations      | 92       |
|    time_elapsed    | 2181     |
|    total_timesteps | 188416   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 189000      |
| train/                  |             |
|    approx_kl            | 0.019295864 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.02       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0237     |
|    n_updates            | 920         |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 0.00272     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.1     |
|    ep_rew_mean     | 0.914    |
| time/              |          |
|    fps             | 87       |
|    iterations      | 93       |
|    time_elapsed    | 2186     |
|    total_timesteps | 190464   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 191000      |
| train/                  |             |
|    approx_kl            | 0.020508636 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.57       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0165     |
|    n_updates            | 930         |
|    policy_gradient_loss | -0.0184     |
|    value_loss           | 0.00297     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.6     |
|    ep_rew_mean     | 0.915    |
| time/              |          |
|    fps             | 87       |
|    iterations      | 94       |
|    time_elapsed    | 2192     |
|    total_timesteps | 192512   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 193000      |
| train/                  |             |
|    approx_kl            | 0.021724809 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.27       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0469     |
|    n_updates            | 940         |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 0.00344     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | 0.911    |
| time/              |          |
|    fps             | 88       |
|    iterations      | 95       |
|    time_elapsed    | 2198     |
|    total_timesteps | 194560   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 195000      |
| train/                  |             |
|    approx_kl            | 0.026122797 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.88       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0568     |
|    n_updates            | 950         |
|    policy_gradient_loss | -0.0278     |
|    value_loss           | 0.00128     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.7     |
|    ep_rew_mean     | 0.915    |
| time/              |          |
|    fps             | 89       |
|    iterations      | 96       |
|    time_elapsed    | 2203     |
|    total_timesteps | 196608   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 197000      |
| train/                  |             |
|    approx_kl            | 0.017034458 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.22       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0302     |
|    n_updates            | 960         |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 0.00285     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 13.5     |
|    ep_rew_mean     | 0.906    |
| time/              |          |
|    fps             | 89       |
|    iterations      | 97       |
|    time_elapsed    | 2209     |
|    total_timesteps | 198656   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 199000      |
| train/                  |             |
|    approx_kl            | 0.052789286 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.54       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0166     |
|    n_updates            | 970         |
|    policy_gradient_loss | -0.0212     |
|    value_loss           | 0.00177     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.1     |
|    ep_rew_mean     | 0.904    |
| time/              |          |
|    fps             | 90       |
|    iterations      | 98       |
|    time_elapsed    | 2215     |
|    total_timesteps | 200704   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 201000      |
| train/                  |             |
|    approx_kl            | 0.015975554 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.41       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0502     |
|    n_updates            | 980         |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.00209     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 11.7     |
|    ep_rew_mean     | 0.905    |
| time/              |          |
|    fps             | 91       |
|    iterations      | 99       |
|    time_elapsed    | 2221     |
|    total_timesteps | 202752   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 203000      |
| train/                  |             |
|    approx_kl            | 0.022546314 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.924      |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0356     |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.00168     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 11       |
|    ep_rew_mean     | 0.907    |
| time/              |          |
|    fps             | 91       |
|    iterations      | 100      |
|    time_elapsed    | 2227     |
|    total_timesteps | 204800   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 205000      |
| train/                  |             |
|    approx_kl            | 0.032111794 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.688      |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00208    |
|    n_updates            | 1000        |
|    policy_gradient_loss | -0.023      |
|    value_loss           | 0.00161     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20       |
|    ep_rew_mean     | 0.9      |
| time/              |          |
|    fps             | 92       |
|    iterations      | 101      |
|    time_elapsed    | 2232     |
|    total_timesteps | 206848   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 207000      |
| train/                  |             |
|    approx_kl            | 0.012021258 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.26       |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0189     |
|    n_updates            | 1010        |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 0.00016     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 10.4     |
|    ep_rew_mean     | 0.908    |
| time/              |          |
|    fps             | 93       |
|    iterations      | 102      |
|    time_elapsed    | 2238     |
|    total_timesteps | 208896   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 209000      |
| train/                  |             |
|    approx_kl            | 0.018475154 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.51       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0559     |
|    n_updates            | 1020        |
|    policy_gradient_loss | -0.019      |
|    value_loss           | 0.00192     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 10.1     |
|    ep_rew_mean     | 0.908    |
| time/              |          |
|    fps             | 93       |
|    iterations      | 103      |
|    time_elapsed    | 2244     |
|    total_timesteps | 210944   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 9          |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 211000     |
| train/                  |            |
|    approx_kl            | 0.02716925 |
|    clip_fraction        | 0.0907     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.802     |
|    explained_variance   | 0.972      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0275    |
|    n_updates            | 1030       |
|    policy_gradient_loss | -0.0174    |
|    value_loss           | 0.00191    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 10.4     |
|    ep_rew_mean     | 0.907    |
| time/              |          |
|    fps             | 94       |
|    iterations      | 104      |
|    time_elapsed    | 2250     |
|    total_timesteps | 212992   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 213000      |
| train/                  |             |
|    approx_kl            | 0.017410189 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.815      |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0214     |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.00128     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.67     |
|    ep_rew_mean     | 0.9      |
| time/              |          |
|    fps             | 95       |
|    iterations      | 105      |
|    time_elapsed    | 2256     |
|    total_timesteps | 215040   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 216000      |
| train/                  |             |
|    approx_kl            | 0.044942062 |
|    clip_fraction        | 0.0946      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.462      |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0433     |
|    n_updates            | 1050        |
|    policy_gradient_loss | -0.0196     |
|    value_loss           | 0.000115    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.69     |
|    ep_rew_mean     | 0.902    |
| time/              |          |
|    fps             | 95       |
|    iterations      | 106      |
|    time_elapsed    | 2262     |
|    total_timesteps | 217088   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 9          |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 218000     |
| train/                  |            |
|    approx_kl            | 0.06999803 |
|    clip_fraction        | 0.299      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.05      |
|    explained_variance   | 0.996      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.109     |
|    n_updates            | 1060       |
|    policy_gradient_loss | -0.0318    |
|    value_loss           | 0.000174   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.55     |
|    ep_rew_mean     | 0.9      |
| time/              |          |
|    fps             | 96       |
|    iterations      | 107      |
|    time_elapsed    | 2267     |
|    total_timesteps | 219136   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 220000      |
| train/                  |             |
|    approx_kl            | 0.015553225 |
|    clip_fraction        | 0.0604      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.513      |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0104      |
|    n_updates            | 1070        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.000962    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.38     |
|    ep_rew_mean     | 0.9      |
| time/              |          |
|    fps             | 97       |
|    iterations      | 108      |
|    time_elapsed    | 2273     |
|    total_timesteps | 221184   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 222000      |
| train/                  |             |
|    approx_kl            | 0.034884214 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.02       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0699     |
|    n_updates            | 1080        |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 3.71e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.37     |
|    ep_rew_mean     | 0.902    |
| time/              |          |
|    fps             | 97       |
|    iterations      | 109      |
|    time_elapsed    | 2279     |
|    total_timesteps | 223232   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 224000      |
| train/                  |             |
|    approx_kl            | 0.003452737 |
|    clip_fraction        | 0.0272      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.225      |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00384     |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 0.000981    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.35     |
|    ep_rew_mean     | 0.907    |
| time/              |          |
|    fps             | 98       |
|    iterations      | 110      |
|    time_elapsed    | 2285     |
|    total_timesteps | 225280   |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 9            |
|    mean_reward          | 0.9          |
| time/                   |              |
|    total_timesteps      | 226000       |
| train/                  |              |
|    approx_kl            | 0.0066595944 |
|    clip_fraction        | 0.0372       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.222       |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0414      |
|    n_updates            | 1100         |
|    policy_gradient_loss | -0.0123      |
|    value_loss           | 0.00113      |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.36     |
|    ep_rew_mean     | 0.9      |
| time/              |          |
|    fps             | 99       |
|    iterations      | 111      |
|    time_elapsed    | 2291     |
|    total_timesteps | 227328   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 9          |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 228000     |
| train/                  |            |
|    approx_kl            | 0.04098034 |
|    clip_fraction        | 0.19       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.354     |
|    explained_variance   | 0.974      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.049     |
|    n_updates            | 1110       |
|    policy_gradient_loss | -0.0206    |
|    value_loss           | 0.00163    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.7      |
|    ep_rew_mean     | 0.9      |
| time/              |          |
|    fps             | 99       |
|    iterations      | 112      |
|    time_elapsed    | 2297     |
|    total_timesteps | 229376   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 230000      |
| train/                  |             |
|    approx_kl            | 0.028383605 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.274      |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0642     |
|    n_updates            | 1120        |
|    policy_gradient_loss | -0.0293     |
|    value_loss           | 0.00062     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.33     |
|    ep_rew_mean     | 0.901    |
| time/              |          |
|    fps             | 100      |
|    iterations      | 113      |
|    time_elapsed    | 2303     |
|    total_timesteps | 231424   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 9          |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 232000     |
| train/                  |            |
|    approx_kl            | 0.02122562 |
|    clip_fraction        | 0.0489     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.24      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0374    |
|    n_updates            | 1130       |
|    policy_gradient_loss | -0.0124    |
|    value_loss           | 0.000114   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.55     |
|    ep_rew_mean     | 0.9      |
| time/              |          |
|    fps             | 101      |
|    iterations      | 114      |
|    time_elapsed    | 2309     |
|    total_timesteps | 233472   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 234000      |
| train/                  |             |
|    approx_kl            | 0.057355724 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0224     |
|    n_updates            | 1140        |
|    policy_gradient_loss | -0.0235     |
|    value_loss           | 0.000677    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.33     |
|    ep_rew_mean     | 0.906    |
| time/              |          |
|    fps             | 101      |
|    iterations      | 115      |
|    time_elapsed    | 2315     |
|    total_timesteps | 235520   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 236000      |
| train/                  |             |
|    approx_kl            | 0.016773432 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0448     |
|    n_updates            | 1150        |
|    policy_gradient_loss | -0.00719    |
|    value_loss           | 0.00152     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 237000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.67     |
|    ep_rew_mean     | 0.901    |
| time/              |          |
|    fps             | 102      |
|    iterations      | 116      |
|    time_elapsed    | 2321     |
|    total_timesteps | 237568   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 238000      |
| train/                  |             |
|    approx_kl            | 0.022265775 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.181      |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0323     |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 6.61e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 239000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.82     |
|    ep_rew_mean     | 0.908    |
| time/              |          |
|    fps             | 102      |
|    iterations      | 117      |
|    time_elapsed    | 2326     |
|    total_timesteps | 239616   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 0.016415555 |
|    clip_fraction        | 0.0952      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.293      |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00342     |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.0137     |
|    value_loss           | 0.00351     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 241000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.68     |
|    ep_rew_mean     | 0.906    |
| time/              |          |
|    fps             | 103      |
|    iterations      | 118      |
|    time_elapsed    | 2332     |
|    total_timesteps | 241664   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 242000      |
| train/                  |             |
|    approx_kl            | 0.017670533 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.27       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0465     |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.0145     |
|    value_loss           | 0.00192     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 243000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 13.2     |
|    ep_rew_mean     | 0.905    |
| time/              |          |
|    fps             | 104      |
|    iterations      | 119      |
|    time_elapsed    | 2338     |
|    total_timesteps | 243712   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 244000      |
| train/                  |             |
|    approx_kl            | 0.020885453 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.922      |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0689     |
|    n_updates            | 1190        |
|    policy_gradient_loss | -0.00676    |
|    value_loss           | 0.00259     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.93     |
|    ep_rew_mean     | 0.906    |
| time/              |          |
|    fps             | 104      |
|    iterations      | 120      |
|    time_elapsed    | 2344     |
|    total_timesteps | 245760   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 246000      |
| train/                  |             |
|    approx_kl            | 0.008193323 |
|    clip_fraction        | 0.0837      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.432      |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0277     |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.0126     |
|    value_loss           | 0.00106     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 247000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.5     |
|    ep_rew_mean     | 0.902    |
| time/              |          |
|    fps             | 105      |
|    iterations      | 121      |
|    time_elapsed    | 2350     |
|    total_timesteps | 247808   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 248000      |
| train/                  |             |
|    approx_kl            | 0.018015549 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.08       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0203      |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 2.91e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.49     |
|    ep_rew_mean     | 0.9      |
| time/              |          |
|    fps             | 106      |
|    iterations      | 122      |
|    time_elapsed    | 2356     |
|    total_timesteps | 249856   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.032687593 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.52       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0207      |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 9.14e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.28     |
|    ep_rew_mean     | 0.9      |
| time/              |          |
|    fps             | 106      |
|    iterations      | 123      |
|    time_elapsed    | 2362     |
|    total_timesteps | 251904   |
---------------------------------
