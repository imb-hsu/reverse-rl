Logging to ../Logging/RPPO_BW_Then_FW_10_100
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 56.9                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 1                    |
|    time_elapsed    | -1683717608645157376 |
|    total_timesteps | 2048                 |
---------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 3000        |
| train/                  |             |
|    approx_kl            | 0.023111358 |
|    clip_fraction        | 0.29        |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.53       |
|    explained_variance   | -3.01       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0776     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0694     |
|    value_loss           | 0.0346      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 56                   |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 2                    |
|    time_elapsed    | -1683717608645157120 |
|    total_timesteps | 4096                 |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+03     |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 5000      |
| train/                  |           |
|    approx_kl            | 0.1201604 |
|    clip_fraction        | 0.518     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.45     |
|    explained_variance   | -0.082    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0396   |
|    n_updates            | 40        |
|    policy_gradient_loss | -0.0517   |
|    value_loss           | 0.00955   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 51.7                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 3                    |
|    time_elapsed    | -1683717608645157120 |
|    total_timesteps | 6144                 |
---------------------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 7000       |
| train/                  |            |
|    approx_kl            | 0.20598543 |
|    clip_fraction        | 0.617      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.43      |
|    explained_variance   | 0.492      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0745    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0264    |
|    value_loss           | 0.0057     |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 45.8                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 4                    |
|    time_elapsed    | -1683717608645156864 |
|    total_timesteps | 8192                 |
---------------------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 9000       |
| train/                  |            |
|    approx_kl            | 0.58640724 |
|    clip_fraction        | 0.741      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.34      |
|    explained_variance   | 0.496      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0384    |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0286    |
|    value_loss           | 0.00286    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 38.9                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 5                    |
|    time_elapsed    | -1683717608645156864 |
|    total_timesteps | 10240                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+03     |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 11000     |
| train/                  |           |
|    approx_kl            | 0.9972623 |
|    clip_fraction        | 0.845     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.18     |
|    explained_variance   | 0.549     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0397   |
|    n_updates            | 100       |
|    policy_gradient_loss | -0.0231   |
|    value_loss           | 0.0016    |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 33.5                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 6                    |
|    time_elapsed    | -1683717608645156608 |
|    total_timesteps | 12288                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 1e+03    |
|    mean_reward          | 0.1      |
| time/                   |          |
|    total_timesteps      | 13000    |
| train/                  |          |
|    approx_kl            | 2.291905 |
|    clip_fraction        | 0.901    |
|    clip_range           | 0.2      |
|    entropy_loss         | -6.03    |
|    explained_variance   | 0.412    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0149  |
|    n_updates            | 120      |
|    policy_gradient_loss | -0.031   |
|    value_loss           | 0.00249  |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 25.8                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 7                    |
|    time_elapsed    | -1683717608645156608 |
|    total_timesteps | 14336                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 15000     |
| train/                  |           |
|    approx_kl            | 3.8657813 |
|    clip_fraction        | 0.918     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.85     |
|    explained_variance   | 0.352     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.062    |
|    n_updates            | 140       |
|    policy_gradient_loss | -0.0348   |
|    value_loss           | 0.00169   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 23.3                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 8                    |
|    time_elapsed    | -1683717608645156608 |
|    total_timesteps | 16384                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 17000     |
| train/                  |           |
|    approx_kl            | 6.6204762 |
|    clip_fraction        | 0.931     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.51     |
|    explained_variance   | 0.369     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0333   |
|    n_updates            | 160       |
|    policy_gradient_loss | -0.0459   |
|    value_loss           | 0.00154   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 18.9                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 9                    |
|    time_elapsed    | -1683717608645156608 |
|    total_timesteps | 18432                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 19000     |
| train/                  |           |
|    approx_kl            | 10.114784 |
|    clip_fraction        | 0.945     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.23     |
|    explained_variance   | 0.29      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0138   |
|    n_updates            | 180       |
|    policy_gradient_loss | -0.0366   |
|    value_loss           | 0.00269   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 17.7                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 10                   |
|    time_elapsed    | -1683717608645156608 |
|    total_timesteps | 20480                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 21000     |
| train/                  |           |
|    approx_kl            | 18.492775 |
|    clip_fraction        | 0.95      |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.77     |
|    explained_variance   | 0.246     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0457   |
|    n_updates            | 200       |
|    policy_gradient_loss | -0.0117   |
|    value_loss           | 0.00369   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 15.4                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 11                   |
|    time_elapsed    | -1683717608645156352 |
|    total_timesteps | 22528                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 10       |
|    mean_reward          | 0.9      |
| time/                   |          |
|    total_timesteps      | 23000    |
| train/                  |          |
|    approx_kl            | 25.64139 |
|    clip_fraction        | 0.944    |
|    clip_range           | 0.2      |
|    entropy_loss         | -4.51    |
|    explained_variance   | 0.165    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0728  |
|    n_updates            | 220      |
|    policy_gradient_loss | -0.0493  |
|    value_loss           | 0.0021   |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 13.9                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 12                   |
|    time_elapsed    | -1683717608645156352 |
|    total_timesteps | 24576                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 10       |
|    mean_reward          | 0.9      |
| time/                   |          |
|    total_timesteps      | 25000    |
| train/                  |          |
|    approx_kl            | 64.86004 |
|    clip_fraction        | 0.942    |
|    clip_range           | 0.2      |
|    entropy_loss         | -4.19    |
|    explained_variance   | 0.118    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0735  |
|    n_updates            | 240      |
|    policy_gradient_loss | -0.0418  |
|    value_loss           | 0.00347  |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 13.3                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 13                   |
|    time_elapsed    | -1683717608645156352 |
|    total_timesteps | 26624                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 10       |
|    mean_reward          | 0.9      |
| time/                   |          |
|    total_timesteps      | 27000    |
| train/                  |          |
|    approx_kl            | 75.77888 |
|    clip_fraction        | 0.953    |
|    clip_range           | 0.2      |
|    entropy_loss         | -3.83    |
|    explained_variance   | -0.0149  |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0546  |
|    n_updates            | 260      |
|    policy_gradient_loss | -0.0608  |
|    value_loss           | 0.00103  |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 13.3                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 14                   |
|    time_elapsed    | -1683717608645156352 |
|    total_timesteps | 28672                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 29000     |
| train/                  |           |
|    approx_kl            | 154.98466 |
|    clip_fraction        | 0.937     |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.49     |
|    explained_variance   | 0.0661    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0627   |
|    n_updates            | 280       |
|    policy_gradient_loss | -0.0257   |
|    value_loss           | 0.00343   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 12.7                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 15                   |
|    time_elapsed    | -1683717608645156352 |
|    total_timesteps | 30720                |
---------------------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 10         |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 31000      |
| train/                  |            |
|    approx_kl            | 122.932236 |
|    clip_fraction        | 0.931      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.05      |
|    explained_variance   | -0.0308    |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0772    |
|    n_updates            | 300        |
|    policy_gradient_loss | 0.0853     |
|    value_loss           | 0.000933   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 11.8                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 16                   |
|    time_elapsed    | -1683717608645156096 |
|    total_timesteps | 32768                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 33000     |
| train/                  |           |
|    approx_kl            | 312.83667 |
|    clip_fraction        | 0.921     |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.64     |
|    explained_variance   | -0.112    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0705   |
|    n_updates            | 320       |
|    policy_gradient_loss | -0.0579   |
|    value_loss           | 0.00298   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 12.1                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 17                   |
|    time_elapsed    | -1683717608645156096 |
|    total_timesteps | 34816                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 274.73114 |
|    clip_fraction        | 0.929     |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.13     |
|    explained_variance   | -0.061    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0596   |
|    n_updates            | 340       |
|    policy_gradient_loss | -0.0599   |
|    value_loss           | 0.000169  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 12.2                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 18                   |
|    time_elapsed    | -1683717608645156096 |
|    total_timesteps | 36864                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 10       |
|    mean_reward          | 0.9      |
| time/                   |          |
|    total_timesteps      | 37000    |
| train/                  |          |
|    approx_kl            | 66.73318 |
|    clip_fraction        | 0.816    |
|    clip_range           | 0.2      |
|    entropy_loss         | -1.6     |
|    explained_variance   | -0.0316  |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0657  |
|    n_updates            | 360      |
|    policy_gradient_loss | -0.0571  |
|    value_loss           | 0.00143  |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 11.7                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 19                   |
|    time_elapsed    | -1683717608645156096 |
|    total_timesteps | 38912                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 39000     |
| train/                  |           |
|    approx_kl            | 57.755722 |
|    clip_fraction        | 0.684     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.26     |
|    explained_variance   | -0.101    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0867   |
|    n_updates            | 380       |
|    policy_gradient_loss | -0.0451   |
|    value_loss           | 0.000752  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 11.3                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 20                   |
|    time_elapsed    | -1683717608645156096 |
|    total_timesteps | 40960                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 10       |
|    mean_reward          | 0.9      |
| time/                   |          |
|    total_timesteps      | 41000    |
| train/                  |          |
|    approx_kl            | 74.81316 |
|    clip_fraction        | 0.623    |
|    clip_range           | 0.2      |
|    entropy_loss         | -1.12    |
|    explained_variance   | -0.0984  |
|    learning_rate        | 0.0003   |
|    loss                 | 0.773    |
|    n_updates            | 400      |
|    policy_gradient_loss | -0.0411  |
|    value_loss           | 0.00341  |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 11                   |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 21                   |
|    time_elapsed    | -1683717608645155840 |
|    total_timesteps | 43008                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 10       |
|    mean_reward          | 0.9      |
| time/                   |          |
|    total_timesteps      | 44000    |
| train/                  |          |
|    approx_kl            | 73.59216 |
|    clip_fraction        | 0.599    |
|    clip_range           | 0.2      |
|    entropy_loss         | -1.12    |
|    explained_variance   | -0.114   |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0325  |
|    n_updates            | 420      |
|    policy_gradient_loss | -0.0357  |
|    value_loss           | 0.00342  |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 11.3                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 22                   |
|    time_elapsed    | -1683717608645155840 |
|    total_timesteps | 45056                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 46000     |
| train/                  |           |
|    approx_kl            | 153.50874 |
|    clip_fraction        | 0.593     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.994    |
|    explained_variance   | -0.136    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0247   |
|    n_updates            | 440       |
|    policy_gradient_loss | -0.0415   |
|    value_loss           | 0.00292   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 10.8                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 23                   |
|    time_elapsed    | -1683717608645155840 |
|    total_timesteps | 47104                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 48000     |
| train/                  |           |
|    approx_kl            | 139.54855 |
|    clip_fraction        | 0.581     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.897    |
|    explained_variance   | -0.0722   |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0393   |
|    n_updates            | 460       |
|    policy_gradient_loss | -0.0413   |
|    value_loss           | 0.00214   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 10.8                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 24                   |
|    time_elapsed    | -1683717608645155840 |
|    total_timesteps | 49152                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 10       |
|    mean_reward          | 0.9      |
| time/                   |          |
|    total_timesteps      | 50000    |
| train/                  |          |
|    approx_kl            | 52.64545 |
|    clip_fraction        | 0.589    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.952   |
|    explained_variance   | -0.0229  |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0503  |
|    n_updates            | 480      |
|    policy_gradient_loss | -0.0304  |
|    value_loss           | 0.00232  |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 11.5                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 25                   |
|    time_elapsed    | -1683717608645155840 |
|    total_timesteps | 51200                |
---------------------------------------------
{'reward': [0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9], 'std': [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]}
{'reward': [0.9000000134110451, 0.9000000134110451, 0.9000000134110451, 0.9000000134110451, 0.9000000134110451, 0.9000000134110451, 0.9000000134110451, 0.9000000134110451, 0.9000000134110451, 0.9000000134110451], 'std': [9, 9, 9, 9, 9, 9, 9, 9, 9, 9]}
