Logging to ../Logging/RPPO_BW_Then_FW_5_800
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 15.6                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 1                    |
|    time_elapsed    | -1683715400403608576 |
|    total_timesteps | 2048                 |
---------------------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50         |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 3000       |
| train/                  |            |
|    approx_kl            | 0.02868017 |
|    clip_fraction        | 0.512      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.11      |
|    explained_variance   | -12.5      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.139     |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.119     |
|    value_loss           | 0.0159     |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 13.2                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 2                    |
|    time_elapsed    | -1683715400403608576 |
|    total_timesteps | 4096                 |
---------------------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50         |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.13745219 |
|    clip_fraction        | 0.604      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.07      |
|    explained_variance   | -0.0971    |
|    learning_rate        | 0.0003     |
|    loss                 | -0.044     |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.034     |
|    value_loss           | 0.00092    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 10.5                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 3                    |
|    time_elapsed    | -1683715400403608320 |
|    total_timesteps | 6144                 |
---------------------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50         |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 7000       |
| train/                  |            |
|    approx_kl            | 0.45463127 |
|    clip_fraction        | 0.8        |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.99      |
|    explained_variance   | -0.158     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0919    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0244    |
|    value_loss           | 0.000471   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 10.7                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 4                    |
|    time_elapsed    | -1683715400403608320 |
|    total_timesteps | 8192                 |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 9000      |
| train/                  |           |
|    approx_kl            | 1.0266974 |
|    clip_fraction        | 0.865     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.87     |
|    explained_variance   | -0.287    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.113    |
|    n_updates            | 80        |
|    policy_gradient_loss | -0.0234   |
|    value_loss           | 0.000208  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 9.28                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 5                    |
|    time_elapsed    | -1683715400403608320 |
|    total_timesteps | 10240                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5        |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 11000    |
| train/                  |          |
|    approx_kl            | 1.870716 |
|    clip_fraction        | 0.912    |
|    clip_range           | 0.2      |
|    entropy_loss         | -4.76    |
|    explained_variance   | -0.303   |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0831  |
|    n_updates            | 100      |
|    policy_gradient_loss | -0.0202  |
|    value_loss           | 0.000463 |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 7.9                  |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 6                    |
|    time_elapsed    | -1683715400403608064 |
|    total_timesteps | 12288                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 13000     |
| train/                  |           |
|    approx_kl            | 3.6153443 |
|    clip_fraction        | 0.935     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.4      |
|    explained_variance   | -0.508    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.111    |
|    n_updates            | 120       |
|    policy_gradient_loss | -0.0288   |
|    value_loss           | 0.000113  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 8.85                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 7                    |
|    time_elapsed    | -1683715400403608064 |
|    total_timesteps | 14336                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5        |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 15000    |
| train/                  |          |
|    approx_kl            | 8.24972  |
|    clip_fraction        | 0.942    |
|    clip_range           | 0.2      |
|    entropy_loss         | -3.94    |
|    explained_variance   | -0.447   |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0411  |
|    n_updates            | 140      |
|    policy_gradient_loss | -0.0214  |
|    value_loss           | 0.00047  |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 7.17                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 8                    |
|    time_elapsed    | -1683715400403608064 |
|    total_timesteps | 16384                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 17000     |
| train/                  |           |
|    approx_kl            | 21.120644 |
|    clip_fraction        | 0.947     |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.31     |
|    explained_variance   | -0.69     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.103    |
|    n_updates            | 160       |
|    policy_gradient_loss | -0.0469   |
|    value_loss           | 5.35e-05  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 6.07                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 9                    |
|    time_elapsed    | -1683715400403608064 |
|    total_timesteps | 18432                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5        |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 19000    |
| train/                  |          |
|    approx_kl            | 54.6092  |
|    clip_fraction        | 0.955    |
|    clip_range           | 0.2      |
|    entropy_loss         | -2.51    |
|    explained_variance   | -0.628   |
|    learning_rate        | 0.0003   |
|    loss                 | -0.00717 |
|    n_updates            | 180      |
|    policy_gradient_loss | -0.083   |
|    value_loss           | 0.000771 |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.85                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 10                   |
|    time_elapsed    | -1683715400403607808 |
|    total_timesteps | 20480                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 21000     |
| train/                  |           |
|    approx_kl            | 29.875084 |
|    clip_fraction        | 0.937     |
|    clip_range           | 0.2       |
|    entropy_loss         | -2        |
|    explained_variance   | -0.938    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0985   |
|    n_updates            | 200       |
|    policy_gradient_loss | -0.086    |
|    value_loss           | 0.000784  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.55                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 11                   |
|    time_elapsed    | -1683715400403607808 |
|    total_timesteps | 22528                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 23000     |
| train/                  |           |
|    approx_kl            | 57.475143 |
|    clip_fraction        | 0.904     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.28     |
|    explained_variance   | -0.757    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.119    |
|    n_updates            | 220       |
|    policy_gradient_loss | -0.108    |
|    value_loss           | 0.000693  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.96                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 12                   |
|    time_elapsed    | -1683715400403607808 |
|    total_timesteps | 24576                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 25000     |
| train/                  |           |
|    approx_kl            | 44.593384 |
|    clip_fraction        | 0.682     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.724    |
|    explained_variance   | -1.03     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.109    |
|    n_updates            | 240       |
|    policy_gradient_loss | -0.1      |
|    value_loss           | 0.000277  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.34                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 13                   |
|    time_elapsed    | -1683715400403607552 |
|    total_timesteps | 26624                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5        |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 27000    |
| train/                  |          |
|    approx_kl            | 78.43596 |
|    clip_fraction        | 0.494    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.417   |
|    explained_variance   | -0.999   |
|    learning_rate        | 0.0003   |
|    loss                 | -0.121   |
|    n_updates            | 260      |
|    policy_gradient_loss | -0.0933  |
|    value_loss           | 0.000147 |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.17                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 14                   |
|    time_elapsed    | -1683715400403607552 |
|    total_timesteps | 28672                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5        |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 29000    |
| train/                  |          |
|    approx_kl            | 80.6204  |
|    clip_fraction        | 0.457    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.322   |
|    explained_variance   | -1.01    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0935  |
|    n_updates            | 280      |
|    policy_gradient_loss | -0.0966  |
|    value_loss           | 0.000523 |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.12                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 15                   |
|    time_elapsed    | -1683715400403607552 |
|    total_timesteps | 30720                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 31000     |
| train/                  |           |
|    approx_kl            | 13.443933 |
|    clip_fraction        | 0.457     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.288    |
|    explained_variance   | -1.02     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0928   |
|    n_updates            | 300       |
|    policy_gradient_loss | -0.0902   |
|    value_loss           | 0.000621  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.11                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 16                   |
|    time_elapsed    | -1683715400403607296 |
|    total_timesteps | 32768                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 33000     |
| train/                  |           |
|    approx_kl            | 12.414572 |
|    clip_fraction        | 0.444     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.285    |
|    explained_variance   | -0.985    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0921   |
|    n_updates            | 320       |
|    policy_gradient_loss | -0.0944   |
|    value_loss           | 0.000242  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.09                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 17                   |
|    time_elapsed    | -1683715400403607296 |
|    total_timesteps | 34816                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 30.761345 |
|    clip_fraction        | 0.458     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.282    |
|    explained_variance   | -1.06     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0339   |
|    n_updates            | 340       |
|    policy_gradient_loss | -0.088    |
|    value_loss           | 0.00049   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.11                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 18                   |
|    time_elapsed    | -1683715400403607296 |
|    total_timesteps | 36864                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 37000     |
| train/                  |           |
|    approx_kl            | 10.195593 |
|    clip_fraction        | 0.507     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.287    |
|    explained_variance   | -1.07     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0876   |
|    n_updates            | 360       |
|    policy_gradient_loss | -0.075    |
|    value_loss           | 0.000303  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.16                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 19                   |
|    time_elapsed    | -1683715400403607296 |
|    total_timesteps | 38912                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5        |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 39000    |
| train/                  |          |
|    approx_kl            | 87.39549 |
|    clip_fraction        | 0.517    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.302   |
|    explained_variance   | -1.09    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0587  |
|    n_updates            | 380      |
|    policy_gradient_loss | -0.0732  |
|    value_loss           | 0.000224 |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.17                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 20                   |
|    time_elapsed    | -1683715400403607040 |
|    total_timesteps | 40960                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 41000     |
| train/                  |           |
|    approx_kl            | 22.712608 |
|    clip_fraction        | 0.446     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.315    |
|    explained_variance   | -0.992    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.096    |
|    n_updates            | 400       |
|    policy_gradient_loss | -0.103    |
|    value_loss           | 0.000331  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.13                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 21                   |
|    time_elapsed    | -1683715400403607040 |
|    total_timesteps | 43008                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 44000     |
| train/                  |           |
|    approx_kl            | 15.084806 |
|    clip_fraction        | 0.434     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.326    |
|    explained_variance   | -0.987    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0952   |
|    n_updates            | 420       |
|    policy_gradient_loss | -0.11     |
|    value_loss           | 0.000977  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.04                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 22                   |
|    time_elapsed    | -1683715400403607040 |
|    total_timesteps | 45056                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 46000     |
| train/                  |           |
|    approx_kl            | 15.999266 |
|    clip_fraction        | 0.427     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.333    |
|    explained_variance   | -0.982    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.107    |
|    n_updates            | 440       |
|    policy_gradient_loss | -0.106    |
|    value_loss           | 0.000112  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.03                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 23                   |
|    time_elapsed    | -1683715400403606784 |
|    total_timesteps | 47104                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 48000     |
| train/                  |           |
|    approx_kl            | 15.404169 |
|    clip_fraction        | 0.435     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.351    |
|    explained_variance   | -1.07     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.11     |
|    n_updates            | 460       |
|    policy_gradient_loss | -0.109    |
|    value_loss           | 0.00016   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5                    |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 24                   |
|    time_elapsed    | -1683715400403606784 |
|    total_timesteps | 49152                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5        |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 50000    |
| train/                  |          |
|    approx_kl            | 30.86363 |
|    clip_fraction        | 0.454    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.372   |
|    explained_variance   | -0.985   |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0947  |
|    n_updates            | 480      |
|    policy_gradient_loss | -0.103   |
|    value_loss           | 6.94e-05 |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.03                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 25                   |
|    time_elapsed    | -1683715400403606784 |
|    total_timesteps | 51200                |
---------------------------------------------
{'reward': [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4], 'std': [5, 5, 5, 5, 5, 5, 5, 5, 5, 5]}
{'reward': [0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645], 'std': [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]}
