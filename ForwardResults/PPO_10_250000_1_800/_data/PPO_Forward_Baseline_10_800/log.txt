Logging to ../Logging/PPO_Forward_Baseline_10_800
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.6      |
| time/              |          |
|    fps             | 25       |
|    iterations      | 1        |
|    time_elapsed    | 78       |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 3000        |
| train/                  |             |
|    approx_kl            | 0.014875114 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.54       |
|    explained_variance   | 0.628       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0296     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0285     |
|    value_loss           | 0.00772     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.65     |
| time/              |          |
|    fps             | 25       |
|    iterations      | 2        |
|    time_elapsed    | 161      |
|    total_timesteps | 4096     |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.015680533 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.53       |
|    explained_variance   | 0.767       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0458     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0256     |
|    value_loss           | 0.00591     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.617    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 3        |
|    time_elapsed    | 244      |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 7000        |
| train/                  |             |
|    approx_kl            | 0.018359408 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.51       |
|    explained_variance   | 0.811       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0411     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0294     |
|    value_loss           | 0.00727     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.65     |
| time/              |          |
|    fps             | 25       |
|    iterations      | 4        |
|    time_elapsed    | 327      |
|    total_timesteps | 8192     |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 9000        |
| train/                  |             |
|    approx_kl            | 0.015361104 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.49       |
|    explained_variance   | 0.866       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0125     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0234     |
|    value_loss           | 0.00312     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.66     |
| time/              |          |
|    fps             | 25       |
|    iterations      | 5        |
|    time_elapsed    | 409      |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 11000       |
| train/                  |             |
|    approx_kl            | 0.018827602 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.48       |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0542     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0285     |
|    value_loss           | 0.00316     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 990      |
|    ep_rew_mean     | 0.667    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 6        |
|    time_elapsed    | 490      |
|    total_timesteps | 12288    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 13000       |
| train/                  |             |
|    approx_kl            | 0.018510163 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.48       |
|    explained_variance   | 0.814       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0319     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0266     |
|    value_loss           | 0.00352     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 979      |
|    ep_rew_mean     | 0.707    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 7        |
|    time_elapsed    | 572      |
|    total_timesteps | 14336    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.01620911 |
|    clip_fraction        | 0.175      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.47      |
|    explained_variance   | 0.786      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0436    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.029     |
|    value_loss           | 0.00205    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.725    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 8        |
|    time_elapsed    | 653      |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 17000       |
| train/                  |             |
|    approx_kl            | 0.014933901 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.43       |
|    explained_variance   | 0.674       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0485     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0214     |
|    value_loss           | 0.00148     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.722    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 9        |
|    time_elapsed    | 734      |
|    total_timesteps | 18432    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 19000       |
| train/                  |             |
|    approx_kl            | 0.018878438 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.44       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0374     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0296     |
|    value_loss           | 0.0016      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.715    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 10       |
|    time_elapsed    | 815      |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 21000       |
| train/                  |             |
|    approx_kl            | 0.017506042 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.41       |
|    explained_variance   | 0.897       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00268     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0262     |
|    value_loss           | 0.00124     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.718    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 11       |
|    time_elapsed    | 896      |
|    total_timesteps | 22528    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 23000       |
| train/                  |             |
|    approx_kl            | 0.018238815 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.4        |
|    explained_variance   | 0.684       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0443     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0362     |
|    value_loss           | 0.00192     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.742    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 12       |
|    time_elapsed    | 978      |
|    total_timesteps | 24576    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.015253939 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.41       |
|    explained_variance   | 0.855       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0157     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0249     |
|    value_loss           | 0.00134     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.731    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 13       |
|    time_elapsed    | 1059     |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 27000      |
| train/                  |            |
|    approx_kl            | 0.01543555 |
|    clip_fraction        | 0.174      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.39      |
|    explained_variance   | 0.899      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0243    |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0295    |
|    value_loss           | 0.00046    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.732    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 14       |
|    time_elapsed    | 1140     |
|    total_timesteps | 28672    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 29000       |
| train/                  |             |
|    approx_kl            | 0.017788198 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.36       |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0508     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0222     |
|    value_loss           | 0.00136     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.717    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 15       |
|    time_elapsed    | 1222     |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 31000      |
| train/                  |            |
|    approx_kl            | 0.01957356 |
|    clip_fraction        | 0.189      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.3       |
|    explained_variance   | 0.877      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0058    |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0241    |
|    value_loss           | 0.0016     |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.722    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 16       |
|    time_elapsed    | 1303     |
|    total_timesteps | 32768    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 33000       |
| train/                  |             |
|    approx_kl            | 0.015266003 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.33       |
|    explained_variance   | 0.851       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.042      |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.025      |
|    value_loss           | 0.0012      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.715    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 17       |
|    time_elapsed    | 1385     |
|    total_timesteps | 34816    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.016236551 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.34       |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0036     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0288     |
|    value_loss           | 0.000634    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.719    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 18       |
|    time_elapsed    | 1466     |
|    total_timesteps | 36864    |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | 0            |
| time/                   |              |
|    total_timesteps      | 37000        |
| train/                  |              |
|    approx_kl            | 0.0142268175 |
|    clip_fraction        | 0.166        |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.35        |
|    explained_variance   | 0.864        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00102     |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.0254      |
|    value_loss           | 0.000689     |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.711    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 19       |
|    time_elapsed    | 1548     |
|    total_timesteps | 38912    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 39000       |
| train/                  |             |
|    approx_kl            | 0.018466419 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.32       |
|    explained_variance   | 0.841       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0293     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.029      |
|    value_loss           | 0.000466    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.71     |
| time/              |          |
|    fps             | 25       |
|    iterations      | 20       |
|    time_elapsed    | 1629     |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 41000       |
| train/                  |             |
|    approx_kl            | 0.020604488 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.27       |
|    explained_variance   | 0.829       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0604     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0343     |
|    value_loss           | 0.000781    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.707    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 21       |
|    time_elapsed    | 1737     |
|    total_timesteps | 43008    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 44000       |
| train/                  |             |
|    approx_kl            | 0.014288854 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.34       |
|    explained_variance   | 0.787       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0153     |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0241     |
|    value_loss           | 0.00116     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.702    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 22       |
|    time_elapsed    | 1818     |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 46000       |
| train/                  |             |
|    approx_kl            | 0.015214082 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.25       |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0235     |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0194     |
|    value_loss           | 0.0008      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.702    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 23       |
|    time_elapsed    | 1899     |
|    total_timesteps | 47104    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 48000       |
| train/                  |             |
|    approx_kl            | 0.016066127 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.26       |
|    explained_variance   | 0.898       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0154     |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0232     |
|    value_loss           | 0.000896    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.7      |
| time/              |          |
|    fps             | 24       |
|    iterations      | 24       |
|    time_elapsed    | 1980     |
|    total_timesteps | 49152    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.020557987 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.2        |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0255     |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0273     |
|    value_loss           | 0.000513    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.696    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 25       |
|    time_elapsed    | 2061     |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 52000       |
| train/                  |             |
|    approx_kl            | 0.016593885 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.25       |
|    explained_variance   | 0.825       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0224     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0244     |
|    value_loss           | 0.000973    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.696    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 26       |
|    time_elapsed    | 2143     |
|    total_timesteps | 53248    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 54000       |
| train/                  |             |
|    approx_kl            | 0.015613603 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.23       |
|    explained_variance   | 0.833       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0393     |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0257     |
|    value_loss           | 0.000571    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.693    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 27       |
|    time_elapsed    | 2225     |
|    total_timesteps | 55296    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 56000      |
| train/                  |            |
|    approx_kl            | 0.01828432 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.22      |
|    explained_variance   | 0.8        |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0394    |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.0245    |
|    value_loss           | 0.00123    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.689    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 28       |
|    time_elapsed    | 2306     |
|    total_timesteps | 57344    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 58000       |
| train/                  |             |
|    approx_kl            | 0.014482347 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.2        |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0384     |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0182     |
|    value_loss           | 0.000744    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.692    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 29       |
|    time_elapsed    | 2387     |
|    total_timesteps | 59392    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.017594066 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.25       |
|    explained_variance   | 0.846       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0319     |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.027      |
|    value_loss           | 0.000715    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.685    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 30       |
|    time_elapsed    | 2469     |
|    total_timesteps | 61440    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 62000       |
| train/                  |             |
|    approx_kl            | 0.023821354 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.18       |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.052      |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.0271     |
|    value_loss           | 0.000168    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.689    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 31       |
|    time_elapsed    | 2550     |
|    total_timesteps | 63488    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 64000       |
| train/                  |             |
|    approx_kl            | 0.014805959 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.16       |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0418     |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.0213     |
|    value_loss           | 0.000643    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 995      |
|    ep_rew_mean     | 0.694    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 32       |
|    time_elapsed    | 2631     |
|    total_timesteps | 65536    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 66000       |
| train/                  |             |
|    approx_kl            | 0.016045395 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.16       |
|    explained_variance   | 0.816       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0433     |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0245     |
|    value_loss           | 0.000673    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.691    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 33       |
|    time_elapsed    | 2712     |
|    total_timesteps | 67584    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 68000       |
| train/                  |             |
|    approx_kl            | 0.016531896 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.16       |
|    explained_variance   | 0.867       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0365     |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.0273     |
|    value_loss           | 0.000806    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.691    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 34       |
|    time_elapsed    | 2793     |
|    total_timesteps | 69632    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.017956542 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.21       |
|    explained_variance   | 0.779       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00279    |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.0243     |
|    value_loss           | 0.000833    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.69     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 35       |
|    time_elapsed    | 2875     |
|    total_timesteps | 71680    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 72000       |
| train/                  |             |
|    approx_kl            | 0.015059219 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.13       |
|    explained_variance   | 0.893       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0243     |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.0279     |
|    value_loss           | 0.000316    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 995      |
|    ep_rew_mean     | 0.693    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 36       |
|    time_elapsed    | 2956     |
|    total_timesteps | 73728    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 74000       |
| train/                  |             |
|    approx_kl            | 0.016328603 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.19       |
|    explained_variance   | 0.668       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0496     |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.0285     |
|    value_loss           | 0.000715    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 994      |
|    ep_rew_mean     | 0.713    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 37       |
|    time_elapsed    | 3037     |
|    total_timesteps | 75776    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 76000       |
| train/                  |             |
|    approx_kl            | 0.017053705 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.19       |
|    explained_variance   | 0.86        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00437    |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.0286     |
|    value_loss           | 0.000813    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.719    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 38       |
|    time_elapsed    | 3119     |
|    total_timesteps | 77824    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 78000      |
| train/                  |            |
|    approx_kl            | 0.01925229 |
|    clip_fraction        | 0.234      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.08      |
|    explained_variance   | 0.837      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00167   |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.0255    |
|    value_loss           | 0.000753   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.718    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 39       |
|    time_elapsed    | 3200     |
|    total_timesteps | 79872    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.015797827 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.1        |
|    explained_variance   | 0.904       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0169     |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.0232     |
|    value_loss           | 0.000409    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.717    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 40       |
|    time_elapsed    | 3282     |
|    total_timesteps | 81920    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 82000       |
| train/                  |             |
|    approx_kl            | 0.015932094 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.06       |
|    explained_variance   | 0.904       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0645     |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.0242     |
|    value_loss           | 0.000569    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 999      |
|    ep_rew_mean     | 0.72     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 41       |
|    time_elapsed    | 3363     |
|    total_timesteps | 83968    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 84000       |
| train/                  |             |
|    approx_kl            | 0.015728327 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.03       |
|    explained_variance   | 0.904       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0432     |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.0259     |
|    value_loss           | 0.000603    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.727    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 42       |
|    time_elapsed    | 3471     |
|    total_timesteps | 86016    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 87000       |
| train/                  |             |
|    approx_kl            | 0.017490003 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.12       |
|    explained_variance   | 0.789       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0345     |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0273     |
|    value_loss           | 0.000694    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 993      |
|    ep_rew_mean     | 0.739    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 43       |
|    time_elapsed    | 3553     |
|    total_timesteps | 88064    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 89000       |
| train/                  |             |
|    approx_kl            | 0.016325042 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.01       |
|    explained_variance   | 0.804       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00623    |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.0218     |
|    value_loss           | 0.00135     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.747    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 44       |
|    time_elapsed    | 3634     |
|    total_timesteps | 90112    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 91000       |
| train/                  |             |
|    approx_kl            | 0.019025594 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.05       |
|    explained_variance   | 0.893       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0361     |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.0253     |
|    value_loss           | 0.000508    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 989      |
|    ep_rew_mean     | 0.748    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 45       |
|    time_elapsed    | 3716     |
|    total_timesteps | 92160    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 93000      |
| train/                  |            |
|    approx_kl            | 0.01735227 |
|    clip_fraction        | 0.197      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.96      |
|    explained_variance   | 0.872      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0329    |
|    n_updates            | 450        |
|    policy_gradient_loss | -0.0267    |
|    value_loss           | 0.00057    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 989      |
|    ep_rew_mean     | 0.756    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 46       |
|    time_elapsed    | 3797     |
|    total_timesteps | 94208    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 95000       |
| train/                  |             |
|    approx_kl            | 0.016174912 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.08       |
|    explained_variance   | 0.815       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0447     |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.0241     |
|    value_loss           | 0.00055     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 990      |
|    ep_rew_mean     | 0.756    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 47       |
|    time_elapsed    | 3878     |
|    total_timesteps | 96256    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 97000       |
| train/                  |             |
|    approx_kl            | 0.014188172 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.97       |
|    explained_variance   | 0.86        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0335     |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.0139     |
|    value_loss           | 0.00049     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 983      |
|    ep_rew_mean     | 0.766    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 48       |
|    time_elapsed    | 3960     |
|    total_timesteps | 98304    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 99000       |
| train/                  |             |
|    approx_kl            | 0.017733086 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.97       |
|    explained_variance   | 0.684       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.015       |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.0274     |
|    value_loss           | 0.0011      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 980      |
|    ep_rew_mean     | 0.776    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 49       |
|    time_elapsed    | 4041     |
|    total_timesteps | 100352   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 101000      |
| train/                  |             |
|    approx_kl            | 0.017006077 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.9        |
|    explained_variance   | 0.739       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0308     |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.0257     |
|    value_loss           | 0.000693    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 980      |
|    ep_rew_mean     | 0.777    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 50       |
|    time_elapsed    | 4123     |
|    total_timesteps | 102400   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.4        |
| time/                   |            |
|    total_timesteps      | 103000     |
| train/                  |            |
|    approx_kl            | 0.01789318 |
|    clip_fraction        | 0.192      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.93      |
|    explained_variance   | 0.863      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0325    |
|    n_updates            | 500        |
|    policy_gradient_loss | -0.0182    |
|    value_loss           | 0.000221   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 980      |
|    ep_rew_mean     | 0.782    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 51       |
|    time_elapsed    | 4204     |
|    total_timesteps | 104448   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 105000      |
| train/                  |             |
|    approx_kl            | 0.019107642 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.96       |
|    explained_variance   | 0.897       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0392     |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.0203     |
|    value_loss           | 0.000272    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 980      |
|    ep_rew_mean     | 0.791    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 52       |
|    time_elapsed    | 4286     |
|    total_timesteps | 106496   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 107000      |
| train/                  |             |
|    approx_kl            | 0.014254903 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.94       |
|    explained_variance   | 0.771       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0183      |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.0195     |
|    value_loss           | 0.000866    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 974      |
|    ep_rew_mean     | 0.794    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 53       |
|    time_elapsed    | 4369     |
|    total_timesteps | 108544   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 109000      |
| train/                  |             |
|    approx_kl            | 0.016780885 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.94       |
|    explained_variance   | 0.788       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0427     |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.0211     |
|    value_loss           | 0.00092     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 969      |
|    ep_rew_mean     | 0.804    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 54       |
|    time_elapsed    | 4450     |
|    total_timesteps | 110592   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 111000      |
| train/                  |             |
|    approx_kl            | 0.016154446 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.93       |
|    explained_variance   | 0.839       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0592     |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.0227     |
|    value_loss           | 0.000719    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 965      |
|    ep_rew_mean     | 0.813    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 55       |
|    time_elapsed    | 4532     |
|    total_timesteps | 112640   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 113000      |
| train/                  |             |
|    approx_kl            | 0.017474934 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.93       |
|    explained_variance   | 0.857       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0369     |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.0258     |
|    value_loss           | 0.00062     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 956      |
|    ep_rew_mean     | 0.833    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 56       |
|    time_elapsed    | 4615     |
|    total_timesteps | 114688   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 115000      |
| train/                  |             |
|    approx_kl            | 0.016539238 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.79       |
|    explained_variance   | 0.775       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00228    |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.0016      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 958      |
|    ep_rew_mean     | 0.844    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 57       |
|    time_elapsed    | 4697     |
|    total_timesteps | 116736   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 117000      |
| train/                  |             |
|    approx_kl            | 0.019730303 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.83       |
|    explained_variance   | 0.802       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0243      |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.00146     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 957      |
|    ep_rew_mean     | 0.866    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 58       |
|    time_elapsed    | 4779     |
|    total_timesteps | 118784   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 119000      |
| train/                  |             |
|    approx_kl            | 0.017527472 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.64       |
|    explained_variance   | 0.648       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0179     |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.0226     |
|    value_loss           | 0.0018      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 925      |
|    ep_rew_mean     | 0.884    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 59       |
|    time_elapsed    | 4861     |
|    total_timesteps | 120832   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 121000      |
| train/                  |             |
|    approx_kl            | 0.015878271 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.65       |
|    explained_variance   | 0.804       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.047      |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.0201     |
|    value_loss           | 0.00169     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 886      |
|    ep_rew_mean     | 0.913    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 60       |
|    time_elapsed    | 4943     |
|    total_timesteps | 122880   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 123000      |
| train/                  |             |
|    approx_kl            | 0.015845604 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.55       |
|    explained_variance   | 0.804       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0502     |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.0209     |
|    value_loss           | 0.00205     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 869      |
|    ep_rew_mean     | 0.927    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 61       |
|    time_elapsed    | 5025     |
|    total_timesteps | 124928   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 125000      |
| train/                  |             |
|    approx_kl            | 0.016953293 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.79       |
|    explained_variance   | 0.778       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0393     |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.0236     |
|    value_loss           | 0.00157     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 844      |
|    ep_rew_mean     | 0.951    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 62       |
|    time_elapsed    | 5107     |
|    total_timesteps | 126976   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 127000      |
| train/                  |             |
|    approx_kl            | 0.017781073 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.57       |
|    explained_variance   | 0.792       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0123     |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.00145     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 819      |
|    ep_rew_mean     | 0.979    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 63       |
|    time_elapsed    | 5217     |
|    total_timesteps | 129024   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.5        |
| time/                   |            |
|    total_timesteps      | 130000     |
| train/                  |            |
|    approx_kl            | 0.01631396 |
|    clip_fraction        | 0.182      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.61      |
|    explained_variance   | 0.612      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0243    |
|    n_updates            | 630        |
|    policy_gradient_loss | -0.0191    |
|    value_loss           | 0.00222    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 799      |
|    ep_rew_mean     | 1.01     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 64       |
|    time_elapsed    | 5299     |
|    total_timesteps | 131072   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 132000      |
| train/                  |             |
|    approx_kl            | 0.017580157 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.53       |
|    explained_variance   | 0.789       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0418     |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.0204     |
|    value_loss           | 0.00178     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 760      |
|    ep_rew_mean     | 1.03     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 65       |
|    time_elapsed    | 5381     |
|    total_timesteps | 133120   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 134000      |
| train/                  |             |
|    approx_kl            | 0.017393224 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.49       |
|    explained_variance   | 0.764       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0386     |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.0247     |
|    value_loss           | 0.00157     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 709      |
|    ep_rew_mean     | 1.06     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 66       |
|    time_elapsed    | 5464     |
|    total_timesteps | 135168   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 136000      |
| train/                  |             |
|    approx_kl            | 0.016168451 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.42       |
|    explained_variance   | 0.79        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0261     |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.00246     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 668      |
|    ep_rew_mean     | 1.09     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 67       |
|    time_elapsed    | 5546     |
|    total_timesteps | 137216   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 138000      |
| train/                  |             |
|    approx_kl            | 0.017569099 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.45       |
|    explained_variance   | 0.613       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00635    |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.0221     |
|    value_loss           | 0.00322     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 592      |
|    ep_rew_mean     | 1.11     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 68       |
|    time_elapsed    | 5628     |
|    total_timesteps | 139264   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 140000      |
| train/                  |             |
|    approx_kl            | 0.017420407 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.35       |
|    explained_variance   | 0.822       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0421     |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.0202     |
|    value_loss           | 0.00243     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 522      |
|    ep_rew_mean     | 1.11     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 69       |
|    time_elapsed    | 5710     |
|    total_timesteps | 141312   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 142000      |
| train/                  |             |
|    approx_kl            | 0.018088195 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.36       |
|    explained_variance   | 0.76        |
|    learning_rate        | 0.0003      |
|    loss                 | -3.5e-05    |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.0204     |
|    value_loss           | 0.00342     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 412      |
|    ep_rew_mean     | 1.12     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 70       |
|    time_elapsed    | 5793     |
|    total_timesteps | 143360   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 144000      |
| train/                  |             |
|    approx_kl            | 0.017217634 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.16       |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0178     |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.0242     |
|    value_loss           | 0.00236     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 1.09     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 71       |
|    time_elapsed    | 5875     |
|    total_timesteps | 145408   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 146000      |
| train/                  |             |
|    approx_kl            | 0.019739361 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.08       |
|    explained_variance   | 0.868       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0288     |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.0218     |
|    value_loss           | 0.00218     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 238      |
|    ep_rew_mean     | 1.06     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 72       |
|    time_elapsed    | 5957     |
|    total_timesteps | 147456   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 148000      |
| train/                  |             |
|    approx_kl            | 0.015974615 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.03       |
|    explained_variance   | 0.82        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0185     |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.0139     |
|    value_loss           | 0.00352     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 158      |
|    ep_rew_mean     | 1.01     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 73       |
|    time_elapsed    | 6039     |
|    total_timesteps | 149504   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.016395662 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.88       |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0183     |
|    n_updates            | 730         |
|    policy_gradient_loss | -0.0212     |
|    value_loss           | 0.0024      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 151000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 123      |
|    ep_rew_mean     | 0.985    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 74       |
|    time_elapsed    | 6121     |
|    total_timesteps | 151552   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 152000      |
| train/                  |             |
|    approx_kl            | 0.017078081 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.92       |
|    explained_variance   | 0.841       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0144     |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.00332     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 153000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 0.978    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 75       |
|    time_elapsed    | 6203     |
|    total_timesteps | 153600   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 154000      |
| train/                  |             |
|    approx_kl            | 0.014284005 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.93       |
|    explained_variance   | 0.831       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00619     |
|    n_updates            | 750         |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 0.0041      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87       |
|    ep_rew_mean     | 0.968    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 76       |
|    time_elapsed    | 6285     |
|    total_timesteps | 155648   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.8        |
| time/                   |            |
|    total_timesteps      | 156000     |
| train/                  |            |
|    approx_kl            | 0.01713863 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.55      |
|    explained_variance   | 0.873      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0174    |
|    n_updates            | 760        |
|    policy_gradient_loss | -0.0187    |
|    value_loss           | 0.0044     |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.2     |
|    ep_rew_mean     | 0.964    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 77       |
|    time_elapsed    | 6367     |
|    total_timesteps | 157696   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 158000      |
| train/                  |             |
|    approx_kl            | 0.013441062 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.885       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0191      |
|    n_updates            | 770         |
|    policy_gradient_loss | -0.00892    |
|    value_loss           | 0.00403     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 159000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67.7     |
|    ep_rew_mean     | 0.955    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 78       |
|    time_elapsed    | 6450     |
|    total_timesteps | 159744   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 160000      |
| train/                  |             |
|    approx_kl            | 0.018748665 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.902       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00268    |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.00366     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 161000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69       |
|    ep_rew_mean     | 0.955    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 79       |
|    time_elapsed    | 6532     |
|    total_timesteps | 161792   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 162000      |
| train/                  |             |
|    approx_kl            | 0.015993273 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 0.86        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0178     |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.00445     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 163000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 63.1     |
|    ep_rew_mean     | 0.945    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 80       |
|    time_elapsed    | 6615     |
|    total_timesteps | 163840   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 164000      |
| train/                  |             |
|    approx_kl            | 0.019287387 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0308     |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.0025      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 165000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 62.3     |
|    ep_rew_mean     | 0.938    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 81       |
|    time_elapsed    | 6698     |
|    total_timesteps | 165888   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 166000      |
| train/                  |             |
|    approx_kl            | 0.027095806 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.36       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00158     |
|    n_updates            | 810         |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.00116     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 167000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.4     |
|    ep_rew_mean     | 0.932    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 82       |
|    time_elapsed    | 6724     |
|    total_timesteps | 167936   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 168000      |
| train/                  |             |
|    approx_kl            | 0.017666642 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00973    |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.00864    |
|    value_loss           | 0.00475     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 169000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.6     |
|    ep_rew_mean     | 0.927    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 83       |
|    time_elapsed    | 6749     |
|    total_timesteps | 169984   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 9          |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 170000     |
| train/                  |            |
|    approx_kl            | 0.01437421 |
|    clip_fraction        | 0.167      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.25      |
|    explained_variance   | 0.937      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0268     |
|    n_updates            | 830        |
|    policy_gradient_loss | -0.0139    |
|    value_loss           | 0.00254    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41       |
|    ep_rew_mean     | 0.927    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 84       |
|    time_elapsed    | 6777     |
|    total_timesteps | 172032   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 9          |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 173000     |
| train/                  |            |
|    approx_kl            | 0.01664762 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.91      |
|    explained_variance   | 0.908      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0128     |
|    n_updates            | 840        |
|    policy_gradient_loss | -0.0112    |
|    value_loss           | 0.00455    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 0.931    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 85       |
|    time_elapsed    | 6802     |
|    total_timesteps | 174080   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 175000      |
| train/                  |             |
|    approx_kl            | 0.018236965 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.59       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00595    |
|    n_updates            | 850         |
|    policy_gradient_loss | -0.0195     |
|    value_loss           | 0.00205     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.4     |
|    ep_rew_mean     | 0.923    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 86       |
|    time_elapsed    | 6828     |
|    total_timesteps | 176128   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 177000      |
| train/                  |             |
|    approx_kl            | 0.014139886 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.23       |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0252      |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.00726    |
|    value_loss           | 0.00443     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.2     |
|    ep_rew_mean     | 0.917    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 87       |
|    time_elapsed    | 6855     |
|    total_timesteps | 178176   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 179000      |
| train/                  |             |
|    approx_kl            | 0.023968857 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.86       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0621     |
|    n_updates            | 870         |
|    policy_gradient_loss | -0.0187     |
|    value_loss           | 0.00231     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.7     |
|    ep_rew_mean     | 0.91     |
| time/              |          |
|    fps             | 26       |
|    iterations      | 88       |
|    time_elapsed    | 6881     |
|    total_timesteps | 180224   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 181000      |
| train/                  |             |
|    approx_kl            | 0.014214755 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.63       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0197      |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 0.00231     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.3     |
|    ep_rew_mean     | 0.907    |
| time/              |          |
|    fps             | 26       |
|    iterations      | 89       |
|    time_elapsed    | 6907     |
|    total_timesteps | 182272   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 183000      |
| train/                  |             |
|    approx_kl            | 0.026792709 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.75       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0245     |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.022      |
|    value_loss           | 0.000739    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | 0.902    |
| time/              |          |
|    fps             | 26       |
|    iterations      | 90       |
|    time_elapsed    | 6933     |
|    total_timesteps | 184320   |
---------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 9         |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 185000    |
| train/                  |           |
|    approx_kl            | 0.0182086 |
|    clip_fraction        | 0.174     |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.14     |
|    explained_variance   | 0.998     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0135   |
|    n_updates            | 900       |
|    policy_gradient_loss | -0.0186   |
|    value_loss           | 6.4e-05   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.9     |
|    ep_rew_mean     | 0.912    |
| time/              |          |
|    fps             | 26       |
|    iterations      | 91       |
|    time_elapsed    | 6959     |
|    total_timesteps | 186368   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 187000      |
| train/                  |             |
|    approx_kl            | 0.020793483 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.73       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0519     |
|    n_updates            | 910         |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.00142     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 12.7     |
|    ep_rew_mean     | 0.908    |
| time/              |          |
|    fps             | 26       |
|    iterations      | 92       |
|    time_elapsed    | 6986     |
|    total_timesteps | 188416   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 9          |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 189000     |
| train/                  |            |
|    approx_kl            | 0.02117296 |
|    clip_fraction        | 0.158      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.38      |
|    explained_variance   | 0.945      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0216     |
|    n_updates            | 920        |
|    policy_gradient_loss | -0.0114    |
|    value_loss           | 0.00408    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 11.8     |
|    ep_rew_mean     | 0.908    |
| time/              |          |
|    fps             | 27       |
|    iterations      | 93       |
|    time_elapsed    | 7012     |
|    total_timesteps | 190464   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 191000      |
| train/                  |             |
|    approx_kl            | 0.015916286 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.13       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0427     |
|    n_updates            | 930         |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.00295     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.3     |
|    ep_rew_mean     | 0.907    |
| time/              |          |
|    fps             | 27       |
|    iterations      | 94       |
|    time_elapsed    | 7039     |
|    total_timesteps | 192512   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 193000      |
| train/                  |             |
|    approx_kl            | 0.020096008 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.17       |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00123     |
|    n_updates            | 940         |
|    policy_gradient_loss | -0.0219     |
|    value_loss           | 0.000235    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.2     |
|    ep_rew_mean     | 0.905    |
| time/              |          |
|    fps             | 27       |
|    iterations      | 95       |
|    time_elapsed    | 7065     |
|    total_timesteps | 194560   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 195000      |
| train/                  |             |
|    approx_kl            | 0.016146714 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.05       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0156      |
|    n_updates            | 950         |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 0.000276    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 11.9     |
|    ep_rew_mean     | 0.903    |
| time/              |          |
|    fps             | 27       |
|    iterations      | 96       |
|    time_elapsed    | 7091     |
|    total_timesteps | 196608   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 197000      |
| train/                  |             |
|    approx_kl            | 0.013361757 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.88       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0305     |
|    n_updates            | 960         |
|    policy_gradient_loss | -0.0194     |
|    value_loss           | 6.18e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 10.5     |
|    ep_rew_mean     | 0.9      |
| time/              |          |
|    fps             | 27       |
|    iterations      | 97       |
|    time_elapsed    | 7118     |
|    total_timesteps | 198656   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 9          |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 199000     |
| train/                  |            |
|    approx_kl            | 0.03252077 |
|    clip_fraction        | 0.204      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.723     |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00356   |
|    n_updates            | 970        |
|    policy_gradient_loss | -0.0374    |
|    value_loss           | 5.6e-05    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.3     |
|    ep_rew_mean     | 0.902    |
| time/              |          |
|    fps             | 28       |
|    iterations      | 98       |
|    time_elapsed    | 7144     |
|    total_timesteps | 200704   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 201000      |
| train/                  |             |
|    approx_kl            | 0.019211475 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.94       |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0223      |
|    n_updates            | 980         |
|    policy_gradient_loss | -0.00766    |
|    value_loss           | 0.000409    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | 0.901    |
| time/              |          |
|    fps             | 28       |
|    iterations      | 99       |
|    time_elapsed    | 7171     |
|    total_timesteps | 202752   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 203000      |
| train/                  |             |
|    approx_kl            | 0.030818217 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.54       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00685    |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.0203     |
|    value_loss           | 0.000172    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 11.2     |
|    ep_rew_mean     | 0.903    |
| time/              |          |
|    fps             | 28       |
|    iterations      | 100      |
|    time_elapsed    | 7197     |
|    total_timesteps | 204800   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 205000      |
| train/                  |             |
|    approx_kl            | 0.024271231 |
|    clip_fraction        | 0.085       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.609      |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0248     |
|    n_updates            | 1000        |
|    policy_gradient_loss | -0.0213     |
|    value_loss           | 8.77e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.4     |
|    ep_rew_mean     | 0.902    |
| time/              |          |
|    fps             | 28       |
|    iterations      | 101      |
|    time_elapsed    | 7224     |
|    total_timesteps | 206848   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 207000      |
| train/                  |             |
|    approx_kl            | 0.017532356 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.15       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0288     |
|    n_updates            | 1010        |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 0.000288    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.48     |
|    ep_rew_mean     | 0.9      |
| time/              |          |
|    fps             | 28       |
|    iterations      | 102      |
|    time_elapsed    | 7250     |
|    total_timesteps | 208896   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 209000      |
| train/                  |             |
|    approx_kl            | 0.013520597 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.95       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0268     |
|    n_updates            | 1020        |
|    policy_gradient_loss | -0.00986    |
|    value_loss           | 0.000203    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.59     |
|    ep_rew_mean     | 0.908    |
| time/              |          |
|    fps             | 28       |
|    iterations      | 103      |
|    time_elapsed    | 7276     |
|    total_timesteps | 210944   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 211000      |
| train/                  |             |
|    approx_kl            | 0.003149124 |
|    clip_fraction        | 0.0389      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.514      |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00596     |
|    n_updates            | 1030        |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 0.00127     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.48     |
|    ep_rew_mean     | 0.9      |
| time/              |          |
|    fps             | 29       |
|    iterations      | 104      |
|    time_elapsed    | 7303     |
|    total_timesteps | 212992   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 213000      |
| train/                  |             |
|    approx_kl            | 0.046274163 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.31       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00452    |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 8.96e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.76     |
|    ep_rew_mean     | 0.908    |
| time/              |          |
|    fps             | 29       |
|    iterations      | 105      |
|    time_elapsed    | 7330     |
|    total_timesteps | 215040   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 216000      |
| train/                  |             |
|    approx_kl            | 0.011520883 |
|    clip_fraction        | 0.0921      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.316      |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0231     |
|    n_updates            | 1050        |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 0.00179     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.47     |
|    ep_rew_mean     | 0.905    |
| time/              |          |
|    fps             | 29       |
|    iterations      | 106      |
|    time_elapsed    | 7356     |
|    total_timesteps | 217088   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 9          |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 218000     |
| train/                  |            |
|    approx_kl            | 0.01658342 |
|    clip_fraction        | 0.0464     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.29      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00132   |
|    n_updates            | 1060       |
|    policy_gradient_loss | -0.013     |
|    value_loss           | 0.000171   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.96     |
|    ep_rew_mean     | 0.902    |
| time/              |          |
|    fps             | 29       |
|    iterations      | 107      |
|    time_elapsed    | 7383     |
|    total_timesteps | 219136   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 220000      |
| train/                  |             |
|    approx_kl            | 0.035836115 |
|    clip_fraction        | 0.0514      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.322      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0374     |
|    n_updates            | 1070        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 6.15e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.6     |
|    ep_rew_mean     | 0.905    |
| time/              |          |
|    fps             | 29       |
|    iterations      | 108      |
|    time_elapsed    | 7467     |
|    total_timesteps | 221184   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 222000      |
| train/                  |             |
|    approx_kl            | 0.050797015 |
|    clip_fraction        | 0.314       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0223     |
|    n_updates            | 1080        |
|    policy_gradient_loss | -0.0232     |
|    value_loss           | 0.000418    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 12       |
|    ep_rew_mean     | 0.908    |
| time/              |          |
|    fps             | 29       |
|    iterations      | 109      |
|    time_elapsed    | 7551     |
|    total_timesteps | 223232   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 224000      |
| train/                  |             |
|    approx_kl            | 0.026415413 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0201     |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.00523     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 10.6     |
|    ep_rew_mean     | 0.903    |
| time/              |          |
|    fps             | 29       |
|    iterations      | 110      |
|    time_elapsed    | 7577     |
|    total_timesteps | 225280   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 9          |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 226000     |
| train/                  |            |
|    approx_kl            | 0.02018755 |
|    clip_fraction        | 0.135      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.485     |
|    explained_variance   | 0.954      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0469    |
|    n_updates            | 1100       |
|    policy_gradient_loss | -0.0195    |
|    value_loss           | 0.00197    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 10.4     |
|    ep_rew_mean     | 0.902    |
| time/              |          |
|    fps             | 29       |
|    iterations      | 111      |
|    time_elapsed    | 7603     |
|    total_timesteps | 227328   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 228000      |
| train/                  |             |
|    approx_kl            | 0.021529388 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.544      |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00904    |
|    n_updates            | 1110        |
|    policy_gradient_loss | -0.0254     |
|    value_loss           | 0.000282    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 10.9     |
|    ep_rew_mean     | 0.903    |
| time/              |          |
|    fps             | 30       |
|    iterations      | 112      |
|    time_elapsed    | 7630     |
|    total_timesteps | 229376   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 230000      |
| train/                  |             |
|    approx_kl            | 0.022549344 |
|    clip_fraction        | 0.302       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.999      |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0452     |
|    n_updates            | 1120        |
|    policy_gradient_loss | -0.0206     |
|    value_loss           | 0.000333    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.2     |
|    ep_rew_mean     | 0.903    |
| time/              |          |
|    fps             | 30       |
|    iterations      | 113      |
|    time_elapsed    | 7657     |
|    total_timesteps | 231424   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 232000      |
| train/                  |             |
|    approx_kl            | 0.040046852 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.93       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0545     |
|    n_updates            | 1130        |
|    policy_gradient_loss | -0.019      |
|    value_loss           | 5.19e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.37     |
|    ep_rew_mean     | 0.9      |
| time/              |          |
|    fps             | 30       |
|    iterations      | 114      |
|    time_elapsed    | 7679     |
|    total_timesteps | 233472   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 234000      |
| train/                  |             |
|    approx_kl            | 0.045559935 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.46       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0731     |
|    n_updates            | 1140        |
|    policy_gradient_loss | -0.0187     |
|    value_loss           | 0.000111    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.3      |
|    ep_rew_mean     | 0.9      |
| time/              |          |
|    fps             | 30       |
|    iterations      | 115      |
|    time_elapsed    | 7702     |
|    total_timesteps | 235520   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 236000      |
| train/                  |             |
|    approx_kl            | 0.015461873 |
|    clip_fraction        | 0.0903      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.41       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00787     |
|    n_updates            | 1150        |
|    policy_gradient_loss | -0.00669    |
|    value_loss           | 4.83e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 237000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.33     |
|    ep_rew_mean     | 0.9      |
| time/              |          |
|    fps             | 30       |
|    iterations      | 116      |
|    time_elapsed    | 7726     |
|    total_timesteps | 237568   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 238000      |
| train/                  |             |
|    approx_kl            | 0.012317926 |
|    clip_fraction        | 0.0931      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00336     |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.00545    |
|    value_loss           | 2.31e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 239000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.32     |
|    ep_rew_mean     | 0.901    |
| time/              |          |
|    fps             | 30       |
|    iterations      | 117      |
|    time_elapsed    | 7749     |
|    total_timesteps | 239616   |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 9            |
|    mean_reward          | 0.9          |
| time/                   |              |
|    total_timesteps      | 240000       |
| train/                  |              |
|    approx_kl            | 0.0030914582 |
|    clip_fraction        | 0.0223       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.12        |
|    explained_variance   | 0.973        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0293      |
|    n_updates            | 1170         |
|    policy_gradient_loss | -0.0102      |
|    value_loss           | 0.00181      |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 241000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.1     |
|    ep_rew_mean     | 0.904    |
| time/              |          |
|    fps             | 31       |
|    iterations      | 118      |
|    time_elapsed    | 7772     |
|    total_timesteps | 241664   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 242000      |
| train/                  |             |
|    approx_kl            | 0.027939584 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.45       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0416     |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 0.00021     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 243000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 13.8     |
|    ep_rew_mean     | 0.906    |
| time/              |          |
|    fps             | 31       |
|    iterations      | 119      |
|    time_elapsed    | 7795     |
|    total_timesteps | 243712   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 244000      |
| train/                  |             |
|    approx_kl            | 0.011608718 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00302    |
|    n_updates            | 1190        |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 0.00114     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.21     |
|    ep_rew_mean     | 0.906    |
| time/              |          |
|    fps             | 31       |
|    iterations      | 120      |
|    time_elapsed    | 7818     |
|    total_timesteps | 245760   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 246000      |
| train/                  |             |
|    approx_kl            | 0.020356826 |
|    clip_fraction        | 0.0563      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.186      |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000772   |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.0129     |
|    value_loss           | 0.000776    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 247000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 10.2     |
|    ep_rew_mean     | 0.903    |
| time/              |          |
|    fps             | 31       |
|    iterations      | 121      |
|    time_elapsed    | 7841     |
|    total_timesteps | 247808   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 248000      |
| train/                  |             |
|    approx_kl            | 0.032870516 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.449      |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00237    |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.0242     |
|    value_loss           | 0.00183     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 12.4     |
|    ep_rew_mean     | 0.908    |
| time/              |          |
|    fps             | 31       |
|    iterations      | 122      |
|    time_elapsed    | 7865     |
|    total_timesteps | 249856   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.048865624 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.852      |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0251     |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 0.003       |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 10.3     |
|    ep_rew_mean     | 0.901    |
| time/              |          |
|    fps             | 31       |
|    iterations      | 123      |
|    time_elapsed    | 7886     |
|    total_timesteps | 251904   |
---------------------------------
{'reward': [0.9000000134110451, 0.9000000134110451, 0.9000000134110451, 0.9000000134110451, 0.9000000134110451, 0.9000000134110451, 0.9000000134110451, 0.9000000134110451, 0.9000000134110451, 0.9000000134110451], 'std': [9, 9, 9, 9, 9, 9, 9, 9, 9, 9]}
