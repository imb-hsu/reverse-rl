Logging to ./Logging/PPO_Forward_Baseline_25_300
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
-----------------------------
| time/              |      |
|    fps             | 10   |
|    iterations      | 1    |
|    time_elapsed    | 204  |
|    total_timesteps | 2048 |
-----------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 3000        |
| train/                  |             |
|    approx_kl            | 0.017860513 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.37       |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0365     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.031      |
|    value_loss           | 0.00376     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
-----------------------------
| time/              |      |
|    fps             | 10   |
|    iterations      | 2    |
|    time_elapsed    | 409  |
|    total_timesteps | 4096 |
-----------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.021654151 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.37       |
|    explained_variance   | 0.843       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0319     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0378     |
|    value_loss           | 0.0275      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
-----------------------------
| time/              |      |
|    fps             | 9    |
|    iterations      | 3    |
|    time_elapsed    | 614  |
|    total_timesteps | 6144 |
-----------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 7000        |
| train/                  |             |
|    approx_kl            | 0.028875608 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.35       |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0681     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0401     |
|    value_loss           | 0.0136      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
-----------------------------
| time/              |      |
|    fps             | 9    |
|    iterations      | 4    |
|    time_elapsed    | 820  |
|    total_timesteps | 8192 |
-----------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 9000        |
| train/                  |             |
|    approx_kl            | 0.026927277 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.33       |
|    explained_variance   | 0.889       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0688     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0397     |
|    value_loss           | 0.00214     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 5     |
|    time_elapsed    | 1025  |
|    total_timesteps | 10240 |
------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 11000      |
| train/                  |            |
|    approx_kl            | 0.02041135 |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.31      |
|    explained_variance   | 0.868      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00129   |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0314    |
|    value_loss           | 0.0017     |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 6     |
|    time_elapsed    | 1230  |
|    total_timesteps | 12288 |
------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 13000      |
| train/                  |            |
|    approx_kl            | 0.02030178 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.29      |
|    explained_variance   | 0.833      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0532    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0387    |
|    value_loss           | 0.00747    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 7     |
|    time_elapsed    | 1436  |
|    total_timesteps | 14336 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.021669488 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.31       |
|    explained_variance   | 0.882       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0823     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0439     |
|    value_loss           | 0.00673     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 8     |
|    time_elapsed    | 1641  |
|    total_timesteps | 16384 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 17000       |
| train/                  |             |
|    approx_kl            | 0.024886813 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.22       |
|    explained_variance   | 0.881       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0306     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0327     |
|    value_loss           | 0.00368     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 9     |
|    time_elapsed    | 1846  |
|    total_timesteps | 18432 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 19000       |
| train/                  |             |
|    approx_kl            | 0.021486446 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.31       |
|    explained_variance   | 0.851       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.058      |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0411     |
|    value_loss           | 0.0068      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 10    |
|    time_elapsed    | 2052  |
|    total_timesteps | 20480 |
------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 21000      |
| train/                  |            |
|    approx_kl            | 0.02139051 |
|    clip_fraction        | 0.211      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.27      |
|    explained_variance   | 0.698      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0629    |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0334    |
|    value_loss           | 0.00176    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 11    |
|    time_elapsed    | 2257  |
|    total_timesteps | 22528 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 23000       |
| train/                  |             |
|    approx_kl            | 0.017709881 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.27       |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0473     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0342     |
|    value_loss           | 0.00267     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 12    |
|    time_elapsed    | 2462  |
|    total_timesteps | 24576 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.022105634 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.28       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0366     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0316     |
|    value_loss           | 0.00286     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 13    |
|    time_elapsed    | 2668  |
|    total_timesteps | 26624 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 27000       |
| train/                  |             |
|    approx_kl            | 0.017648097 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.25       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0626     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0378     |
|    value_loss           | 0.00121     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 14    |
|    time_elapsed    | 2873  |
|    total_timesteps | 28672 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 29000       |
| train/                  |             |
|    approx_kl            | 0.021555573 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.18       |
|    explained_variance   | 0.822       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0263     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0345     |
|    value_loss           | 0.00353     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 15    |
|    time_elapsed    | 3078  |
|    total_timesteps | 30720 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 31000       |
| train/                  |             |
|    approx_kl            | 0.017962817 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.04       |
|    explained_variance   | 0.523       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0285     |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0285     |
|    value_loss           | 0.00116     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 16    |
|    time_elapsed    | 3284  |
|    total_timesteps | 32768 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 33000       |
| train/                  |             |
|    approx_kl            | 0.019489098 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.19       |
|    explained_variance   | 0.745       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0675     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.033      |
|    value_loss           | 0.000883    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 17    |
|    time_elapsed    | 3489  |
|    total_timesteps | 34816 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.023453953 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.24       |
|    explained_variance   | 0.841       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0486     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0401     |
|    value_loss           | 0.00274     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 18    |
|    time_elapsed    | 3694  |
|    total_timesteps | 36864 |
------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 37000      |
| train/                  |            |
|    approx_kl            | 0.01786478 |
|    clip_fraction        | 0.208      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.09      |
|    explained_variance   | 0.747      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00795   |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.028     |
|    value_loss           | 0.000794   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 19    |
|    time_elapsed    | 3900  |
|    total_timesteps | 38912 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 39000       |
| train/                  |             |
|    approx_kl            | 0.020382594 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.27       |
|    explained_variance   | 0.86        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0413     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0323     |
|    value_loss           | 0.00191     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 20    |
|    time_elapsed    | 4105  |
|    total_timesteps | 40960 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 41000       |
| train/                  |             |
|    approx_kl            | 0.023615928 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.14       |
|    explained_variance   | 0.776       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0712     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0383     |
|    value_loss           | 0.00179     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 21    |
|    time_elapsed    | 4410  |
|    total_timesteps | 43008 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 44000       |
| train/                  |             |
|    approx_kl            | 0.027916819 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.1        |
|    explained_variance   | 0.876       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0493     |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0411     |
|    value_loss           | 0.0036      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 22    |
|    time_elapsed    | 4616  |
|    total_timesteps | 45056 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 46000       |
| train/                  |             |
|    approx_kl            | 0.020905884 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.17       |
|    explained_variance   | 0.857       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0481     |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0339     |
|    value_loss           | 0.00139     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 23    |
|    time_elapsed    | 4821  |
|    total_timesteps | 47104 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 48000       |
| train/                  |             |
|    approx_kl            | 0.021659903 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.04       |
|    explained_variance   | 0.851       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0407     |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0348     |
|    value_loss           | 0.00154     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 24    |
|    time_elapsed    | 5026  |
|    total_timesteps | 49152 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.019779004 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.08       |
|    explained_variance   | 0.641       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0323     |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0285     |
|    value_loss           | 0.00066     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 25    |
|    time_elapsed    | 5231  |
|    total_timesteps | 51200 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 52000       |
| train/                  |             |
|    approx_kl            | 0.024993313 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.15       |
|    explained_variance   | 0.809       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0135     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0392     |
|    value_loss           | 0.00231     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 26    |
|    time_elapsed    | 5437  |
|    total_timesteps | 53248 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 54000       |
| train/                  |             |
|    approx_kl            | 0.021870295 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.04       |
|    explained_variance   | 0.864       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.063      |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.036      |
|    value_loss           | 0.00206     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 27    |
|    time_elapsed    | 5642  |
|    total_timesteps | 55296 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 56000       |
| train/                  |             |
|    approx_kl            | 0.020905059 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.09       |
|    explained_variance   | 0.824       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.021      |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0348     |
|    value_loss           | 0.00142     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 28    |
|    time_elapsed    | 5847  |
|    total_timesteps | 57344 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 58000       |
| train/                  |             |
|    approx_kl            | 0.022181569 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.95       |
|    explained_variance   | 0.818       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.029      |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.034      |
|    value_loss           | 0.00111     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 29    |
|    time_elapsed    | 6053  |
|    total_timesteps | 59392 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.018690478 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.85       |
|    explained_variance   | 0.575       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0155     |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0298     |
|    value_loss           | 0.00061     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 30    |
|    time_elapsed    | 6258  |
|    total_timesteps | 61440 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 62000       |
| train/                  |             |
|    approx_kl            | 0.024057515 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.92       |
|    explained_variance   | 0.566       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0087      |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.0293     |
|    value_loss           | 0.00162     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 31    |
|    time_elapsed    | 6463  |
|    total_timesteps | 63488 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 64000       |
| train/                  |             |
|    approx_kl            | 0.019917209 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -8          |
|    explained_variance   | 0.867       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0567     |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.0396     |
|    value_loss           | 0.00156     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 32    |
|    time_elapsed    | 6669  |
|    total_timesteps | 65536 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 66000       |
| train/                  |             |
|    approx_kl            | 0.019571815 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.99       |
|    explained_variance   | 0.804       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0109     |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0363     |
|    value_loss           | 0.00219     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 33    |
|    time_elapsed    | 6874  |
|    total_timesteps | 67584 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 68000       |
| train/                  |             |
|    approx_kl            | 0.019108508 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.76       |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0525     |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.0278     |
|    value_loss           | 0.000773    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 34    |
|    time_elapsed    | 7079  |
|    total_timesteps | 69632 |
------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 70000      |
| train/                  |            |
|    approx_kl            | 0.02536185 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.84      |
|    explained_variance   | 0.822      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0569    |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.0267    |
|    value_loss           | 0.000798   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 35    |
|    time_elapsed    | 7285  |
|    total_timesteps | 71680 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 72000       |
| train/                  |             |
|    approx_kl            | 0.022382293 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.96       |
|    explained_variance   | 0.842       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0438     |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.0378     |
|    value_loss           | 0.00363     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 36    |
|    time_elapsed    | 7490  |
|    total_timesteps | 73728 |
------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 74000      |
| train/                  |            |
|    approx_kl            | 0.01966258 |
|    clip_fraction        | 0.207      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.81      |
|    explained_variance   | 0.813      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0581    |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.035     |
|    value_loss           | 0.00221    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 37    |
|    time_elapsed    | 7695  |
|    total_timesteps | 75776 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 76000       |
| train/                  |             |
|    approx_kl            | 0.022115732 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.55       |
|    explained_variance   | 0.585       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0462     |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.0268     |
|    value_loss           | 0.000407    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 38    |
|    time_elapsed    | 7901  |
|    total_timesteps | 77824 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 78000       |
| train/                  |             |
|    approx_kl            | 0.026721094 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.44       |
|    explained_variance   | 0.842       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0533     |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.0317     |
|    value_loss           | 0.000978    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 39    |
|    time_elapsed    | 8106  |
|    total_timesteps | 79872 |
------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 80000      |
| train/                  |            |
|    approx_kl            | 0.01848581 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.8       |
|    explained_variance   | 0.915      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0206    |
|    n_updates            | 390        |
|    policy_gradient_loss | -0.0319    |
|    value_loss           | 0.00103    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 40    |
|    time_elapsed    | 8311  |
|    total_timesteps | 81920 |
------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 82000      |
| train/                  |            |
|    approx_kl            | 0.02352736 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.79      |
|    explained_variance   | 0.837      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0605    |
|    n_updates            | 400        |
|    policy_gradient_loss | -0.0383    |
|    value_loss           | 0.00217    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 41    |
|    time_elapsed    | 8516  |
|    total_timesteps | 83968 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 84000       |
| train/                  |             |
|    approx_kl            | 0.022525588 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.79       |
|    explained_variance   | 0.592       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0422     |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.0426     |
|    value_loss           | 0.00155     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 42    |
|    time_elapsed    | 8822  |
|    total_timesteps | 86016 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 87000       |
| train/                  |             |
|    approx_kl            | 0.020509921 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.71       |
|    explained_variance   | 0.769       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0451     |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0388     |
|    value_loss           | 0.000689    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 43    |
|    time_elapsed    | 9027  |
|    total_timesteps | 88064 |
------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+04     |
|    mean_reward          | 0         |
| time/                   |           |
|    total_timesteps      | 89000     |
| train/                  |           |
|    approx_kl            | 0.0198518 |
|    clip_fraction        | 0.228     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.64     |
|    explained_variance   | 0.751     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0434   |
|    n_updates            | 430       |
|    policy_gradient_loss | -0.0365   |
|    value_loss           | 0.00168   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 44    |
|    time_elapsed    | 9232  |
|    total_timesteps | 90112 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 91000       |
| train/                  |             |
|    approx_kl            | 0.022616152 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.87       |
|    explained_variance   | 0.887       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0809     |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.0318     |
|    value_loss           | 0.00404     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 45    |
|    time_elapsed    | 9438  |
|    total_timesteps | 92160 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 93000       |
| train/                  |             |
|    approx_kl            | 0.024265142 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.53       |
|    explained_variance   | 0.622       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0104     |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.0314     |
|    value_loss           | 0.0007      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 46    |
|    time_elapsed    | 9643  |
|    total_timesteps | 94208 |
------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 95000      |
| train/                  |            |
|    approx_kl            | 0.02110026 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.62      |
|    explained_variance   | 0.662      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0591    |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.0366    |
|    value_loss           | 0.00165    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 47    |
|    time_elapsed    | 9848  |
|    total_timesteps | 96256 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 97000       |
| train/                  |             |
|    approx_kl            | 0.023557892 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.9        |
|    explained_variance   | 0.716       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0341     |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.0335     |
|    value_loss           | 0.00265     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 48    |
|    time_elapsed    | 10054 |
|    total_timesteps | 98304 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 99000       |
| train/                  |             |
|    approx_kl            | 0.021037206 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.65       |
|    explained_variance   | 0.754       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0116     |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.0341     |
|    value_loss           | 0.000885    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 49     |
|    time_elapsed    | 10259  |
|    total_timesteps | 100352 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 101000     |
| train/                  |            |
|    approx_kl            | 0.02361929 |
|    clip_fraction        | 0.277      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.78      |
|    explained_variance   | 0.788      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0262    |
|    n_updates            | 490        |
|    policy_gradient_loss | -0.0431    |
|    value_loss           | 0.00182    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 50     |
|    time_elapsed    | 10464  |
|    total_timesteps | 102400 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 103000      |
| train/                  |             |
|    approx_kl            | 0.019947179 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.73       |
|    explained_variance   | 0.777       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00729    |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.0251     |
|    value_loss           | 0.00131     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 51     |
|    time_elapsed    | 10670  |
|    total_timesteps | 104448 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 105000      |
| train/                  |             |
|    approx_kl            | 0.025651114 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.76       |
|    explained_variance   | 0.661       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0501     |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.0401     |
|    value_loss           | 0.000777    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 52     |
|    time_elapsed    | 10875  |
|    total_timesteps | 106496 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 107000     |
| train/                  |            |
|    approx_kl            | 0.01931663 |
|    clip_fraction        | 0.234      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.72      |
|    explained_variance   | 0.77       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0388    |
|    n_updates            | 520        |
|    policy_gradient_loss | -0.038     |
|    value_loss           | 0.00122    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 53     |
|    time_elapsed    | 11080  |
|    total_timesteps | 108544 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 109000      |
| train/                  |             |
|    approx_kl            | 0.019943574 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.64       |
|    explained_variance   | 0.857       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0796     |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.0407     |
|    value_loss           | 0.000963    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 54     |
|    time_elapsed    | 11285  |
|    total_timesteps | 110592 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 111000      |
| train/                  |             |
|    approx_kl            | 0.021869825 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.81       |
|    explained_variance   | 0.871       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0325     |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.0348     |
|    value_loss           | 0.00251     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 55     |
|    time_elapsed    | 11491  |
|    total_timesteps | 112640 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 113000     |
| train/                  |            |
|    approx_kl            | 0.02096783 |
|    clip_fraction        | 0.242      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.93      |
|    explained_variance   | 0.848      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0556    |
|    n_updates            | 550        |
|    policy_gradient_loss | -0.0324    |
|    value_loss           | 0.0012     |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 56     |
|    time_elapsed    | 11696  |
|    total_timesteps | 114688 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 115000      |
| train/                  |             |
|    approx_kl            | 0.020069972 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.9        |
|    explained_variance   | 0.745       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0491     |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.0335     |
|    value_loss           | 0.00386     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 57     |
|    time_elapsed    | 11901  |
|    total_timesteps | 116736 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 117000     |
| train/                  |            |
|    approx_kl            | 0.01893165 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.7       |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0523    |
|    n_updates            | 570        |
|    policy_gradient_loss | -0.027     |
|    value_loss           | 0.000901   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 58     |
|    time_elapsed    | 12107  |
|    total_timesteps | 118784 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 119000      |
| train/                  |             |
|    approx_kl            | 0.021867875 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.78       |
|    explained_variance   | 0.786       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0229     |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.0321     |
|    value_loss           | 0.000869    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 59     |
|    time_elapsed    | 12312  |
|    total_timesteps | 120832 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 121000      |
| train/                  |             |
|    approx_kl            | 0.022024479 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.81       |
|    explained_variance   | 0.756       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0205     |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.0341     |
|    value_loss           | 0.000542    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 60     |
|    time_elapsed    | 12517  |
|    total_timesteps | 122880 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 123000      |
| train/                  |             |
|    approx_kl            | 0.019914787 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.75       |
|    explained_variance   | 0.585       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0199     |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.0286     |
|    value_loss           | 0.000753    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 61     |
|    time_elapsed    | 12723  |
|    total_timesteps | 124928 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 125000      |
| train/                  |             |
|    approx_kl            | 0.018158272 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.73       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0774     |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.0363     |
|    value_loss           | 0.000812    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 62     |
|    time_elapsed    | 12928  |
|    total_timesteps | 126976 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 127000      |
| train/                  |             |
|    approx_kl            | 0.019678004 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.59       |
|    explained_variance   | 0.78        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0538     |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.0342     |
|    value_loss           | 0.00113     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 63     |
|    time_elapsed    | 13233  |
|    total_timesteps | 129024 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 130000     |
| train/                  |            |
|    approx_kl            | 0.02031882 |
|    clip_fraction        | 0.248      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.71      |
|    explained_variance   | 0.76       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0263    |
|    n_updates            | 630        |
|    policy_gradient_loss | -0.0325    |
|    value_loss           | 0.000843   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 64     |
|    time_elapsed    | 13439  |
|    total_timesteps | 131072 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 132000      |
| train/                  |             |
|    approx_kl            | 0.019955507 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.78       |
|    explained_variance   | 0.0399      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0461     |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.0311     |
|    value_loss           | 0.000441    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 65     |
|    time_elapsed    | 13644  |
|    total_timesteps | 133120 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 134000      |
| train/                  |             |
|    approx_kl            | 0.020071788 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.79       |
|    explained_variance   | 0.535       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0454     |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.0307     |
|    value_loss           | 0.000679    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 66     |
|    time_elapsed    | 13849  |
|    total_timesteps | 135168 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 136000      |
| train/                  |             |
|    approx_kl            | 0.024668545 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.65       |
|    explained_variance   | 0.713       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0446     |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.0353     |
|    value_loss           | 0.000906    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 67     |
|    time_elapsed    | 14055  |
|    total_timesteps | 137216 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 138000      |
| train/                  |             |
|    approx_kl            | 0.022038076 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.6        |
|    explained_variance   | 0.613       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0611     |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.034      |
|    value_loss           | 0.000628    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 68     |
|    time_elapsed    | 14260  |
|    total_timesteps | 139264 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 140000      |
| train/                  |             |
|    approx_kl            | 0.021462632 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.7        |
|    explained_variance   | 0.484       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0711     |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.0301     |
|    value_loss           | 0.000659    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 69     |
|    time_elapsed    | 14465  |
|    total_timesteps | 141312 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 142000      |
| train/                  |             |
|    approx_kl            | 0.021934804 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.73       |
|    explained_variance   | 0.764       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.047      |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.0366     |
|    value_loss           | 0.00131     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 70     |
|    time_elapsed    | 14670  |
|    total_timesteps | 143360 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 144000     |
| train/                  |            |
|    approx_kl            | 0.02089141 |
|    clip_fraction        | 0.24       |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.7       |
|    explained_variance   | 0.816      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0325    |
|    n_updates            | 700        |
|    policy_gradient_loss | -0.026     |
|    value_loss           | 0.000897   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 71     |
|    time_elapsed    | 14876  |
|    total_timesteps | 145408 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 146000      |
| train/                  |             |
|    approx_kl            | 0.024491988 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.78       |
|    explained_variance   | 0.834       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0712     |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.0348     |
|    value_loss           | 0.0022      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 72     |
|    time_elapsed    | 15081  |
|    total_timesteps | 147456 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 148000      |
| train/                  |             |
|    approx_kl            | 0.023581725 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.77       |
|    explained_variance   | 0.74        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0524     |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.0363     |
|    value_loss           | 0.00137     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 73     |
|    time_elapsed    | 15286  |
|    total_timesteps | 149504 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.020916414 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.62       |
|    explained_variance   | 0.832       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0576     |
|    n_updates            | 730         |
|    policy_gradient_loss | -0.0346     |
|    value_loss           | 0.00138     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 151000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 74     |
|    time_elapsed    | 15492  |
|    total_timesteps | 151552 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 152000      |
| train/                  |             |
|    approx_kl            | 0.022289403 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.73       |
|    explained_variance   | 0.652       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0132     |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.0356     |
|    value_loss           | 0.000752    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 153000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 75     |
|    time_elapsed    | 15697  |
|    total_timesteps | 153600 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 154000     |
| train/                  |            |
|    approx_kl            | 0.02180747 |
|    clip_fraction        | 0.241      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.72      |
|    explained_variance   | 0.853      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0487    |
|    n_updates            | 750        |
|    policy_gradient_loss | -0.0358    |
|    value_loss           | 0.001      |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 76     |
|    time_elapsed    | 15902  |
|    total_timesteps | 155648 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 156000      |
| train/                  |             |
|    approx_kl            | 0.019911444 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.66       |
|    explained_variance   | 0.724       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0654     |
|    n_updates            | 760         |
|    policy_gradient_loss | -0.0337     |
|    value_loss           | 0.00136     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 77     |
|    time_elapsed    | 16108  |
|    total_timesteps | 157696 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 158000     |
| train/                  |            |
|    approx_kl            | 0.02249361 |
|    clip_fraction        | 0.255      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.66      |
|    explained_variance   | 0.599      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0574    |
|    n_updates            | 770        |
|    policy_gradient_loss | -0.0333    |
|    value_loss           | 0.0012     |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 159000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 78     |
|    time_elapsed    | 16313  |
|    total_timesteps | 159744 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 160000      |
| train/                  |             |
|    approx_kl            | 0.021557838 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.63       |
|    explained_variance   | 0.848       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0663     |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.034      |
|    value_loss           | 0.00106     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 161000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 79     |
|    time_elapsed    | 16518  |
|    total_timesteps | 161792 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.3        |
| time/                   |            |
|    total_timesteps      | 162000     |
| train/                  |            |
|    approx_kl            | 0.01944247 |
|    clip_fraction        | 0.233      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.59      |
|    explained_variance   | 0.736      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0354    |
|    n_updates            | 790        |
|    policy_gradient_loss | -0.0335    |
|    value_loss           | 0.000835   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 163000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 80     |
|    time_elapsed    | 16724  |
|    total_timesteps | 163840 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 164000      |
| train/                  |             |
|    approx_kl            | 0.024180416 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.61       |
|    explained_variance   | 0.864       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0544     |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.0304     |
|    value_loss           | 0.00155     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 165000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 81     |
|    time_elapsed    | 16929  |
|    total_timesteps | 165888 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.3        |
| time/                   |            |
|    total_timesteps      | 166000     |
| train/                  |            |
|    approx_kl            | 0.02199716 |
|    clip_fraction        | 0.254      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.63      |
|    explained_variance   | 0.832      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0644    |
|    n_updates            | 810        |
|    policy_gradient_loss | -0.032     |
|    value_loss           | 0.00151    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 167000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 82     |
|    time_elapsed    | 17134  |
|    total_timesteps | 167936 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 168000      |
| train/                  |             |
|    approx_kl            | 0.026869832 |
|    clip_fraction        | 0.291       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.68       |
|    explained_variance   | 0.749       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.023      |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.0333     |
|    value_loss           | 0.00134     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 169000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 83     |
|    time_elapsed    | 17340  |
|    total_timesteps | 169984 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 170000      |
| train/                  |             |
|    approx_kl            | 0.022763655 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.76       |
|    explained_variance   | 0.907       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0578     |
|    n_updates            | 830         |
|    policy_gradient_loss | -0.0302     |
|    value_loss           | 0.00126     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 84     |
|    time_elapsed    | 17645  |
|    total_timesteps | 172032 |
-------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+04     |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 173000    |
| train/                  |           |
|    approx_kl            | 0.0248937 |
|    clip_fraction        | 0.286     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.65     |
|    explained_variance   | 0.827     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0607   |
|    n_updates            | 840       |
|    policy_gradient_loss | -0.0441   |
|    value_loss           | 0.00208   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 85     |
|    time_elapsed    | 17850  |
|    total_timesteps | 174080 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 175000      |
| train/                  |             |
|    approx_kl            | 0.025067292 |
|    clip_fraction        | 0.27        |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.62       |
|    explained_variance   | 0.764       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00761    |
|    n_updates            | 850         |
|    policy_gradient_loss | -0.0333     |
|    value_loss           | 0.0011      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 86     |
|    time_elapsed    | 18056  |
|    total_timesteps | 176128 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 177000      |
| train/                  |             |
|    approx_kl            | 0.023942692 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.48       |
|    explained_variance   | 0.747       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0239     |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.0325     |
|    value_loss           | 0.00222     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 87     |
|    time_elapsed    | 18261  |
|    total_timesteps | 178176 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 179000      |
| train/                  |             |
|    approx_kl            | 0.022023583 |
|    clip_fraction        | 0.27        |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.55       |
|    explained_variance   | 0.742       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0609     |
|    n_updates            | 870         |
|    policy_gradient_loss | -0.0356     |
|    value_loss           | 0.00185     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 88     |
|    time_elapsed    | 18466  |
|    total_timesteps | 180224 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 181000      |
| train/                  |             |
|    approx_kl            | 0.020392079 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.59       |
|    explained_variance   | 0.478       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.048      |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.0298     |
|    value_loss           | 0.00114     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 89     |
|    time_elapsed    | 18672  |
|    total_timesteps | 182272 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.4        |
| time/                   |            |
|    total_timesteps      | 183000     |
| train/                  |            |
|    approx_kl            | 0.02251942 |
|    clip_fraction        | 0.258      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.59      |
|    explained_variance   | 0.652      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0189    |
|    n_updates            | 890        |
|    policy_gradient_loss | -0.0257    |
|    value_loss           | 0.00133    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 90     |
|    time_elapsed    | 18877  |
|    total_timesteps | 184320 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.3        |
| time/                   |            |
|    total_timesteps      | 185000     |
| train/                  |            |
|    approx_kl            | 0.03084773 |
|    clip_fraction        | 0.314      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.63      |
|    explained_variance   | 0.793      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0363    |
|    n_updates            | 900        |
|    policy_gradient_loss | -0.0309    |
|    value_loss           | 0.000843   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 91     |
|    time_elapsed    | 19082  |
|    total_timesteps | 186368 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 187000      |
| train/                  |             |
|    approx_kl            | 0.022803511 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.58       |
|    explained_variance   | 0.629       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0266     |
|    n_updates            | 910         |
|    policy_gradient_loss | -0.0318     |
|    value_loss           | 0.00136     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 92     |
|    time_elapsed    | 19287  |
|    total_timesteps | 188416 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 189000      |
| train/                  |             |
|    approx_kl            | 0.023198286 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.57       |
|    explained_variance   | 0.711       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0437     |
|    n_updates            | 920         |
|    policy_gradient_loss | -0.0321     |
|    value_loss           | 0.0015      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 93     |
|    time_elapsed    | 19493  |
|    total_timesteps | 190464 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 191000      |
| train/                  |             |
|    approx_kl            | 0.020713018 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.52       |
|    explained_variance   | 0.6         |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0377     |
|    n_updates            | 930         |
|    policy_gradient_loss | -0.0309     |
|    value_loss           | 0.00126     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 94     |
|    time_elapsed    | 19698  |
|    total_timesteps | 192512 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.3        |
| time/                   |            |
|    total_timesteps      | 193000     |
| train/                  |            |
|    approx_kl            | 0.02049398 |
|    clip_fraction        | 0.216      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.54      |
|    explained_variance   | 0.81       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0222    |
|    n_updates            | 940        |
|    policy_gradient_loss | -0.0258    |
|    value_loss           | 0.00118    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 95     |
|    time_elapsed    | 19903  |
|    total_timesteps | 194560 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 195000      |
| train/                  |             |
|    approx_kl            | 0.022717863 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.49       |
|    explained_variance   | 0.823       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0251     |
|    n_updates            | 950         |
|    policy_gradient_loss | -0.0365     |
|    value_loss           | 0.00155     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 96     |
|    time_elapsed    | 20109  |
|    total_timesteps | 196608 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 197000      |
| train/                  |             |
|    approx_kl            | 0.021754855 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.5        |
|    explained_variance   | 0.783       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00703    |
|    n_updates            | 960         |
|    policy_gradient_loss | -0.0294     |
|    value_loss           | 0.00177     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 97     |
|    time_elapsed    | 20314  |
|    total_timesteps | 198656 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 199000      |
| train/                  |             |
|    approx_kl            | 0.022222184 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.34       |
|    explained_variance   | 0.718       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0238     |
|    n_updates            | 970         |
|    policy_gradient_loss | -0.0222     |
|    value_loss           | 0.000878    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 98     |
|    time_elapsed    | 20519  |
|    total_timesteps | 200704 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.5        |
| time/                   |            |
|    total_timesteps      | 201000     |
| train/                  |            |
|    approx_kl            | 0.01913874 |
|    clip_fraction        | 0.228      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.43      |
|    explained_variance   | 0.77       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0357    |
|    n_updates            | 980        |
|    policy_gradient_loss | -0.024     |
|    value_loss           | 0.00135    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 99     |
|    time_elapsed    | 20725  |
|    total_timesteps | 202752 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.5        |
| time/                   |            |
|    total_timesteps      | 203000     |
| train/                  |            |
|    approx_kl            | 0.02167948 |
|    clip_fraction        | 0.235      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.47      |
|    explained_variance   | 0.615      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0404    |
|    n_updates            | 990        |
|    policy_gradient_loss | -0.0297    |
|    value_loss           | 0.00179    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 100    |
|    time_elapsed    | 20930  |
|    total_timesteps | 204800 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 205000      |
| train/                  |             |
|    approx_kl            | 0.024255432 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.35       |
|    explained_variance   | 0.723       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0309     |
|    n_updates            | 1000        |
|    policy_gradient_loss | -0.0329     |
|    value_loss           | 0.00163     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 101    |
|    time_elapsed    | 21135  |
|    total_timesteps | 206848 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.5        |
| time/                   |            |
|    total_timesteps      | 207000     |
| train/                  |            |
|    approx_kl            | 0.02053731 |
|    clip_fraction        | 0.256      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.42      |
|    explained_variance   | 0.46       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.047     |
|    n_updates            | 1010       |
|    policy_gradient_loss | -0.0322    |
|    value_loss           | 0.00112    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 102    |
|    time_elapsed    | 21341  |
|    total_timesteps | 208896 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 209000      |
| train/                  |             |
|    approx_kl            | 0.020464342 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.42       |
|    explained_variance   | 0.629       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0351     |
|    n_updates            | 1020        |
|    policy_gradient_loss | -0.032      |
|    value_loss           | 0.00131     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 103    |
|    time_elapsed    | 21546  |
|    total_timesteps | 210944 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 211000      |
| train/                  |             |
|    approx_kl            | 0.020505501 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.29       |
|    explained_variance   | 0.542       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0548     |
|    n_updates            | 1030        |
|    policy_gradient_loss | -0.0301     |
|    value_loss           | 0.00108     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 104    |
|    time_elapsed    | 21751  |
|    total_timesteps | 212992 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 213000      |
| train/                  |             |
|    approx_kl            | 0.022678198 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.23       |
|    explained_variance   | 0.614       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0399     |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.0355     |
|    value_loss           | 0.00123     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 105    |
|    time_elapsed    | 22057  |
|    total_timesteps | 215040 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 216000      |
| train/                  |             |
|    approx_kl            | 0.020573344 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.16       |
|    explained_variance   | 0.357       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0567     |
|    n_updates            | 1050        |
|    policy_gradient_loss | -0.0317     |
|    value_loss           | 0.0018      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 106    |
|    time_elapsed    | 22262  |
|    total_timesteps | 217088 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 218000      |
| train/                  |             |
|    approx_kl            | 0.023455907 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.33       |
|    explained_variance   | 0.399       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0419     |
|    n_updates            | 1060        |
|    policy_gradient_loss | -0.0348     |
|    value_loss           | 0.00155     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 107    |
|    time_elapsed    | 22467  |
|    total_timesteps | 219136 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 220000      |
| train/                  |             |
|    approx_kl            | 0.017558929 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.13       |
|    explained_variance   | 0.552       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0239     |
|    n_updates            | 1070        |
|    policy_gradient_loss | -0.0265     |
|    value_loss           | 0.00136     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 108    |
|    time_elapsed    | 22673  |
|    total_timesteps | 221184 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 222000      |
| train/                  |             |
|    approx_kl            | 0.019627739 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.13       |
|    explained_variance   | 0.365       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0273     |
|    n_updates            | 1080        |
|    policy_gradient_loss | -0.0219     |
|    value_loss           | 0.00178     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 109    |
|    time_elapsed    | 22878  |
|    total_timesteps | 223232 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 224000      |
| train/                  |             |
|    approx_kl            | 0.021993462 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.06       |
|    explained_variance   | 0.687       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0547     |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.0319     |
|    value_loss           | 0.00156     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 110    |
|    time_elapsed    | 23083  |
|    total_timesteps | 225280 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 226000      |
| train/                  |             |
|    approx_kl            | 0.019417927 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.01       |
|    explained_variance   | 0.497       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0427     |
|    n_updates            | 1100        |
|    policy_gradient_loss | -0.0257     |
|    value_loss           | 0.00175     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 111    |
|    time_elapsed    | 23289  |
|    total_timesteps | 227328 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 228000      |
| train/                  |             |
|    approx_kl            | 0.021946393 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.16       |
|    explained_variance   | 0.471       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0429     |
|    n_updates            | 1110        |
|    policy_gradient_loss | -0.0284     |
|    value_loss           | 0.00149     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 112    |
|    time_elapsed    | 23494  |
|    total_timesteps | 229376 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 230000      |
| train/                  |             |
|    approx_kl            | 0.018900543 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.13       |
|    explained_variance   | 0.653       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0539     |
|    n_updates            | 1120        |
|    policy_gradient_loss | -0.0261     |
|    value_loss           | 0.00212     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 113    |
|    time_elapsed    | 23699  |
|    total_timesteps | 231424 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 232000      |
| train/                  |             |
|    approx_kl            | 0.021376133 |
|    clip_fraction        | 0.276       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.03       |
|    explained_variance   | 0.374       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0334     |
|    n_updates            | 1130        |
|    policy_gradient_loss | -0.0212     |
|    value_loss           | 0.00195     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 114    |
|    time_elapsed    | 23905  |
|    total_timesteps | 233472 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 234000      |
| train/                  |             |
|    approx_kl            | 0.019111736 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.15       |
|    explained_variance   | 0.674       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0233     |
|    n_updates            | 1140        |
|    policy_gradient_loss | -0.0255     |
|    value_loss           | 0.00271     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 115    |
|    time_elapsed    | 24110  |
|    total_timesteps | 235520 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 236000      |
| train/                  |             |
|    approx_kl            | 0.019119257 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.24       |
|    explained_variance   | 0.472       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.015      |
|    n_updates            | 1150        |
|    policy_gradient_loss | -0.0257     |
|    value_loss           | 0.00189     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 237000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 116    |
|    time_elapsed    | 24315  |
|    total_timesteps | 237568 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 238000      |
| train/                  |             |
|    approx_kl            | 0.017776914 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.06       |
|    explained_variance   | 0.568       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.031      |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.0253     |
|    value_loss           | 0.00279     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 239000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 117    |
|    time_elapsed    | 24521  |
|    total_timesteps | 239616 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 0.019187517 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.91       |
|    explained_variance   | 0.41        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0307     |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.0239     |
|    value_loss           | 0.00212     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 241000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 118    |
|    time_elapsed    | 24726  |
|    total_timesteps | 241664 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 242000      |
| train/                  |             |
|    approx_kl            | 0.019076867 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.2        |
|    explained_variance   | 0.439       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0288     |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.0196     |
|    value_loss           | 0.0019      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 243000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 119    |
|    time_elapsed    | 24931  |
|    total_timesteps | 243712 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.5        |
| time/                   |            |
|    total_timesteps      | 244000     |
| train/                  |            |
|    approx_kl            | 0.02027221 |
|    clip_fraction        | 0.243      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.21      |
|    explained_variance   | 0.355      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0437    |
|    n_updates            | 1190       |
|    policy_gradient_loss | -0.019     |
|    value_loss           | 0.00221    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 120    |
|    time_elapsed    | 25137  |
|    total_timesteps | 245760 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.5        |
| time/                   |            |
|    total_timesteps      | 246000     |
| train/                  |            |
|    approx_kl            | 0.01994533 |
|    clip_fraction        | 0.25       |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.25      |
|    explained_variance   | 0.312      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00684   |
|    n_updates            | 1200       |
|    policy_gradient_loss | -0.0219    |
|    value_loss           | 0.00252    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 247000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 121    |
|    time_elapsed    | 25342  |
|    total_timesteps | 247808 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 248000      |
| train/                  |             |
|    approx_kl            | 0.016813584 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.17       |
|    explained_variance   | 0.52        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0287     |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.0213     |
|    value_loss           | 0.00279     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 122    |
|    time_elapsed    | 25547  |
|    total_timesteps | 249856 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.019561728 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.21       |
|    explained_variance   | 0.592       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0225     |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.0231     |
|    value_loss           | 0.0022      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 123    |
|    time_elapsed    | 25752  |
|    total_timesteps | 251904 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 252000      |
| train/                  |             |
|    approx_kl            | 0.026611576 |
|    clip_fraction        | 0.282       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.14       |
|    explained_variance   | 0.51        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0274     |
|    n_updates            | 1230        |
|    policy_gradient_loss | -0.0206     |
|    value_loss           | 0.00285     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 253000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 124    |
|    time_elapsed    | 25958  |
|    total_timesteps | 253952 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 254000      |
| train/                  |             |
|    approx_kl            | 0.018006051 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.99       |
|    explained_variance   | 0.361       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0324     |
|    n_updates            | 1240        |
|    policy_gradient_loss | -0.0184     |
|    value_loss           | 0.00266     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 255000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 125    |
|    time_elapsed    | 26263  |
|    total_timesteps | 256000 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.6        |
| time/                   |            |
|    total_timesteps      | 257000     |
| train/                  |            |
|    approx_kl            | 0.01942119 |
|    clip_fraction        | 0.227      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.97      |
|    explained_variance   | 0.769      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0213    |
|    n_updates            | 1250       |
|    policy_gradient_loss | -0.0245    |
|    value_loss           | 0.00283    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 126    |
|    time_elapsed    | 26468  |
|    total_timesteps | 258048 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 259000      |
| train/                  |             |
|    approx_kl            | 0.020146223 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.85       |
|    explained_variance   | 0.489       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0488     |
|    n_updates            | 1260        |
|    policy_gradient_loss | -0.0252     |
|    value_loss           | 0.00274     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 127    |
|    time_elapsed    | 26674  |
|    total_timesteps | 260096 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 261000      |
| train/                  |             |
|    approx_kl            | 0.022916965 |
|    clip_fraction        | 0.27        |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.91       |
|    explained_variance   | 0.505       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00996    |
|    n_updates            | 1270        |
|    policy_gradient_loss | -0.0271     |
|    value_loss           | 0.00229     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 128    |
|    time_elapsed    | 26879  |
|    total_timesteps | 262144 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 263000      |
| train/                  |             |
|    approx_kl            | 0.019167885 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.12       |
|    explained_variance   | 0.4         |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0274     |
|    n_updates            | 1280        |
|    policy_gradient_loss | -0.0303     |
|    value_loss           | 0.00227     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 129    |
|    time_elapsed    | 27084  |
|    total_timesteps | 264192 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 265000      |
| train/                  |             |
|    approx_kl            | 0.018447628 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.1        |
|    explained_variance   | 0.383       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0399     |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.0186     |
|    value_loss           | 0.00287     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 130    |
|    time_elapsed    | 27290  |
|    total_timesteps | 266240 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 267000      |
| train/                  |             |
|    approx_kl            | 0.019528266 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.96       |
|    explained_variance   | 0.696       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0377     |
|    n_updates            | 1300        |
|    policy_gradient_loss | -0.0228     |
|    value_loss           | 0.00242     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 131    |
|    time_elapsed    | 27495  |
|    total_timesteps | 268288 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 269000      |
| train/                  |             |
|    approx_kl            | 0.021931496 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.9        |
|    explained_variance   | 0.53        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0334     |
|    n_updates            | 1310        |
|    policy_gradient_loss | -0.0231     |
|    value_loss           | 0.00333     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 132    |
|    time_elapsed    | 27701  |
|    total_timesteps | 270336 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 271000      |
| train/                  |             |
|    approx_kl            | 0.015903294 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.84       |
|    explained_variance   | 0.482       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0266     |
|    n_updates            | 1320        |
|    policy_gradient_loss | -0.0184     |
|    value_loss           | 0.00264     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 133    |
|    time_elapsed    | 27906  |
|    total_timesteps | 272384 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.6        |
| time/                   |            |
|    total_timesteps      | 273000     |
| train/                  |            |
|    approx_kl            | 0.02148439 |
|    clip_fraction        | 0.243      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.83      |
|    explained_variance   | 0.633      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0548    |
|    n_updates            | 1330       |
|    policy_gradient_loss | -0.0253    |
|    value_loss           | 0.00307    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 134    |
|    time_elapsed    | 28111  |
|    total_timesteps | 274432 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 275000      |
| train/                  |             |
|    approx_kl            | 0.016929375 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.75       |
|    explained_variance   | 0.799       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.024      |
|    n_updates            | 1340        |
|    policy_gradient_loss | -0.0139     |
|    value_loss           | 0.00247     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 135    |
|    time_elapsed    | 28317  |
|    total_timesteps | 276480 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 277000      |
| train/                  |             |
|    approx_kl            | 0.018948358 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.72       |
|    explained_variance   | 0.563       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.02       |
|    n_updates            | 1350        |
|    policy_gradient_loss | -0.0209     |
|    value_loss           | 0.00312     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 278000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 136    |
|    time_elapsed    | 28522  |
|    total_timesteps | 278528 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 279000      |
| train/                  |             |
|    approx_kl            | 0.014772464 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.66       |
|    explained_variance   | 0.734       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0276     |
|    n_updates            | 1360        |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 0.0023      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 280000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 137    |
|    time_elapsed    | 28727  |
|    total_timesteps | 280576 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 281000      |
| train/                  |             |
|    approx_kl            | 0.015588369 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.64       |
|    explained_variance   | 0.575       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0354     |
|    n_updates            | 1370        |
|    policy_gradient_loss | -0.022      |
|    value_loss           | 0.00229     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 282000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 138    |
|    time_elapsed    | 28933  |
|    total_timesteps | 282624 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.6        |
| time/                   |            |
|    total_timesteps      | 283000     |
| train/                  |            |
|    approx_kl            | 0.02337734 |
|    clip_fraction        | 0.254      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.59      |
|    explained_variance   | 0.665      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0126    |
|    n_updates            | 1380       |
|    policy_gradient_loss | -0.0246    |
|    value_loss           | 0.00295    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 284000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 139    |
|    time_elapsed    | 29138  |
|    total_timesteps | 284672 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 285000      |
| train/                  |             |
|    approx_kl            | 0.016040353 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.58       |
|    explained_variance   | 0.766       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.06       |
|    n_updates            | 1390        |
|    policy_gradient_loss | -0.022      |
|    value_loss           | 0.00218     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 286000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 140    |
|    time_elapsed    | 29343  |
|    total_timesteps | 286720 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 287000      |
| train/                  |             |
|    approx_kl            | 0.018321654 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.45       |
|    explained_variance   | 0.683       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0157     |
|    n_updates            | 1400        |
|    policy_gradient_loss | -0.0225     |
|    value_loss           | 0.00286     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 288000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 141    |
|    time_elapsed    | 29549  |
|    total_timesteps | 288768 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.6        |
| time/                   |            |
|    total_timesteps      | 289000     |
| train/                  |            |
|    approx_kl            | 0.02033227 |
|    clip_fraction        | 0.225      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.4       |
|    explained_variance   | 0.501      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0284    |
|    n_updates            | 1410       |
|    policy_gradient_loss | -0.0214    |
|    value_loss           | 0.00394    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 290000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 142    |
|    time_elapsed    | 29754  |
|    total_timesteps | 290816 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 291000      |
| train/                  |             |
|    approx_kl            | 0.019068344 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.36       |
|    explained_variance   | 0.526       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0468     |
|    n_updates            | 1420        |
|    policy_gradient_loss | -0.0263     |
|    value_loss           | 0.00335     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 292000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 143    |
|    time_elapsed    | 29959  |
|    total_timesteps | 292864 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.6        |
| time/                   |            |
|    total_timesteps      | 293000     |
| train/                  |            |
|    approx_kl            | 0.02140611 |
|    clip_fraction        | 0.241      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.63      |
|    explained_variance   | 0.337      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0435    |
|    n_updates            | 1430       |
|    policy_gradient_loss | -0.0216    |
|    value_loss           | 0.00436    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 294000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 144    |
|    time_elapsed    | 30165  |
|    total_timesteps | 294912 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 295000      |
| train/                  |             |
|    approx_kl            | 0.023012217 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.3        |
|    explained_variance   | 0.393       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00701     |
|    n_updates            | 1440        |
|    policy_gradient_loss | -0.0184     |
|    value_loss           | 0.00489     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 296000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 145    |
|    time_elapsed    | 30370  |
|    total_timesteps | 296960 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 297000      |
| train/                  |             |
|    approx_kl            | 0.015166735 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.29       |
|    explained_variance   | 0.514       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0434     |
|    n_updates            | 1450        |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 0.00421     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 298000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 146    |
|    time_elapsed    | 30675  |
|    total_timesteps | 299008 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 300000      |
| train/                  |             |
|    approx_kl            | 0.017574187 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.35       |
|    explained_variance   | 0.504       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00177    |
|    n_updates            | 1460        |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.006       |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 301000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 147    |
|    time_elapsed    | 30881  |
|    total_timesteps | 301056 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 302000      |
| train/                  |             |
|    approx_kl            | 0.021153767 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.37       |
|    explained_variance   | 0.61        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00418    |
|    n_updates            | 1470        |
|    policy_gradient_loss | -0.0191     |
|    value_loss           | 0.00471     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 303000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 148    |
|    time_elapsed    | 31086  |
|    total_timesteps | 303104 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 304000      |
| train/                  |             |
|    approx_kl            | 0.019885158 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.39       |
|    explained_variance   | 0.48        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0333     |
|    n_updates            | 1480        |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.0048      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 305000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 149    |
|    time_elapsed    | 31291  |
|    total_timesteps | 305152 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 306000      |
| train/                  |             |
|    approx_kl            | 0.016388088 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.1        |
|    explained_variance   | 0.589       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00189     |
|    n_updates            | 1490        |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.00399     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 307000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 150    |
|    time_elapsed    | 31497  |
|    total_timesteps | 307200 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 308000      |
| train/                  |             |
|    approx_kl            | 0.016176712 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.11       |
|    explained_variance   | 0.683       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0346     |
|    n_updates            | 1500        |
|    policy_gradient_loss | -0.0203     |
|    value_loss           | 0.00314     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 309000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 151    |
|    time_elapsed    | 31702  |
|    total_timesteps | 309248 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 310000      |
| train/                  |             |
|    approx_kl            | 0.017744001 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.16       |
|    explained_variance   | 0.559       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0145     |
|    n_updates            | 1510        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.00349     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 311000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 152    |
|    time_elapsed    | 31907  |
|    total_timesteps | 311296 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 312000      |
| train/                  |             |
|    approx_kl            | 0.019206636 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.17       |
|    explained_variance   | 0.53        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0336     |
|    n_updates            | 1520        |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.00401     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 313000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 153    |
|    time_elapsed    | 32113  |
|    total_timesteps | 313344 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 314000      |
| train/                  |             |
|    approx_kl            | 0.023592448 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.02       |
|    explained_variance   | 0.762       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0121     |
|    n_updates            | 1530        |
|    policy_gradient_loss | -0.022      |
|    value_loss           | 0.00323     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 315000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 154    |
|    time_elapsed    | 32318  |
|    total_timesteps | 315392 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 316000      |
| train/                  |             |
|    approx_kl            | 0.016735781 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.11       |
|    explained_variance   | 0.474       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00235     |
|    n_updates            | 1540        |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.0036      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 317000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 155    |
|    time_elapsed    | 32523  |
|    total_timesteps | 317440 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 318000      |
| train/                  |             |
|    approx_kl            | 0.020304993 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.91       |
|    explained_variance   | 0.627       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0336     |
|    n_updates            | 1550        |
|    policy_gradient_loss | -0.0199     |
|    value_loss           | 0.00362     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 319000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 156    |
|    time_elapsed    | 32729  |
|    total_timesteps | 319488 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 320000      |
| train/                  |             |
|    approx_kl            | 0.017881146 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.05       |
|    explained_variance   | 0.492       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00051     |
|    n_updates            | 1560        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.00344     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 321000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 157    |
|    time_elapsed    | 32934  |
|    total_timesteps | 321536 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 322000      |
| train/                  |             |
|    approx_kl            | 0.018755555 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.75       |
|    explained_variance   | 0.463       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0254     |
|    n_updates            | 1570        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.00435     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 323000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 158    |
|    time_elapsed    | 33139  |
|    total_timesteps | 323584 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 324000      |
| train/                  |             |
|    approx_kl            | 0.015752923 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.65       |
|    explained_variance   | 0.685       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.03       |
|    n_updates            | 1580        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.00322     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 325000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 159    |
|    time_elapsed    | 33345  |
|    total_timesteps | 325632 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 326000     |
| train/                  |            |
|    approx_kl            | 0.01862426 |
|    clip_fraction        | 0.228      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.57      |
|    explained_variance   | 0.514      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0419    |
|    n_updates            | 1590       |
|    policy_gradient_loss | -0.0183    |
|    value_loss           | 0.00455    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 327000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 160    |
|    time_elapsed    | 33550  |
|    total_timesteps | 327680 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 328000      |
| train/                  |             |
|    approx_kl            | 0.017954193 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.67       |
|    explained_variance   | 0.736       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.012      |
|    n_updates            | 1600        |
|    policy_gradient_loss | -0.02       |
|    value_loss           | 0.0033      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 329000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 161    |
|    time_elapsed    | 33755  |
|    total_timesteps | 329728 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 330000      |
| train/                  |             |
|    approx_kl            | 0.015775759 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.54       |
|    explained_variance   | 0.601       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0257     |
|    n_updates            | 1610        |
|    policy_gradient_loss | -0.0185     |
|    value_loss           | 0.00377     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 331000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 162    |
|    time_elapsed    | 33961  |
|    total_timesteps | 331776 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.7        |
| time/                   |            |
|    total_timesteps      | 332000     |
| train/                  |            |
|    approx_kl            | 0.01844113 |
|    clip_fraction        | 0.243      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.51      |
|    explained_variance   | 0.695      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0149    |
|    n_updates            | 1620       |
|    policy_gradient_loss | -0.0173    |
|    value_loss           | 0.00452    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 333000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 163    |
|    time_elapsed    | 34166  |
|    total_timesteps | 333824 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 334000     |
| train/                  |            |
|    approx_kl            | 0.01459293 |
|    clip_fraction        | 0.173      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.48      |
|    explained_variance   | 0.721      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.027     |
|    n_updates            | 1630       |
|    policy_gradient_loss | -0.0151    |
|    value_loss           | 0.00322    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 335000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 164    |
|    time_elapsed    | 34371  |
|    total_timesteps | 335872 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 336000      |
| train/                  |             |
|    approx_kl            | 0.013611037 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.44       |
|    explained_variance   | 0.657       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0119     |
|    n_updates            | 1640        |
|    policy_gradient_loss | -0.0079     |
|    value_loss           | 0.00483     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 337000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 165    |
|    time_elapsed    | 34577  |
|    total_timesteps | 337920 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 338000      |
| train/                  |             |
|    approx_kl            | 0.013316089 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.42       |
|    explained_variance   | 0.558       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00133     |
|    n_updates            | 1650        |
|    policy_gradient_loss | -0.0131     |
|    value_loss           | 0.00452     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 339000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 166    |
|    time_elapsed    | 34782  |
|    total_timesteps | 339968 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 340000      |
| train/                  |             |
|    approx_kl            | 0.027349051 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.34       |
|    explained_variance   | 0.803       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0438     |
|    n_updates            | 1660        |
|    policy_gradient_loss | -0.0234     |
|    value_loss           | 0.00402     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 341000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 342000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 167    |
|    time_elapsed    | 35087  |
|    total_timesteps | 342016 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 343000      |
| train/                  |             |
|    approx_kl            | 0.018980702 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.44       |
|    explained_variance   | 0.734       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.026      |
|    n_updates            | 1670        |
|    policy_gradient_loss | -0.0182     |
|    value_loss           | 0.00348     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 344000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 168    |
|    time_elapsed    | 35293  |
|    total_timesteps | 344064 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 345000      |
| train/                  |             |
|    approx_kl            | 0.016862892 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.35       |
|    explained_variance   | 0.808       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0495     |
|    n_updates            | 1680        |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.00338     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 346000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 169    |
|    time_elapsed    | 35498  |
|    total_timesteps | 346112 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 347000      |
| train/                  |             |
|    approx_kl            | 0.015360622 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.45       |
|    explained_variance   | 0.863       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00555     |
|    n_updates            | 1690        |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.00367     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 348000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 170    |
|    time_elapsed    | 35703  |
|    total_timesteps | 348160 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 349000      |
| train/                  |             |
|    approx_kl            | 0.016348489 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.44       |
|    explained_variance   | 0.784       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00632     |
|    n_updates            | 1700        |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 0.00456     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 350000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 171    |
|    time_elapsed    | 35909  |
|    total_timesteps | 350208 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 351000      |
| train/                  |             |
|    approx_kl            | 0.013114577 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.49       |
|    explained_variance   | 0.575       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000698    |
|    n_updates            | 1710        |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 0.00456     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 352000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 172    |
|    time_elapsed    | 36114  |
|    total_timesteps | 352256 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 353000     |
| train/                  |            |
|    approx_kl            | 0.01435874 |
|    clip_fraction        | 0.175      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.45      |
|    explained_variance   | 0.48       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00917   |
|    n_updates            | 1720       |
|    policy_gradient_loss | -0.0141    |
|    value_loss           | 0.0047     |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 354000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 173    |
|    time_elapsed    | 36319  |
|    total_timesteps | 354304 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 355000      |
| train/                  |             |
|    approx_kl            | 0.023524947 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.37       |
|    explained_variance   | 0.336       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000868   |
|    n_updates            | 1730        |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 0.00636     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 356000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 174    |
|    time_elapsed    | 36525  |
|    total_timesteps | 356352 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 357000      |
| train/                  |             |
|    approx_kl            | 0.017238548 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.17       |
|    explained_variance   | 0.341       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0213     |
|    n_updates            | 1740        |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.00646     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 358000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 175    |
|    time_elapsed    | 36730  |
|    total_timesteps | 358400 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 359000      |
| train/                  |             |
|    approx_kl            | 0.014756876 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.2        |
|    explained_variance   | 0.397       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00816    |
|    n_updates            | 1750        |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 0.00666     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 360000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 176    |
|    time_elapsed    | 36935  |
|    total_timesteps | 360448 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 361000      |
| train/                  |             |
|    approx_kl            | 0.014176567 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.07       |
|    explained_variance   | 0.444       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.018      |
|    n_updates            | 1760        |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 0.00647     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 362000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 177    |
|    time_elapsed    | 37141  |
|    total_timesteps | 362496 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 363000      |
| train/                  |             |
|    approx_kl            | 0.014458206 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.09       |
|    explained_variance   | 0.365       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0154     |
|    n_updates            | 1770        |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 0.0065      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 364000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 178    |
|    time_elapsed    | 37346  |
|    total_timesteps | 364544 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 365000      |
| train/                  |             |
|    approx_kl            | 0.015443237 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.25       |
|    explained_variance   | 0.375       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0352     |
|    n_updates            | 1780        |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 0.00684     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 366000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 179    |
|    time_elapsed    | 37551  |
|    total_timesteps | 366592 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 367000      |
| train/                  |             |
|    approx_kl            | 0.018013578 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.2        |
|    explained_variance   | 0.357       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00158     |
|    n_updates            | 1790        |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 0.00724     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 368000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 180    |
|    time_elapsed    | 37757  |
|    total_timesteps | 368640 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 369000      |
| train/                  |             |
|    approx_kl            | 0.015693825 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.99       |
|    explained_variance   | 0.401       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0173     |
|    n_updates            | 1800        |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 0.00772     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 370000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 181    |
|    time_elapsed    | 37962  |
|    total_timesteps | 370688 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 371000      |
| train/                  |             |
|    approx_kl            | 0.012816365 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.83       |
|    explained_variance   | 0.427       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0205     |
|    n_updates            | 1810        |
|    policy_gradient_loss | -0.00987    |
|    value_loss           | 0.0079      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 372000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 182    |
|    time_elapsed    | 38167  |
|    total_timesteps | 372736 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 373000      |
| train/                  |             |
|    approx_kl            | 0.019616053 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.81       |
|    explained_variance   | 0.468       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00652    |
|    n_updates            | 1820        |
|    policy_gradient_loss | -0.00873    |
|    value_loss           | 0.00775     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 374000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 183    |
|    time_elapsed    | 38372  |
|    total_timesteps | 374784 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 375000      |
| train/                  |             |
|    approx_kl            | 0.017122112 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.68       |
|    explained_variance   | 0.448       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0235      |
|    n_updates            | 1830        |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 0.00808     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 376000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 184    |
|    time_elapsed    | 38578  |
|    total_timesteps | 376832 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 377000      |
| train/                  |             |
|    approx_kl            | 0.014387054 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.54       |
|    explained_variance   | 0.433       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0287      |
|    n_updates            | 1840        |
|    policy_gradient_loss | -0.00615    |
|    value_loss           | 0.00862     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 378000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 185    |
|    time_elapsed    | 38783  |
|    total_timesteps | 378880 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 379000      |
| train/                  |             |
|    approx_kl            | 0.014896595 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.42       |
|    explained_variance   | 0.41        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0113      |
|    n_updates            | 1850        |
|    policy_gradient_loss | -0.0075     |
|    value_loss           | 0.00871     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 380000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 186    |
|    time_elapsed    | 38988  |
|    total_timesteps | 380928 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 381000      |
| train/                  |             |
|    approx_kl            | 0.014539499 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.409       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00439    |
|    n_updates            | 1860        |
|    policy_gradient_loss | -0.00611    |
|    value_loss           | 0.00886     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 382000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 187    |
|    time_elapsed    | 39194  |
|    total_timesteps | 382976 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 1          |
| time/                   |            |
|    total_timesteps      | 383000     |
| train/                  |            |
|    approx_kl            | 0.01556453 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.13      |
|    explained_variance   | 0.46       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0134    |
|    n_updates            | 1870       |
|    policy_gradient_loss | -0.00759   |
|    value_loss           | 0.00914    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 384000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 385000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 188    |
|    time_elapsed    | 39499  |
|    total_timesteps | 385024 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 386000      |
| train/                  |             |
|    approx_kl            | 0.018161133 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.11       |
|    explained_variance   | 0.439       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000563   |
|    n_updates            | 1880        |
|    policy_gradient_loss | -0.0068     |
|    value_loss           | 0.0136      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 387000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 189    |
|    time_elapsed    | 39704  |
|    total_timesteps | 387072 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 388000      |
| train/                  |             |
|    approx_kl            | 0.013304986 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.452       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0153     |
|    n_updates            | 1890        |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 0.00887     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 389000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 190    |
|    time_elapsed    | 39910  |
|    total_timesteps | 389120 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 390000      |
| train/                  |             |
|    approx_kl            | 0.020954795 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.39        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00518     |
|    n_updates            | 1900        |
|    policy_gradient_loss | -0.00733    |
|    value_loss           | 0.0096      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 391000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 191    |
|    time_elapsed    | 40115  |
|    total_timesteps | 391168 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 392000      |
| train/                  |             |
|    approx_kl            | 0.017924307 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.68       |
|    explained_variance   | 0.418       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0218     |
|    n_updates            | 1910        |
|    policy_gradient_loss | -0.00852    |
|    value_loss           | 0.00954     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 393000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 192    |
|    time_elapsed    | 40320  |
|    total_timesteps | 393216 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 394000      |
| train/                  |             |
|    approx_kl            | 0.022978708 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.48       |
|    explained_variance   | 0.43        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0298     |
|    n_updates            | 1920        |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 0.00983     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 395000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 193    |
|    time_elapsed    | 40526  |
|    total_timesteps | 395264 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 1          |
| time/                   |            |
|    total_timesteps      | 396000     |
| train/                  |            |
|    approx_kl            | 0.01011686 |
|    clip_fraction        | 0.135      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.31      |
|    explained_variance   | 0.431      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00583   |
|    n_updates            | 1930       |
|    policy_gradient_loss | -0.00439   |
|    value_loss           | 0.0103     |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 397000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 194    |
|    time_elapsed    | 40731  |
|    total_timesteps | 397312 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 398000      |
| train/                  |             |
|    approx_kl            | 0.011486488 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.37       |
|    explained_variance   | 0.393       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00437     |
|    n_updates            | 1940        |
|    policy_gradient_loss | -0.00453    |
|    value_loss           | 0.0109      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 399000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 195    |
|    time_elapsed    | 40936  |
|    total_timesteps | 399360 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 400000      |
| train/                  |             |
|    approx_kl            | 0.020013873 |
|    clip_fraction        | 0.283       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.48       |
|    explained_variance   | 0.449       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0154     |
|    n_updates            | 1950        |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 0.011       |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 401000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 196    |
|    time_elapsed    | 41142  |
|    total_timesteps | 401408 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 402000      |
| train/                  |             |
|    approx_kl            | 0.025562137 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.55       |
|    explained_variance   | 0.417       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00571    |
|    n_updates            | 1960        |
|    policy_gradient_loss | -0.00443    |
|    value_loss           | 0.0107      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 403000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 197    |
|    time_elapsed    | 41347  |
|    total_timesteps | 403456 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 404000      |
| train/                  |             |
|    approx_kl            | 0.024323825 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.32       |
|    explained_variance   | 0.477       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0132     |
|    n_updates            | 1970        |
|    policy_gradient_loss | -0.00499    |
|    value_loss           | 0.0104      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 405000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 198    |
|    time_elapsed    | 41552  |
|    total_timesteps | 405504 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 406000      |
| train/                  |             |
|    approx_kl            | 0.008045529 |
|    clip_fraction        | 0.095       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.25       |
|    explained_variance   | 0.432       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00666     |
|    n_updates            | 1980        |
|    policy_gradient_loss | -0.00469    |
|    value_loss           | 0.0113      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 407000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 199    |
|    time_elapsed    | 41758  |
|    total_timesteps | 407552 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 408000      |
| train/                  |             |
|    approx_kl            | 0.029516507 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.1        |
|    explained_variance   | 0.482       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0594      |
|    n_updates            | 1990        |
|    policy_gradient_loss | -0.00498    |
|    value_loss           | 0.0117      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 409000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 200    |
|    time_elapsed    | 41963  |
|    total_timesteps | 409600 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 410000      |
| train/                  |             |
|    approx_kl            | 0.016960721 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.39       |
|    explained_variance   | 0.439       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0222      |
|    n_updates            | 2000        |
|    policy_gradient_loss | -0.00431    |
|    value_loss           | 0.012       |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 411000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 201    |
|    time_elapsed    | 42168  |
|    total_timesteps | 411648 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.1          |
| time/                   |              |
|    total_timesteps      | 412000       |
| train/                  |              |
|    approx_kl            | 0.0147180995 |
|    clip_fraction        | 0.147        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.24        |
|    explained_variance   | 0.441        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000207     |
|    n_updates            | 2010         |
|    policy_gradient_loss | -0.00449     |
|    value_loss           | 0.0121       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 413000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 202    |
|    time_elapsed    | 42373  |
|    total_timesteps | 413696 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 414000      |
| train/                  |             |
|    approx_kl            | 0.010741272 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.16       |
|    explained_variance   | 0.42        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.015       |
|    n_updates            | 2020        |
|    policy_gradient_loss | -0.00403    |
|    value_loss           | 0.0123      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 415000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 203    |
|    time_elapsed    | 42579  |
|    total_timesteps | 415744 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.2          |
| time/                   |              |
|    total_timesteps      | 416000       |
| train/                  |              |
|    approx_kl            | 0.0057778535 |
|    clip_fraction        | 0.0683       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.04        |
|    explained_variance   | 0.454        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.041        |
|    n_updates            | 2030         |
|    policy_gradient_loss | -0.00119     |
|    value_loss           | 0.0119       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.2      |
| time/              |          |
|    total_timesteps | 417000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 204    |
|    time_elapsed    | 42784  |
|    total_timesteps | 417792 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.2         |
| time/                   |             |
|    total_timesteps      | 418000      |
| train/                  |             |
|    approx_kl            | 0.008084021 |
|    clip_fraction        | 0.0808      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.428       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000955   |
|    n_updates            | 2040        |
|    policy_gradient_loss | -0.00229    |
|    value_loss           | 0.012       |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.2      |
| time/              |          |
|    total_timesteps | 419000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 205    |
|    time_elapsed    | 42989  |
|    total_timesteps | 419840 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.2         |
| time/                   |             |
|    total_timesteps      | 420000      |
| train/                  |             |
|    approx_kl            | 0.004598023 |
|    clip_fraction        | 0.0446      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.782      |
|    explained_variance   | 0.385       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00503     |
|    n_updates            | 2050        |
|    policy_gradient_loss | -0.00104    |
|    value_loss           | 0.012       |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.2      |
| time/              |          |
|    total_timesteps | 421000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 206    |
|    time_elapsed    | 43195  |
|    total_timesteps | 421888 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.2         |
| time/                   |             |
|    total_timesteps      | 422000      |
| train/                  |             |
|    approx_kl            | 0.007902226 |
|    clip_fraction        | 0.0877      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.816      |
|    explained_variance   | 0.449       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00571     |
|    n_updates            | 2060        |
|    policy_gradient_loss | -0.00333    |
|    value_loss           | 0.0116      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.2      |
| time/              |          |
|    total_timesteps | 423000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 207    |
|    time_elapsed    | 43400  |
|    total_timesteps | 423936 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.2          |
| time/                   |              |
|    total_timesteps      | 424000       |
| train/                  |              |
|    approx_kl            | 0.0069704326 |
|    clip_fraction        | 0.064        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.871       |
|    explained_variance   | 0.437        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0255       |
|    n_updates            | 2070         |
|    policy_gradient_loss | -0.00288     |
|    value_loss           | 0.012        |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.2      |
| time/              |          |
|    total_timesteps | 425000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 208    |
|    time_elapsed    | 43605  |
|    total_timesteps | 425984 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.2          |
| time/                   |              |
|    total_timesteps      | 426000       |
| train/                  |              |
|    approx_kl            | 0.0038132514 |
|    clip_fraction        | 0.0542       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.689       |
|    explained_variance   | 0.419        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0153       |
|    n_updates            | 2080         |
|    policy_gradient_loss | -0.0002      |
|    value_loss           | 0.0121       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.2      |
| time/              |          |
|    total_timesteps | 427000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.2      |
| time/              |          |
|    total_timesteps | 428000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 209    |
|    time_elapsed    | 43911  |
|    total_timesteps | 428032 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.2          |
| time/                   |              |
|    total_timesteps      | 429000       |
| train/                  |              |
|    approx_kl            | 0.0035679743 |
|    clip_fraction        | 0.0592       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.559       |
|    explained_variance   | 0.459        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00659      |
|    n_updates            | 2090         |
|    policy_gradient_loss | -0.00393     |
|    value_loss           | 0.0172       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.2      |
| time/              |          |
|    total_timesteps | 430000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 210    |
|    time_elapsed    | 44116  |
|    total_timesteps | 430080 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.4          |
| time/                   |              |
|    total_timesteps      | 431000       |
| train/                  |              |
|    approx_kl            | 0.0037540118 |
|    clip_fraction        | 0.0586       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.513       |
|    explained_variance   | 0.444        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00594      |
|    n_updates            | 2100         |
|    policy_gradient_loss | -0.00313     |
|    value_loss           | 0.0126       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 432000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 211    |
|    time_elapsed    | 44321  |
|    total_timesteps | 432128 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.3          |
| time/                   |              |
|    total_timesteps      | 433000       |
| train/                  |              |
|    approx_kl            | 0.0031266925 |
|    clip_fraction        | 0.0416       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.512       |
|    explained_variance   | 0.418        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000601     |
|    n_updates            | 2110         |
|    policy_gradient_loss | -0.00314     |
|    value_loss           | 0.0129       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 434000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 212    |
|    time_elapsed    | 44527  |
|    total_timesteps | 434176 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.3          |
| time/                   |              |
|    total_timesteps      | 435000       |
| train/                  |              |
|    approx_kl            | 0.0041826703 |
|    clip_fraction        | 0.0521       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.519       |
|    explained_variance   | 0.446        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00863      |
|    n_updates            | 2120         |
|    policy_gradient_loss | -0.00138     |
|    value_loss           | 0.0126       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 436000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 213    |
|    time_elapsed    | 44732  |
|    total_timesteps | 436224 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.4          |
| time/                   |              |
|    total_timesteps      | 437000       |
| train/                  |              |
|    approx_kl            | 0.0025379641 |
|    clip_fraction        | 0.029        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.327       |
|    explained_variance   | 0.449        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000547    |
|    n_updates            | 2130         |
|    policy_gradient_loss | -0.00256     |
|    value_loss           | 0.0131       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 438000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 214    |
|    time_elapsed    | 44937  |
|    total_timesteps | 438272 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.4          |
| time/                   |              |
|    total_timesteps      | 439000       |
| train/                  |              |
|    approx_kl            | 0.0020552073 |
|    clip_fraction        | 0.0282       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.319       |
|    explained_variance   | 0.425        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0166       |
|    n_updates            | 2140         |
|    policy_gradient_loss | -0.000787    |
|    value_loss           | 0.0139       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 440000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 215    |
|    time_elapsed    | 45143  |
|    total_timesteps | 440320 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.4         |
| time/                   |             |
|    total_timesteps      | 441000      |
| train/                  |             |
|    approx_kl            | 0.003000841 |
|    clip_fraction        | 0.0234      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.323      |
|    explained_variance   | 0.425       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00185    |
|    n_updates            | 2150        |
|    policy_gradient_loss | -0.000186   |
|    value_loss           | 0.0138      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 442000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 216    |
|    time_elapsed    | 45348  |
|    total_timesteps | 442368 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.4          |
| time/                   |              |
|    total_timesteps      | 443000       |
| train/                  |              |
|    approx_kl            | 0.0035495283 |
|    clip_fraction        | 0.0414       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.272       |
|    explained_variance   | 0.417        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00499      |
|    n_updates            | 2160         |
|    policy_gradient_loss | -0.00193     |
|    value_loss           | 0.0136       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 444000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 217    |
|    time_elapsed    | 45553  |
|    total_timesteps | 444416 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.4          |
| time/                   |              |
|    total_timesteps      | 445000       |
| train/                  |              |
|    approx_kl            | 0.0038167136 |
|    clip_fraction        | 0.031        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.282       |
|    explained_variance   | 0.446        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00207      |
|    n_updates            | 2170         |
|    policy_gradient_loss | -0.00288     |
|    value_loss           | 0.0134       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 446000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 218    |
|    time_elapsed    | 45759  |
|    total_timesteps | 446464 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.4          |
| time/                   |              |
|    total_timesteps      | 447000       |
| train/                  |              |
|    approx_kl            | 0.0013194531 |
|    clip_fraction        | 0.0199       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.201       |
|    explained_variance   | 0.414        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00609      |
|    n_updates            | 2180         |
|    policy_gradient_loss | -0.0019      |
|    value_loss           | 0.0136       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 448000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 219    |
|    time_elapsed    | 45964  |
|    total_timesteps | 448512 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.4          |
| time/                   |              |
|    total_timesteps      | 449000       |
| train/                  |              |
|    approx_kl            | 0.0014506674 |
|    clip_fraction        | 0.0229       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.189       |
|    explained_variance   | 0.418        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.019        |
|    n_updates            | 2190         |
|    policy_gradient_loss | -0.00114     |
|    value_loss           | 0.0139       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 450000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 220    |
|    time_elapsed    | 46169  |
|    total_timesteps | 450560 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.4          |
| time/                   |              |
|    total_timesteps      | 451000       |
| train/                  |              |
|    approx_kl            | 0.0014700815 |
|    clip_fraction        | 0.0159       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.163       |
|    explained_variance   | 0.395        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00332     |
|    n_updates            | 2200         |
|    policy_gradient_loss | -0.000942    |
|    value_loss           | 0.0143       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 452000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 221    |
|    time_elapsed    | 46375  |
|    total_timesteps | 452608 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.4          |
| time/                   |              |
|    total_timesteps      | 453000       |
| train/                  |              |
|    approx_kl            | 0.0038614513 |
|    clip_fraction        | 0.0283       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.213       |
|    explained_variance   | 0.4          |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0106       |
|    n_updates            | 2210         |
|    policy_gradient_loss | -0.00112     |
|    value_loss           | 0.014        |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 454000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 222    |
|    time_elapsed    | 46580  |
|    total_timesteps | 454656 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.4         |
| time/                   |             |
|    total_timesteps      | 455000      |
| train/                  |             |
|    approx_kl            | 0.002837427 |
|    clip_fraction        | 0.0247      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.233      |
|    explained_variance   | 0.405       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0155      |
|    n_updates            | 2220        |
|    policy_gradient_loss | -0.000482   |
|    value_loss           | 0.0138      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 456000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 223    |
|    time_elapsed    | 46786  |
|    total_timesteps | 456704 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 457000      |
| train/                  |             |
|    approx_kl            | 0.004443944 |
|    clip_fraction        | 0.0354      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.213      |
|    explained_variance   | 0.409       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0299     |
|    n_updates            | 2230        |
|    policy_gradient_loss | -0.00262    |
|    value_loss           | 0.013       |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 458000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 224    |
|    time_elapsed    | 46991  |
|    total_timesteps | 458752 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.4         |
| time/                   |             |
|    total_timesteps      | 459000      |
| train/                  |             |
|    approx_kl            | 0.001808977 |
|    clip_fraction        | 0.0274      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.111      |
|    explained_variance   | 0.412       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00898     |
|    n_updates            | 2240        |
|    policy_gradient_loss | -0.000771   |
|    value_loss           | 0.013       |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 460000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 225    |
|    time_elapsed    | 47196  |
|    total_timesteps | 460800 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.4          |
| time/                   |              |
|    total_timesteps      | 461000       |
| train/                  |              |
|    approx_kl            | 0.0006586971 |
|    clip_fraction        | 0.0131       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.134       |
|    explained_variance   | 0.393        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00223      |
|    n_updates            | 2250         |
|    policy_gradient_loss | -0.000169    |
|    value_loss           | 0.0138       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 462000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 226    |
|    time_elapsed    | 47401  |
|    total_timesteps | 462848 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.4          |
| time/                   |              |
|    total_timesteps      | 463000       |
| train/                  |              |
|    approx_kl            | 0.0015843124 |
|    clip_fraction        | 0.012        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.103       |
|    explained_variance   | 0.409        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00289      |
|    n_updates            | 2260         |
|    policy_gradient_loss | -0.000597    |
|    value_loss           | 0.0136       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 464000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 227    |
|    time_elapsed    | 47607  |
|    total_timesteps | 464896 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.4         |
| time/                   |             |
|    total_timesteps      | 465000      |
| train/                  |             |
|    approx_kl            | 0.001292758 |
|    clip_fraction        | 0.00957     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0977     |
|    explained_variance   | 0.409       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00183     |
|    n_updates            | 2270        |
|    policy_gradient_loss | -0.000291   |
|    value_loss           | 0.0139      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 466000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 228    |
|    time_elapsed    | 47812  |
|    total_timesteps | 466944 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.4          |
| time/                   |              |
|    total_timesteps      | 467000       |
| train/                  |              |
|    approx_kl            | 0.0005659148 |
|    clip_fraction        | 0.0114       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.123       |
|    explained_variance   | 0.421        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000669     |
|    n_updates            | 2280         |
|    policy_gradient_loss | -0.000471    |
|    value_loss           | 0.0136       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 468000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 229    |
|    time_elapsed    | 48017  |
|    total_timesteps | 468992 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.4         |
| time/                   |             |
|    total_timesteps      | 469000      |
| train/                  |             |
|    approx_kl            | 0.006716691 |
|    clip_fraction        | 0.046       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.317      |
|    explained_variance   | 0.4         |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00209    |
|    n_updates            | 2290        |
|    policy_gradient_loss | -0.00141    |
|    value_loss           | 0.0135      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 470000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 471000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 230    |
|    time_elapsed    | 48323  |
|    total_timesteps | 471040 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 472000      |
| train/                  |             |
|    approx_kl            | 0.006177247 |
|    clip_fraction        | 0.0261      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.173      |
|    explained_variance   | 0.423       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00168    |
|    n_updates            | 2300        |
|    policy_gradient_loss | -0.0023     |
|    value_loss           | 0.0173      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 473000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 231    |
|    time_elapsed    | 48528  |
|    total_timesteps | 473088 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 474000      |
| train/                  |             |
|    approx_kl            | 0.024118388 |
|    clip_fraction        | 0.052       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.244      |
|    explained_variance   | 0.511       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00498     |
|    n_updates            | 2310        |
|    policy_gradient_loss | -0.00426    |
|    value_loss           | 0.011       |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 475000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 232    |
|    time_elapsed    | 48733  |
|    total_timesteps | 475136 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 476000      |
| train/                  |             |
|    approx_kl            | 0.012124572 |
|    clip_fraction        | 0.0836      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.476      |
|    explained_variance   | 0.499       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0129      |
|    n_updates            | 2320        |
|    policy_gradient_loss | -0.000497   |
|    value_loss           | 0.0104      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 477000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 233    |
|    time_elapsed    | 48939  |
|    total_timesteps | 477184 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 478000      |
| train/                  |             |
|    approx_kl            | 0.009881607 |
|    clip_fraction        | 0.0409      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.266      |
|    explained_variance   | 0.532       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000457    |
|    n_updates            | 2330        |
|    policy_gradient_loss | 0.000496    |
|    value_loss           | 0.011       |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 479000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 234    |
|    time_elapsed    | 49144  |
|    total_timesteps | 479232 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1            |
| time/                   |              |
|    total_timesteps      | 480000       |
| train/                  |              |
|    approx_kl            | 0.0016076344 |
|    clip_fraction        | 0.0203       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.141       |
|    explained_variance   | 0.464        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00221      |
|    n_updates            | 2340         |
|    policy_gradient_loss | -0.00129     |
|    value_loss           | 0.012        |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 481000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 235    |
|    time_elapsed    | 49349  |
|    total_timesteps | 481280 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.4          |
| time/                   |              |
|    total_timesteps      | 482000       |
| train/                  |              |
|    approx_kl            | 0.0014894861 |
|    clip_fraction        | 0.0151       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.105       |
|    explained_variance   | 0.482        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00409     |
|    n_updates            | 2350         |
|    policy_gradient_loss | -0.000544    |
|    value_loss           | 0.0127       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 483000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 236    |
|    time_elapsed    | 49555  |
|    total_timesteps | 483328 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 1.4        |
| time/                   |            |
|    total_timesteps      | 484000     |
| train/                  |            |
|    approx_kl            | 0.00783527 |
|    clip_fraction        | 0.0458     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.238     |
|    explained_variance   | 0.41       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00679    |
|    n_updates            | 2360       |
|    policy_gradient_loss | -0.00125   |
|    value_loss           | 0.0134     |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 485000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 237    |
|    time_elapsed    | 49760  |
|    total_timesteps | 485376 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.4          |
| time/                   |              |
|    total_timesteps      | 486000       |
| train/                  |              |
|    approx_kl            | 0.0013512245 |
|    clip_fraction        | 0.0148       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.123       |
|    explained_variance   | 0.425        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0104       |
|    n_updates            | 2370         |
|    policy_gradient_loss | -0.000729    |
|    value_loss           | 0.0135       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 487000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 238    |
|    time_elapsed    | 49966  |
|    total_timesteps | 487424 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.4          |
| time/                   |              |
|    total_timesteps      | 488000       |
| train/                  |              |
|    approx_kl            | 0.0014064125 |
|    clip_fraction        | 0.00659      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0633      |
|    explained_variance   | 0.432        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00395      |
|    n_updates            | 2380         |
|    policy_gradient_loss | -0.000836    |
|    value_loss           | 0.0139       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 489000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 239    |
|    time_elapsed    | 50171  |
|    total_timesteps | 489472 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 1.4           |
| time/                   |               |
|    total_timesteps      | 490000        |
| train/                  |               |
|    approx_kl            | 0.00073126855 |
|    clip_fraction        | 0.00806       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0662       |
|    explained_variance   | 0.441         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00127       |
|    n_updates            | 2390          |
|    policy_gradient_loss | -0.000711     |
|    value_loss           | 0.0144        |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 491000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 240    |
|    time_elapsed    | 50376  |
|    total_timesteps | 491520 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.4          |
| time/                   |              |
|    total_timesteps      | 492000       |
| train/                  |              |
|    approx_kl            | 0.0007352316 |
|    clip_fraction        | 0.00464      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0499      |
|    explained_variance   | 0.421        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00294      |
|    n_updates            | 2400         |
|    policy_gradient_loss | -0.000353    |
|    value_loss           | 0.0146       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 493000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 241    |
|    time_elapsed    | 50582  |
|    total_timesteps | 493568 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 1.4           |
| time/                   |               |
|    total_timesteps      | 494000        |
| train/                  |               |
|    approx_kl            | 0.00044235642 |
|    clip_fraction        | 0.00547       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0472       |
|    explained_variance   | 0.427         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000378      |
|    n_updates            | 2410          |
|    policy_gradient_loss | -0.000547     |
|    value_loss           | 0.0146        |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 495000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 242    |
|    time_elapsed    | 50787  |
|    total_timesteps | 495616 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.4          |
| time/                   |              |
|    total_timesteps      | 496000       |
| train/                  |              |
|    approx_kl            | 0.0011094041 |
|    clip_fraction        | 0.00601      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0436      |
|    explained_variance   | 0.399        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00207      |
|    n_updates            | 2420         |
|    policy_gradient_loss | -0.000553    |
|    value_loss           | 0.0149       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 497000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 243    |
|    time_elapsed    | 50992  |
|    total_timesteps | 497664 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 1.4           |
| time/                   |               |
|    total_timesteps      | 498000        |
| train/                  |               |
|    approx_kl            | 0.00061457884 |
|    clip_fraction        | 0.00562       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0408       |
|    explained_variance   | 0.398         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00622       |
|    n_updates            | 2430          |
|    policy_gradient_loss | -0.000178     |
|    value_loss           | 0.0148        |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 499000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 244    |
|    time_elapsed    | 51198  |
|    total_timesteps | 499712 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 1.4           |
| time/                   |               |
|    total_timesteps      | 500000        |
| train/                  |               |
|    approx_kl            | 0.00082505064 |
|    clip_fraction        | 0.00737       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0436       |
|    explained_variance   | 0.398         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000115      |
|    n_updates            | 2440          |
|    policy_gradient_loss | -0.000313     |
|    value_loss           | 0.0147        |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 501000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 245    |
|    time_elapsed    | 51403  |
|    total_timesteps | 501760 |
-------------------------------
