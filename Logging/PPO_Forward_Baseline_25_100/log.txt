Logging to ./Logging/PPO_Forward_Baseline_25_100
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
-----------------------------
| time/              |      |
|    fps             | 10   |
|    iterations      | 1    |
|    time_elapsed    | 204  |
|    total_timesteps | 2048 |
-----------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 3000        |
| train/                  |             |
|    approx_kl            | 0.018429868 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.37       |
|    explained_variance   | 0.865       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0716     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0345     |
|    value_loss           | 0.00989     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
-----------------------------
| time/              |      |
|    fps             | 10   |
|    iterations      | 2    |
|    time_elapsed    | 409  |
|    total_timesteps | 4096 |
-----------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.01758792 |
|    clip_fraction        | 0.173      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.37      |
|    explained_variance   | 0.849      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0445    |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.0322    |
|    value_loss           | 0.02       |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
-----------------------------
| time/              |      |
|    fps             | 9    |
|    iterations      | 3    |
|    time_elapsed    | 614  |
|    total_timesteps | 6144 |
-----------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 7000        |
| train/                  |             |
|    approx_kl            | 0.019220404 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.35       |
|    explained_variance   | 0.785       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0645     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0364     |
|    value_loss           | 0.00567     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
-----------------------------
| time/              |      |
|    fps             | 9    |
|    iterations      | 4    |
|    time_elapsed    | 820  |
|    total_timesteps | 8192 |
-----------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 9000        |
| train/                  |             |
|    approx_kl            | 0.019477043 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.35       |
|    explained_variance   | 0.79        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0582     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0406     |
|    value_loss           | 0.0132      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 5     |
|    time_elapsed    | 1025  |
|    total_timesteps | 10240 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 11000       |
| train/                  |             |
|    approx_kl            | 0.018561063 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.34       |
|    explained_variance   | 0.822       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0578     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0394     |
|    value_loss           | 0.00471     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 6     |
|    time_elapsed    | 1230  |
|    total_timesteps | 12288 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 13000       |
| train/                  |             |
|    approx_kl            | 0.018745106 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.33       |
|    explained_variance   | 0.543       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0669     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0399     |
|    value_loss           | 0.00475     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 7     |
|    time_elapsed    | 1436  |
|    total_timesteps | 14336 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.018955503 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.31       |
|    explained_variance   | 0.85        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0359     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0387     |
|    value_loss           | 0.00265     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 8     |
|    time_elapsed    | 1641  |
|    total_timesteps | 16384 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 17000       |
| train/                  |             |
|    approx_kl            | 0.018414171 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.3        |
|    explained_variance   | 0.798       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0246     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0304     |
|    value_loss           | 0.00224     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 9     |
|    time_elapsed    | 1846  |
|    total_timesteps | 18432 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 19000       |
| train/                  |             |
|    approx_kl            | 0.020699853 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.3        |
|    explained_variance   | 0.842       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0379     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.04       |
|    value_loss           | 0.00128     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 10    |
|    time_elapsed    | 2052  |
|    total_timesteps | 20480 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 21000       |
| train/                  |             |
|    approx_kl            | 0.021102868 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.24       |
|    explained_variance   | 0.805       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0648     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.034      |
|    value_loss           | 0.00194     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 11    |
|    time_elapsed    | 2257  |
|    total_timesteps | 22528 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 23000       |
| train/                  |             |
|    approx_kl            | 0.020745344 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.29       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0673     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0299     |
|    value_loss           | 0.00157     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 12    |
|    time_elapsed    | 2462  |
|    total_timesteps | 24576 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.021934213 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.24       |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0664     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0356     |
|    value_loss           | 0.00149     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 13    |
|    time_elapsed    | 2668  |
|    total_timesteps | 26624 |
------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 27000      |
| train/                  |            |
|    approx_kl            | 0.02222613 |
|    clip_fraction        | 0.2        |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.27      |
|    explained_variance   | 0.881      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0428    |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0369    |
|    value_loss           | 0.00267    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 14    |
|    time_elapsed    | 2873  |
|    total_timesteps | 28672 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 29000       |
| train/                  |             |
|    approx_kl            | 0.021702144 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.27       |
|    explained_variance   | 0.734       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0427     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0366     |
|    value_loss           | 0.00847     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 15    |
|    time_elapsed    | 3078  |
|    total_timesteps | 30720 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 31000       |
| train/                  |             |
|    approx_kl            | 0.017430892 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.25       |
|    explained_variance   | 0.806       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0576     |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0354     |
|    value_loss           | 0.0014      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 16    |
|    time_elapsed    | 3284  |
|    total_timesteps | 32768 |
------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 33000      |
| train/                  |            |
|    approx_kl            | 0.02690519 |
|    clip_fraction        | 0.256      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.2       |
|    explained_variance   | 0.923      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0737    |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.038     |
|    value_loss           | 0.00156    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 17    |
|    time_elapsed    | 3489  |
|    total_timesteps | 34816 |
------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.01658665 |
|    clip_fraction        | 0.196      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.16      |
|    explained_variance   | 0.831      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0173    |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.024     |
|    value_loss           | 0.00311    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 18    |
|    time_elapsed    | 3694  |
|    total_timesteps | 36864 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 37000       |
| train/                  |             |
|    approx_kl            | 0.020447489 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.2        |
|    explained_variance   | 0.775       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0691     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0353     |
|    value_loss           | 0.00368     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 19    |
|    time_elapsed    | 3900  |
|    total_timesteps | 38912 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 39000       |
| train/                  |             |
|    approx_kl            | 0.020637883 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.13       |
|    explained_variance   | 0.653       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0422     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0273     |
|    value_loss           | 0.00163     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 20    |
|    time_elapsed    | 4105  |
|    total_timesteps | 40960 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 41000       |
| train/                  |             |
|    approx_kl            | 0.023333501 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.26       |
|    explained_variance   | 0.888       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0193     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0312     |
|    value_loss           | 0.00204     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 21    |
|    time_elapsed    | 4410  |
|    total_timesteps | 43008 |
------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 44000      |
| train/                  |            |
|    approx_kl            | 0.02026509 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.25      |
|    explained_variance   | 0.765      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0454    |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.0287    |
|    value_loss           | 0.000769   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 22    |
|    time_elapsed    | 4615  |
|    total_timesteps | 45056 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 46000       |
| train/                  |             |
|    approx_kl            | 0.019186255 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.21       |
|    explained_variance   | 0.764       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0209     |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0283     |
|    value_loss           | 0.00353     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 23    |
|    time_elapsed    | 4821  |
|    total_timesteps | 47104 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 48000       |
| train/                  |             |
|    approx_kl            | 0.022230987 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.2        |
|    explained_variance   | 0.753       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0523     |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0381     |
|    value_loss           | 0.00324     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 24    |
|    time_elapsed    | 5026  |
|    total_timesteps | 49152 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.020892989 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.15       |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0514     |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0322     |
|    value_loss           | 0.00141     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 25    |
|    time_elapsed    | 5231  |
|    total_timesteps | 51200 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 52000       |
| train/                  |             |
|    approx_kl            | 0.020054787 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.16       |
|    explained_variance   | 0.745       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0208     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0366     |
|    value_loss           | 0.00157     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 26    |
|    time_elapsed    | 5437  |
|    total_timesteps | 53248 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 54000       |
| train/                  |             |
|    approx_kl            | 0.021172069 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.16       |
|    explained_variance   | 0.863       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00778    |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0357     |
|    value_loss           | 0.00203     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 27    |
|    time_elapsed    | 5642  |
|    total_timesteps | 55296 |
------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 56000      |
| train/                  |            |
|    approx_kl            | 0.01890107 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.12      |
|    explained_variance   | 0.746      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0443    |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.0346    |
|    value_loss           | 0.00205    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 28    |
|    time_elapsed    | 5847  |
|    total_timesteps | 57344 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 58000       |
| train/                  |             |
|    approx_kl            | 0.022485483 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.15       |
|    explained_variance   | 0.76        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0607     |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0344     |
|    value_loss           | 0.000934    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 29    |
|    time_elapsed    | 6053  |
|    total_timesteps | 59392 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.022320969 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.12       |
|    explained_variance   | 0.68        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0466     |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0378     |
|    value_loss           | 0.00185     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 30    |
|    time_elapsed    | 6258  |
|    total_timesteps | 61440 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 62000       |
| train/                  |             |
|    approx_kl            | 0.023166489 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.13       |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0447     |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.033      |
|    value_loss           | 0.00201     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 31    |
|    time_elapsed    | 6463  |
|    total_timesteps | 63488 |
------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 64000      |
| train/                  |            |
|    approx_kl            | 0.02245016 |
|    clip_fraction        | 0.25       |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.08      |
|    explained_variance   | 0.884      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0789    |
|    n_updates            | 310        |
|    policy_gradient_loss | -0.0356    |
|    value_loss           | 0.00238    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 32    |
|    time_elapsed    | 6669  |
|    total_timesteps | 65536 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 66000       |
| train/                  |             |
|    approx_kl            | 0.020225665 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.07       |
|    explained_variance   | 0.687       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.054      |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0382     |
|    value_loss           | 0.00224     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 33    |
|    time_elapsed    | 6874  |
|    total_timesteps | 67584 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 68000       |
| train/                  |             |
|    approx_kl            | 0.017597046 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.95       |
|    explained_variance   | 0.785       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0378     |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.0268     |
|    value_loss           | 0.000354    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 34    |
|    time_elapsed    | 7079  |
|    total_timesteps | 69632 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.022325274 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.04       |
|    explained_variance   | 0.84        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0489     |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.0382     |
|    value_loss           | 0.00132     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 35    |
|    time_elapsed    | 7285  |
|    total_timesteps | 71680 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 72000       |
| train/                  |             |
|    approx_kl            | 0.023101287 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.12       |
|    explained_variance   | 0.82        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0437     |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.0372     |
|    value_loss           | 0.000874    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 36    |
|    time_elapsed    | 7490  |
|    total_timesteps | 73728 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 74000       |
| train/                  |             |
|    approx_kl            | 0.020401057 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.01       |
|    explained_variance   | 0.818       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0472     |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.0376     |
|    value_loss           | 0.00211     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 37    |
|    time_elapsed    | 7695  |
|    total_timesteps | 75776 |
------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 76000      |
| train/                  |            |
|    approx_kl            | 0.02097959 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.06      |
|    explained_variance   | 0.872      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0481    |
|    n_updates            | 370        |
|    policy_gradient_loss | -0.036     |
|    value_loss           | 0.00207    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 38    |
|    time_elapsed    | 7900  |
|    total_timesteps | 77824 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 78000       |
| train/                  |             |
|    approx_kl            | 0.029140033 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8          |
|    explained_variance   | 0.813       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0396     |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.0327     |
|    value_loss           | 0.000809    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 39    |
|    time_elapsed    | 8106  |
|    total_timesteps | 79872 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.019957267 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.99       |
|    explained_variance   | 0.871       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0384     |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.0308     |
|    value_loss           | 0.000664    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 40    |
|    time_elapsed    | 8311  |
|    total_timesteps | 81920 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 82000       |
| train/                  |             |
|    approx_kl            | 0.021939956 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.98       |
|    explained_variance   | 0.821       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0607     |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.0317     |
|    value_loss           | 0.00254     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 41    |
|    time_elapsed    | 8516  |
|    total_timesteps | 83968 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 84000       |
| train/                  |             |
|    approx_kl            | 0.021947656 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.94       |
|    explained_variance   | 0.811       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0359     |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.0367     |
|    value_loss           | 0.00134     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 42    |
|    time_elapsed    | 8822  |
|    total_timesteps | 86016 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 87000       |
| train/                  |             |
|    approx_kl            | 0.020618454 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.05       |
|    explained_variance   | 0.797       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0488     |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0337     |
|    value_loss           | 0.00317     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 43    |
|    time_elapsed    | 9027  |
|    total_timesteps | 88064 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 89000       |
| train/                  |             |
|    approx_kl            | 0.023337541 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.87       |
|    explained_variance   | 0.853       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0354     |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.0361     |
|    value_loss           | 0.000914    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 44    |
|    time_elapsed    | 9232  |
|    total_timesteps | 90112 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 91000       |
| train/                  |             |
|    approx_kl            | 0.020753467 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.1        |
|    explained_variance   | 0.874       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0366     |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.0376     |
|    value_loss           | 0.00204     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 45    |
|    time_elapsed    | 9438  |
|    total_timesteps | 92160 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 93000       |
| train/                  |             |
|    approx_kl            | 0.026389726 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.91       |
|    explained_variance   | 0.822       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0476     |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.0427     |
|    value_loss           | 0.00102     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 46    |
|    time_elapsed    | 9643  |
|    total_timesteps | 94208 |
------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 95000      |
| train/                  |            |
|    approx_kl            | 0.02155983 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.94      |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0405    |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.0366    |
|    value_loss           | 0.00188    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 47    |
|    time_elapsed    | 9848  |
|    total_timesteps | 96256 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 97000       |
| train/                  |             |
|    approx_kl            | 0.024665734 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.93       |
|    explained_variance   | 0.865       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0526     |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.0409     |
|    value_loss           | 0.000551    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 48    |
|    time_elapsed    | 10054 |
|    total_timesteps | 98304 |
------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 99000      |
| train/                  |            |
|    approx_kl            | 0.03238401 |
|    clip_fraction        | 0.256      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.07      |
|    explained_variance   | 0.915      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0738    |
|    n_updates            | 480        |
|    policy_gradient_loss | -0.0394    |
|    value_loss           | 0.00114    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 49     |
|    time_elapsed    | 10259  |
|    total_timesteps | 100352 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 101000      |
| train/                  |             |
|    approx_kl            | 0.021023853 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.94       |
|    explained_variance   | 0.764       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0403     |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.0364     |
|    value_loss           | 0.0016      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 50     |
|    time_elapsed    | 10464  |
|    total_timesteps | 102400 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 103000      |
| train/                  |             |
|    approx_kl            | 0.022091962 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.02       |
|    explained_variance   | 0.671       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0728     |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.0416     |
|    value_loss           | 0.00186     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 51     |
|    time_elapsed    | 10669  |
|    total_timesteps | 104448 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 105000      |
| train/                  |             |
|    approx_kl            | 0.018125765 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.97       |
|    explained_variance   | 0.808       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0454     |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.0327     |
|    value_loss           | 0.000772    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 52     |
|    time_elapsed    | 10875  |
|    total_timesteps | 106496 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 107000      |
| train/                  |             |
|    approx_kl            | 0.019835688 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.97       |
|    explained_variance   | 0.887       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0278     |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.0248     |
|    value_loss           | 0.001       |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 53     |
|    time_elapsed    | 11080  |
|    total_timesteps | 108544 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 109000      |
| train/                  |             |
|    approx_kl            | 0.021071862 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.99       |
|    explained_variance   | 0.674       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0473     |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.0383     |
|    value_loss           | 0.000793    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 54     |
|    time_elapsed    | 11286  |
|    total_timesteps | 110592 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 111000      |
| train/                  |             |
|    approx_kl            | 0.022349311 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.9        |
|    explained_variance   | 0.778       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0229     |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.0288     |
|    value_loss           | 0.000956    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 55     |
|    time_elapsed    | 11491  |
|    total_timesteps | 112640 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 113000      |
| train/                  |             |
|    approx_kl            | 0.022488486 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.84       |
|    explained_variance   | 0.843       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0609     |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.0392     |
|    value_loss           | 0.00133     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 56     |
|    time_elapsed    | 11696  |
|    total_timesteps | 114688 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 115000      |
| train/                  |             |
|    approx_kl            | 0.020266702 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.92       |
|    explained_variance   | 0.673       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0746     |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.0377     |
|    value_loss           | 0.00124     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 57     |
|    time_elapsed    | 11901  |
|    total_timesteps | 116736 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 117000      |
| train/                  |             |
|    approx_kl            | 0.020301433 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.91       |
|    explained_variance   | 0.608       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0697     |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.0357     |
|    value_loss           | 0.00135     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 58     |
|    time_elapsed    | 12107  |
|    total_timesteps | 118784 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 119000      |
| train/                  |             |
|    approx_kl            | 0.020780435 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.02       |
|    explained_variance   | 0.73        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0399     |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.0355     |
|    value_loss           | 0.00125     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 59     |
|    time_elapsed    | 12312  |
|    total_timesteps | 120832 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 121000      |
| train/                  |             |
|    approx_kl            | 0.022567248 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.89       |
|    explained_variance   | 0.809       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.041      |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.0345     |
|    value_loss           | 0.00094     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 60     |
|    time_elapsed    | 12517  |
|    total_timesteps | 122880 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 123000     |
| train/                  |            |
|    approx_kl            | 0.02133875 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.87      |
|    explained_variance   | 0.561      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0187    |
|    n_updates            | 600        |
|    policy_gradient_loss | -0.0272    |
|    value_loss           | 0.000607   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 61     |
|    time_elapsed    | 12723  |
|    total_timesteps | 124928 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 125000      |
| train/                  |             |
|    approx_kl            | 0.021533417 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.99       |
|    explained_variance   | 0.749       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0473     |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.0354     |
|    value_loss           | 0.00192     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 62     |
|    time_elapsed    | 12928  |
|    total_timesteps | 126976 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 127000      |
| train/                  |             |
|    approx_kl            | 0.026146278 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.73       |
|    explained_variance   | 0.886       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0583     |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.0385     |
|    value_loss           | 0.00116     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 63     |
|    time_elapsed    | 13234  |
|    total_timesteps | 129024 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 130000     |
| train/                  |            |
|    approx_kl            | 0.02284369 |
|    clip_fraction        | 0.275      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.85      |
|    explained_variance   | 0.824      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0603    |
|    n_updates            | 630        |
|    policy_gradient_loss | -0.039     |
|    value_loss           | 0.00111    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 64     |
|    time_elapsed    | 13439  |
|    total_timesteps | 131072 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 132000     |
| train/                  |            |
|    approx_kl            | 0.01772935 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.73      |
|    explained_variance   | 0.64       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.052     |
|    n_updates            | 640        |
|    policy_gradient_loss | -0.0313    |
|    value_loss           | 0.000538   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 65     |
|    time_elapsed    | 13644  |
|    total_timesteps | 133120 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 134000      |
| train/                  |             |
|    approx_kl            | 0.019862596 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.76       |
|    explained_variance   | 0.801       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0166     |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.0319     |
|    value_loss           | 0.000411    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 66     |
|    time_elapsed    | 13849  |
|    total_timesteps | 135168 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 136000      |
| train/                  |             |
|    approx_kl            | 0.019277696 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.75       |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.047      |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.0266     |
|    value_loss           | 0.00149     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 67     |
|    time_elapsed    | 14055  |
|    total_timesteps | 137216 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 138000      |
| train/                  |             |
|    approx_kl            | 0.021942683 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.76       |
|    explained_variance   | 0.496       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.081      |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.0389     |
|    value_loss           | 0.000635    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 68     |
|    time_elapsed    | 14260  |
|    total_timesteps | 139264 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 140000     |
| train/                  |            |
|    approx_kl            | 0.01941362 |
|    clip_fraction        | 0.238      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.74      |
|    explained_variance   | 0.8        |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0512    |
|    n_updates            | 680        |
|    policy_gradient_loss | -0.0298    |
|    value_loss           | 0.00098    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 69     |
|    time_elapsed    | 14465  |
|    total_timesteps | 141312 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 142000      |
| train/                  |             |
|    approx_kl            | 0.020676143 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.64       |
|    explained_variance   | 0.655       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0232     |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.0291     |
|    value_loss           | 0.000694    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 70     |
|    time_elapsed    | 14671  |
|    total_timesteps | 143360 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 144000     |
| train/                  |            |
|    approx_kl            | 0.01733424 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.76      |
|    explained_variance   | 0.898      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0691    |
|    n_updates            | 700        |
|    policy_gradient_loss | -0.0296    |
|    value_loss           | 0.00114    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 71     |
|    time_elapsed    | 14876  |
|    total_timesteps | 145408 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 146000      |
| train/                  |             |
|    approx_kl            | 0.019305442 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.82       |
|    explained_variance   | 0.681       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0636     |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.0322     |
|    value_loss           | 0.000828    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 72     |
|    time_elapsed    | 15081  |
|    total_timesteps | 147456 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 148000     |
| train/                  |            |
|    approx_kl            | 0.02182357 |
|    clip_fraction        | 0.253      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.73      |
|    explained_variance   | 0.766      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00745   |
|    n_updates            | 720        |
|    policy_gradient_loss | -0.0347    |
|    value_loss           | 0.00116    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 73     |
|    time_elapsed    | 15287  |
|    total_timesteps | 149504 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 150000     |
| train/                  |            |
|    approx_kl            | 0.02112911 |
|    clip_fraction        | 0.234      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.63      |
|    explained_variance   | 0.741      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0534    |
|    n_updates            | 730        |
|    policy_gradient_loss | -0.0328    |
|    value_loss           | 0.000912   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 151000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 74     |
|    time_elapsed    | 15492  |
|    total_timesteps | 151552 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 152000      |
| train/                  |             |
|    approx_kl            | 0.018870678 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.7        |
|    explained_variance   | 0.685       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.046      |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.0268     |
|    value_loss           | 0.000929    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 153000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 75     |
|    time_elapsed    | 15697  |
|    total_timesteps | 153600 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 154000      |
| train/                  |             |
|    approx_kl            | 0.025308056 |
|    clip_fraction        | 0.294       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.55       |
|    explained_variance   | 0.696       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0565     |
|    n_updates            | 750         |
|    policy_gradient_loss | -0.0351     |
|    value_loss           | 0.00109     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 76     |
|    time_elapsed    | 15903  |
|    total_timesteps | 155648 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 156000      |
| train/                  |             |
|    approx_kl            | 0.021010365 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.53       |
|    explained_variance   | 0.67        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0218     |
|    n_updates            | 760         |
|    policy_gradient_loss | -0.0278     |
|    value_loss           | 0.000602    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 77     |
|    time_elapsed    | 16108  |
|    total_timesteps | 157696 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 158000      |
| train/                  |             |
|    approx_kl            | 0.020413494 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.6        |
|    explained_variance   | 0.664       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0651     |
|    n_updates            | 770         |
|    policy_gradient_loss | -0.0281     |
|    value_loss           | 0.000824    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 159000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 78     |
|    time_elapsed    | 16313  |
|    total_timesteps | 159744 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 160000      |
| train/                  |             |
|    approx_kl            | 0.020254556 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.63       |
|    explained_variance   | 0.507       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0477     |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.0277     |
|    value_loss           | 0.00109     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 161000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 79     |
|    time_elapsed    | 16519  |
|    total_timesteps | 161792 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 162000      |
| train/                  |             |
|    approx_kl            | 0.017609088 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.61       |
|    explained_variance   | 0.694       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0216     |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.0247     |
|    value_loss           | 0.00222     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 163000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 80     |
|    time_elapsed    | 16724  |
|    total_timesteps | 163840 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 164000      |
| train/                  |             |
|    approx_kl            | 0.019553386 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.66       |
|    explained_variance   | 0.587       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0348     |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.0277     |
|    value_loss           | 0.000833    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 165000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 81     |
|    time_elapsed    | 16929  |
|    total_timesteps | 165888 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 166000      |
| train/                  |             |
|    approx_kl            | 0.019270327 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.59       |
|    explained_variance   | 0.535       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0299     |
|    n_updates            | 810         |
|    policy_gradient_loss | -0.0285     |
|    value_loss           | 0.000855    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 167000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 82     |
|    time_elapsed    | 17134  |
|    total_timesteps | 167936 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 168000      |
| train/                  |             |
|    approx_kl            | 0.017931536 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.44       |
|    explained_variance   | 0.784       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0482     |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.0288     |
|    value_loss           | 0.000998    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 169000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 83     |
|    time_elapsed    | 17340  |
|    total_timesteps | 169984 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 170000     |
| train/                  |            |
|    approx_kl            | 0.01972509 |
|    clip_fraction        | 0.234      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.44      |
|    explained_variance   | -0.0546    |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0144    |
|    n_updates            | 830        |
|    policy_gradient_loss | -0.0315    |
|    value_loss           | 0.00065    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 84     |
|    time_elapsed    | 17645  |
|    total_timesteps | 172032 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 173000      |
| train/                  |             |
|    approx_kl            | 0.022408795 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.45       |
|    explained_variance   | 0.621       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0639     |
|    n_updates            | 840         |
|    policy_gradient_loss | -0.0382     |
|    value_loss           | 0.000896    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 85     |
|    time_elapsed    | 17850  |
|    total_timesteps | 174080 |
-------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+04     |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 175000    |
| train/                  |           |
|    approx_kl            | 0.0196206 |
|    clip_fraction        | 0.237     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.56     |
|    explained_variance   | 0.489     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.031    |
|    n_updates            | 850       |
|    policy_gradient_loss | -0.0306   |
|    value_loss           | 0.00093   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 86     |
|    time_elapsed    | 18056  |
|    total_timesteps | 176128 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 177000     |
| train/                  |            |
|    approx_kl            | 0.01976365 |
|    clip_fraction        | 0.234      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.58      |
|    explained_variance   | 0.694      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0428    |
|    n_updates            | 860        |
|    policy_gradient_loss | -0.0252    |
|    value_loss           | 0.00131    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 87     |
|    time_elapsed    | 18261  |
|    total_timesteps | 178176 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 179000      |
| train/                  |             |
|    approx_kl            | 0.020167675 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.38       |
|    explained_variance   | 0.665       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0273     |
|    n_updates            | 870         |
|    policy_gradient_loss | -0.0327     |
|    value_loss           | 0.00112     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 88     |
|    time_elapsed    | 18466  |
|    total_timesteps | 180224 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 181000      |
| train/                  |             |
|    approx_kl            | 0.022693668 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.56       |
|    explained_variance   | 0.475       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0291     |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.034      |
|    value_loss           | 0.0011      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 89     |
|    time_elapsed    | 18672  |
|    total_timesteps | 182272 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 183000      |
| train/                  |             |
|    approx_kl            | 0.021086564 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.33       |
|    explained_variance   | 0.654       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0429     |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.0269     |
|    value_loss           | 0.00139     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 90     |
|    time_elapsed    | 18877  |
|    total_timesteps | 184320 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 185000      |
| train/                  |             |
|    approx_kl            | 0.021998253 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.48       |
|    explained_variance   | 0.646       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0289     |
|    n_updates            | 900         |
|    policy_gradient_loss | -0.0276     |
|    value_loss           | 0.00188     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 91     |
|    time_elapsed    | 19082  |
|    total_timesteps | 186368 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 187000      |
| train/                  |             |
|    approx_kl            | 0.020565195 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.45       |
|    explained_variance   | 0.313       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0294     |
|    n_updates            | 910         |
|    policy_gradient_loss | -0.0289     |
|    value_loss           | 0.00125     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 92     |
|    time_elapsed    | 19288  |
|    total_timesteps | 188416 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 189000     |
| train/                  |            |
|    approx_kl            | 0.01660122 |
|    clip_fraction        | 0.192      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.32      |
|    explained_variance   | 0.416      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.057     |
|    n_updates            | 920        |
|    policy_gradient_loss | -0.0252    |
|    value_loss           | 0.00112    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 93     |
|    time_elapsed    | 19493  |
|    total_timesteps | 190464 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 191000      |
| train/                  |             |
|    approx_kl            | 0.020476215 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.44       |
|    explained_variance   | 0.269       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0463     |
|    n_updates            | 930         |
|    policy_gradient_loss | -0.0266     |
|    value_loss           | 0.0012      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 94     |
|    time_elapsed    | 19698  |
|    total_timesteps | 192512 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 193000      |
| train/                  |             |
|    approx_kl            | 0.020904996 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.38       |
|    explained_variance   | 0.732       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0448     |
|    n_updates            | 940         |
|    policy_gradient_loss | -0.0295     |
|    value_loss           | 0.00135     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 95     |
|    time_elapsed    | 19904  |
|    total_timesteps | 194560 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 195000      |
| train/                  |             |
|    approx_kl            | 0.021406386 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.24       |
|    explained_variance   | 0.638       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0416     |
|    n_updates            | 950         |
|    policy_gradient_loss | -0.021      |
|    value_loss           | 0.00192     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 96     |
|    time_elapsed    | 20109  |
|    total_timesteps | 196608 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 197000      |
| train/                  |             |
|    approx_kl            | 0.022340178 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.29       |
|    explained_variance   | 0.857       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0453     |
|    n_updates            | 960         |
|    policy_gradient_loss | -0.0266     |
|    value_loss           | 0.00149     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 97     |
|    time_elapsed    | 20314  |
|    total_timesteps | 198656 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 199000      |
| train/                  |             |
|    approx_kl            | 0.018958349 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.36       |
|    explained_variance   | 0.605       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0474     |
|    n_updates            | 970         |
|    policy_gradient_loss | -0.0294     |
|    value_loss           | 0.00136     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 98     |
|    time_elapsed    | 20520  |
|    total_timesteps | 200704 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 201000     |
| train/                  |            |
|    approx_kl            | 0.02608674 |
|    clip_fraction        | 0.299      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.31      |
|    explained_variance   | 0.634      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0524    |
|    n_updates            | 980        |
|    policy_gradient_loss | -0.0338    |
|    value_loss           | 0.00161    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 99     |
|    time_elapsed    | 20725  |
|    total_timesteps | 202752 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.4        |
| time/                   |            |
|    total_timesteps      | 203000     |
| train/                  |            |
|    approx_kl            | 0.02149291 |
|    clip_fraction        | 0.227      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.33      |
|    explained_variance   | 0.621      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0317    |
|    n_updates            | 990        |
|    policy_gradient_loss | -0.0277    |
|    value_loss           | 0.000965   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 100    |
|    time_elapsed    | 20930  |
|    total_timesteps | 204800 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 205000      |
| train/                  |             |
|    approx_kl            | 0.019488394 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.31       |
|    explained_variance   | 0.813       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0159      |
|    n_updates            | 1000        |
|    policy_gradient_loss | -0.0243     |
|    value_loss           | 0.00125     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 101    |
|    time_elapsed    | 21136  |
|    total_timesteps | 206848 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 207000      |
| train/                  |             |
|    approx_kl            | 0.022308564 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.35       |
|    explained_variance   | 0.574       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0359     |
|    n_updates            | 1010        |
|    policy_gradient_loss | -0.024      |
|    value_loss           | 0.00169     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 102    |
|    time_elapsed    | 21341  |
|    total_timesteps | 208896 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 209000      |
| train/                  |             |
|    approx_kl            | 0.016498556 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.31       |
|    explained_variance   | 0.369       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0141      |
|    n_updates            | 1020        |
|    policy_gradient_loss | -0.0189     |
|    value_loss           | 0.00124     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 103    |
|    time_elapsed    | 21546  |
|    total_timesteps | 210944 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 211000     |
| train/                  |            |
|    approx_kl            | 0.02364495 |
|    clip_fraction        | 0.24       |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.08      |
|    explained_variance   | 0.771      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00259   |
|    n_updates            | 1030       |
|    policy_gradient_loss | -0.0262    |
|    value_loss           | 0.00127    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 104    |
|    time_elapsed    | 21752  |
|    total_timesteps | 212992 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 213000      |
| train/                  |             |
|    approx_kl            | 0.022011394 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.2        |
|    explained_variance   | 0.758       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0349     |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.0271     |
|    value_loss           | 0.00116     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 105    |
|    time_elapsed    | 22057  |
|    total_timesteps | 215040 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 216000      |
| train/                  |             |
|    approx_kl            | 0.019042037 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.09       |
|    explained_variance   | 0.596       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0228     |
|    n_updates            | 1050        |
|    policy_gradient_loss | -0.0288     |
|    value_loss           | 0.00114     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 106    |
|    time_elapsed    | 22262  |
|    total_timesteps | 217088 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 218000      |
| train/                  |             |
|    approx_kl            | 0.020684512 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.11       |
|    explained_variance   | 0.638       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0212     |
|    n_updates            | 1060        |
|    policy_gradient_loss | -0.0262     |
|    value_loss           | 0.00142     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 107    |
|    time_elapsed    | 22468  |
|    total_timesteps | 219136 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 220000      |
| train/                  |             |
|    approx_kl            | 0.019779585 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.19       |
|    explained_variance   | 0.555       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0549     |
|    n_updates            | 1070        |
|    policy_gradient_loss | -0.0265     |
|    value_loss           | 0.00108     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 108    |
|    time_elapsed    | 22673  |
|    total_timesteps | 221184 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 222000      |
| train/                  |             |
|    approx_kl            | 0.023116868 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.93       |
|    explained_variance   | 0.557       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.055      |
|    n_updates            | 1080        |
|    policy_gradient_loss | -0.0312     |
|    value_loss           | 0.00125     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 109    |
|    time_elapsed    | 22878  |
|    total_timesteps | 223232 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 224000      |
| train/                  |             |
|    approx_kl            | 0.014284825 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.03       |
|    explained_variance   | 0.593       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0271     |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.001       |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 110    |
|    time_elapsed    | 23084  |
|    total_timesteps | 225280 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.4        |
| time/                   |            |
|    total_timesteps      | 226000     |
| train/                  |            |
|    approx_kl            | 0.01767085 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.74      |
|    explained_variance   | 0.567      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0593    |
|    n_updates            | 1100       |
|    policy_gradient_loss | -0.0234    |
|    value_loss           | 0.000961   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 111    |
|    time_elapsed    | 23289  |
|    total_timesteps | 227328 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 228000      |
| train/                  |             |
|    approx_kl            | 0.022874001 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7          |
|    explained_variance   | 0.801       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0736     |
|    n_updates            | 1110        |
|    policy_gradient_loss | -0.0275     |
|    value_loss           | 0.00117     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 112    |
|    time_elapsed    | 23494  |
|    total_timesteps | 229376 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 230000      |
| train/                  |             |
|    approx_kl            | 0.020576991 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.79       |
|    explained_variance   | 0.729       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0435     |
|    n_updates            | 1120        |
|    policy_gradient_loss | -0.0291     |
|    value_loss           | 0.00121     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 113    |
|    time_elapsed    | 23700  |
|    total_timesteps | 231424 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 232000      |
| train/                  |             |
|    approx_kl            | 0.021480218 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.92       |
|    explained_variance   | 0.597       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0277     |
|    n_updates            | 1130        |
|    policy_gradient_loss | -0.027      |
|    value_loss           | 0.00166     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 114    |
|    time_elapsed    | 23905  |
|    total_timesteps | 233472 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 234000      |
| train/                  |             |
|    approx_kl            | 0.018283548 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.95       |
|    explained_variance   | 0.644       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0301     |
|    n_updates            | 1140        |
|    policy_gradient_loss | -0.0273     |
|    value_loss           | 0.00146     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 115    |
|    time_elapsed    | 24110  |
|    total_timesteps | 235520 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 236000      |
| train/                  |             |
|    approx_kl            | 0.018618371 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.61       |
|    explained_variance   | 0.383       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0156     |
|    n_updates            | 1150        |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 0.0025      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 237000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 116    |
|    time_elapsed    | 24316  |
|    total_timesteps | 237568 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 238000      |
| train/                  |             |
|    approx_kl            | 0.015442826 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.57       |
|    explained_variance   | 0.472       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0217     |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.00205     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 239000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 117    |
|    time_elapsed    | 24521  |
|    total_timesteps | 239616 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 0.017440416 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.7        |
|    explained_variance   | 0.588       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0262     |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.026      |
|    value_loss           | 0.00189     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 241000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 118    |
|    time_elapsed    | 24726  |
|    total_timesteps | 241664 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 242000      |
| train/                  |             |
|    approx_kl            | 0.019204846 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.37       |
|    explained_variance   | 0.455       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.031      |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.021      |
|    value_loss           | 0.00196     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 243000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 119    |
|    time_elapsed    | 24932  |
|    total_timesteps | 243712 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 244000      |
| train/                  |             |
|    approx_kl            | 0.021320857 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.54       |
|    explained_variance   | 0.42        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0432     |
|    n_updates            | 1190        |
|    policy_gradient_loss | -0.0227     |
|    value_loss           | 0.00272     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 120    |
|    time_elapsed    | 25137  |
|    total_timesteps | 245760 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 246000      |
| train/                  |             |
|    approx_kl            | 0.022372762 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.64       |
|    explained_variance   | 0.583       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0311     |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.0234     |
|    value_loss           | 0.00196     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 247000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 121    |
|    time_elapsed    | 25342  |
|    total_timesteps | 247808 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.3        |
| time/                   |            |
|    total_timesteps      | 248000     |
| train/                  |            |
|    approx_kl            | 0.02760583 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.56      |
|    explained_variance   | 0.581      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0449    |
|    n_updates            | 1210       |
|    policy_gradient_loss | -0.021     |
|    value_loss           | 0.00275    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 122    |
|    time_elapsed    | 25547  |
|    total_timesteps | 249856 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.022137536 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.44       |
|    explained_variance   | 0.578       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0204     |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.0191     |
|    value_loss           | 0.00226     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 123    |
|    time_elapsed    | 25753  |
|    total_timesteps | 251904 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 252000      |
| train/                  |             |
|    approx_kl            | 0.024078634 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.4        |
|    explained_variance   | 0.556       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0569     |
|    n_updates            | 1230        |
|    policy_gradient_loss | -0.0228     |
|    value_loss           | 0.00294     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 253000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 124    |
|    time_elapsed    | 25958  |
|    total_timesteps | 253952 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.6        |
| time/                   |            |
|    total_timesteps      | 254000     |
| train/                  |            |
|    approx_kl            | 0.01516055 |
|    clip_fraction        | 0.192      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.27      |
|    explained_variance   | 0.498      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0146    |
|    n_updates            | 1240       |
|    policy_gradient_loss | -0.0163    |
|    value_loss           | 0.00215    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 255000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 125    |
|    time_elapsed    | 26264  |
|    total_timesteps | 256000 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 257000      |
| train/                  |             |
|    approx_kl            | 0.016979177 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.42       |
|    explained_variance   | 0.446       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0137      |
|    n_updates            | 1250        |
|    policy_gradient_loss | -0.0198     |
|    value_loss           | 0.00193     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 126    |
|    time_elapsed    | 26469  |
|    total_timesteps | 258048 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 259000      |
| train/                  |             |
|    approx_kl            | 0.018380783 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.09       |
|    explained_variance   | 0.498       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0237     |
|    n_updates            | 1260        |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.00273     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 127    |
|    time_elapsed    | 26674  |
|    total_timesteps | 260096 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 261000      |
| train/                  |             |
|    approx_kl            | 0.018938962 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6          |
|    explained_variance   | 0.627       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0176     |
|    n_updates            | 1270        |
|    policy_gradient_loss | -0.0207     |
|    value_loss           | 0.00237     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 128    |
|    time_elapsed    | 26880  |
|    total_timesteps | 262144 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 263000      |
| train/                  |             |
|    approx_kl            | 0.017282065 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.99       |
|    explained_variance   | 0.598       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0441     |
|    n_updates            | 1280        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.00252     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 129    |
|    time_elapsed    | 27085  |
|    total_timesteps | 264192 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 265000      |
| train/                  |             |
|    approx_kl            | 0.023451522 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.39       |
|    explained_variance   | 0.635       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0544     |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.0221     |
|    value_loss           | 0.00243     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 130    |
|    time_elapsed    | 27290  |
|    total_timesteps | 266240 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 267000      |
| train/                  |             |
|    approx_kl            | 0.017475277 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.86       |
|    explained_variance   | 0.612       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0313     |
|    n_updates            | 1300        |
|    policy_gradient_loss | -0.0198     |
|    value_loss           | 0.00232     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 131    |
|    time_elapsed    | 27495  |
|    total_timesteps | 268288 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 269000      |
| train/                  |             |
|    approx_kl            | 0.019309625 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.87       |
|    explained_variance   | 0.842       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0227     |
|    n_updates            | 1310        |
|    policy_gradient_loss | -0.0242     |
|    value_loss           | 0.00183     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 132    |
|    time_elapsed    | 27701  |
|    total_timesteps | 270336 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 271000      |
| train/                  |             |
|    approx_kl            | 0.024228163 |
|    clip_fraction        | 0.285       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.53       |
|    explained_variance   | 0.409       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0141     |
|    n_updates            | 1320        |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.00388     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 133    |
|    time_elapsed    | 27906  |
|    total_timesteps | 272384 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 273000      |
| train/                  |             |
|    approx_kl            | 0.024940979 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.65       |
|    explained_variance   | 0.75        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00603    |
|    n_updates            | 1330        |
|    policy_gradient_loss | -0.0186     |
|    value_loss           | 0.00297     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 134    |
|    time_elapsed    | 28111  |
|    total_timesteps | 274432 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 275000      |
| train/                  |             |
|    approx_kl            | 0.021726955 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.12       |
|    explained_variance   | 0.789       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0373     |
|    n_updates            | 1340        |
|    policy_gradient_loss | -0.0281     |
|    value_loss           | 0.00185     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 135    |
|    time_elapsed    | 28317  |
|    total_timesteps | 276480 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 277000      |
| train/                  |             |
|    approx_kl            | 0.017544923 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.91       |
|    explained_variance   | 0.554       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0236     |
|    n_updates            | 1350        |
|    policy_gradient_loss | -0.0223     |
|    value_loss           | 0.00175     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 278000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 136    |
|    time_elapsed    | 28522  |
|    total_timesteps | 278528 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 279000      |
| train/                  |             |
|    approx_kl            | 0.023833577 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.84       |
|    explained_variance   | 0.615       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0108     |
|    n_updates            | 1360        |
|    policy_gradient_loss | -0.0209     |
|    value_loss           | 0.00247     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 280000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 137    |
|    time_elapsed    | 28727  |
|    total_timesteps | 280576 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.6        |
| time/                   |            |
|    total_timesteps      | 281000     |
| train/                  |            |
|    approx_kl            | 0.01808782 |
|    clip_fraction        | 0.233      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.19      |
|    explained_variance   | 0.547      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.036     |
|    n_updates            | 1370       |
|    policy_gradient_loss | -0.0187    |
|    value_loss           | 0.00212    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 282000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 138    |
|    time_elapsed    | 28933  |
|    total_timesteps | 282624 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 283000      |
| train/                  |             |
|    approx_kl            | 0.017497215 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.42       |
|    explained_variance   | 0.68        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0339     |
|    n_updates            | 1380        |
|    policy_gradient_loss | -0.0201     |
|    value_loss           | 0.00251     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 284000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 139    |
|    time_elapsed    | 29138  |
|    total_timesteps | 284672 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 285000      |
| train/                  |             |
|    approx_kl            | 0.025340702 |
|    clip_fraction        | 0.315       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.32       |
|    explained_variance   | 0.732       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0243     |
|    n_updates            | 1390        |
|    policy_gradient_loss | -0.0225     |
|    value_loss           | 0.0026      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 286000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 140    |
|    time_elapsed    | 29343  |
|    total_timesteps | 286720 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 287000      |
| train/                  |             |
|    approx_kl            | 0.018512163 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.19       |
|    explained_variance   | 0.339       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0417     |
|    n_updates            | 1400        |
|    policy_gradient_loss | -0.0207     |
|    value_loss           | 0.00253     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 288000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 141    |
|    time_elapsed    | 29549  |
|    total_timesteps | 288768 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.6        |
| time/                   |            |
|    total_timesteps      | 289000     |
| train/                  |            |
|    approx_kl            | 0.01709883 |
|    clip_fraction        | 0.205      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.84      |
|    explained_variance   | 0.463      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0127    |
|    n_updates            | 1410       |
|    policy_gradient_loss | -0.015     |
|    value_loss           | 0.00278    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 290000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 142    |
|    time_elapsed    | 29754  |
|    total_timesteps | 290816 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 291000      |
| train/                  |             |
|    approx_kl            | 0.018904155 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.02       |
|    explained_variance   | 0.435       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0271     |
|    n_updates            | 1420        |
|    policy_gradient_loss | -0.0211     |
|    value_loss           | 0.00313     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 292000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 143    |
|    time_elapsed    | 29959  |
|    total_timesteps | 292864 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 293000      |
| train/                  |             |
|    approx_kl            | 0.018102318 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.04       |
|    explained_variance   | 0.625       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0165      |
|    n_updates            | 1430        |
|    policy_gradient_loss | -0.0209     |
|    value_loss           | 0.00234     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 294000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 144    |
|    time_elapsed    | 30165  |
|    total_timesteps | 294912 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 295000      |
| train/                  |             |
|    approx_kl            | 0.019661816 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.75       |
|    explained_variance   | 0.64        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0126     |
|    n_updates            | 1440        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.00291     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 296000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 145    |
|    time_elapsed    | 30370  |
|    total_timesteps | 296960 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 297000      |
| train/                  |             |
|    approx_kl            | 0.015003271 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.03       |
|    explained_variance   | 0.578       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0297     |
|    n_updates            | 1450        |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.0021      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 298000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 146    |
|    time_elapsed    | 30675  |
|    total_timesteps | 299008 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 300000      |
| train/                  |             |
|    approx_kl            | 0.018447746 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.87       |
|    explained_variance   | 0.595       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0188     |
|    n_updates            | 1460        |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.00416     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 301000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 147    |
|    time_elapsed    | 30881  |
|    total_timesteps | 301056 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 302000      |
| train/                  |             |
|    approx_kl            | 0.017718643 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.77       |
|    explained_variance   | 0.702       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0321     |
|    n_updates            | 1470        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.00275     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 303000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 148    |
|    time_elapsed    | 31086  |
|    total_timesteps | 303104 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 304000      |
| train/                  |             |
|    approx_kl            | 0.019112768 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.48       |
|    explained_variance   | 0.46        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0248     |
|    n_updates            | 1480        |
|    policy_gradient_loss | -0.0187     |
|    value_loss           | 0.00375     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 305000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 149    |
|    time_elapsed    | 31291  |
|    total_timesteps | 305152 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 306000      |
| train/                  |             |
|    approx_kl            | 0.016459592 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.55       |
|    explained_variance   | 0.792       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0212     |
|    n_updates            | 1490        |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 0.00253     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 307000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 150    |
|    time_elapsed    | 31497  |
|    total_timesteps | 307200 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 308000      |
| train/                  |             |
|    approx_kl            | 0.015945394 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.48       |
|    explained_variance   | 0.7         |
|    learning_rate        | 0.0003      |
|    loss                 | -0.011      |
|    n_updates            | 1500        |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.00274     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 309000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 151    |
|    time_elapsed    | 31702  |
|    total_timesteps | 309248 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 310000      |
| train/                  |             |
|    approx_kl            | 0.018484358 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.53       |
|    explained_variance   | 0.673       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0117      |
|    n_updates            | 1510        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.00288     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 311000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 152    |
|    time_elapsed    | 31907  |
|    total_timesteps | 311296 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.6        |
| time/                   |            |
|    total_timesteps      | 312000     |
| train/                  |            |
|    approx_kl            | 0.01715469 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.75      |
|    explained_variance   | 0.316      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.000356  |
|    n_updates            | 1520       |
|    policy_gradient_loss | -0.0128    |
|    value_loss           | 0.00361    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 313000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 153    |
|    time_elapsed    | 32112  |
|    total_timesteps | 313344 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 314000      |
| train/                  |             |
|    approx_kl            | 0.017867642 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.56       |
|    explained_variance   | 0.373       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00967     |
|    n_updates            | 1530        |
|    policy_gradient_loss | -0.0207     |
|    value_loss           | 0.00322     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 315000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 154    |
|    time_elapsed    | 32318  |
|    total_timesteps | 315392 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 316000      |
| train/                  |             |
|    approx_kl            | 0.017230608 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.05       |
|    explained_variance   | 0.398       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00521    |
|    n_updates            | 1540        |
|    policy_gradient_loss | -0.0185     |
|    value_loss           | 0.00321     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 317000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 155    |
|    time_elapsed    | 32523  |
|    total_timesteps | 317440 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 318000      |
| train/                  |             |
|    approx_kl            | 0.017304705 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.31       |
|    explained_variance   | 0.398       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0137     |
|    n_updates            | 1550        |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.00375     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 319000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 156    |
|    time_elapsed    | 32728  |
|    total_timesteps | 319488 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 320000      |
| train/                  |             |
|    approx_kl            | 0.019189192 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.53       |
|    explained_variance   | 0.443       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00864    |
|    n_updates            | 1560        |
|    policy_gradient_loss | -0.0188     |
|    value_loss           | 0.00301     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 321000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 157    |
|    time_elapsed    | 32934  |
|    total_timesteps | 321536 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.6        |
| time/                   |            |
|    total_timesteps      | 322000     |
| train/                  |            |
|    approx_kl            | 0.02720773 |
|    clip_fraction        | 0.276      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.6       |
|    explained_variance   | 0.569      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0301    |
|    n_updates            | 1570       |
|    policy_gradient_loss | -0.0191    |
|    value_loss           | 0.0033     |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 323000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 158    |
|    time_elapsed    | 33139  |
|    total_timesteps | 323584 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.7        |
| time/                   |            |
|    total_timesteps      | 324000     |
| train/                  |            |
|    approx_kl            | 0.01615692 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.14      |
|    explained_variance   | 0.426      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0188     |
|    n_updates            | 1580       |
|    policy_gradient_loss | -0.0105    |
|    value_loss           | 0.00442    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 325000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 159    |
|    time_elapsed    | 33344  |
|    total_timesteps | 325632 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 326000      |
| train/                  |             |
|    approx_kl            | 0.016794965 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.1        |
|    explained_variance   | 0.493       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00804    |
|    n_updates            | 1590        |
|    policy_gradient_loss | -0.00645    |
|    value_loss           | 0.00439     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 327000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 160    |
|    time_elapsed    | 33550  |
|    total_timesteps | 327680 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.6        |
| time/                   |            |
|    total_timesteps      | 328000     |
| train/                  |            |
|    approx_kl            | 0.01931984 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.32      |
|    explained_variance   | 0.398      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0109    |
|    n_updates            | 1600       |
|    policy_gradient_loss | -0.0135    |
|    value_loss           | 0.00468    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 329000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 161    |
|    time_elapsed    | 33755  |
|    total_timesteps | 329728 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.6        |
| time/                   |            |
|    total_timesteps      | 330000     |
| train/                  |            |
|    approx_kl            | 0.02027268 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.33      |
|    explained_variance   | 0.656      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0291    |
|    n_updates            | 1610       |
|    policy_gradient_loss | -0.0178    |
|    value_loss           | 0.00335    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 331000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 162    |
|    time_elapsed    | 33960  |
|    total_timesteps | 331776 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 332000      |
| train/                  |             |
|    approx_kl            | 0.016151924 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.32       |
|    explained_variance   | 0.487       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0227     |
|    n_updates            | 1620        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.00318     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 333000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 163    |
|    time_elapsed    | 34166  |
|    total_timesteps | 333824 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 334000      |
| train/                  |             |
|    approx_kl            | 0.018958628 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.21       |
|    explained_variance   | 0.656       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0172     |
|    n_updates            | 1630        |
|    policy_gradient_loss | -0.0187     |
|    value_loss           | 0.00314     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 335000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 164    |
|    time_elapsed    | 34371  |
|    total_timesteps | 335872 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.6        |
| time/                   |            |
|    total_timesteps      | 336000     |
| train/                  |            |
|    approx_kl            | 0.02002798 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.12      |
|    explained_variance   | 0.433      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0119    |
|    n_updates            | 1640       |
|    policy_gradient_loss | -0.0134    |
|    value_loss           | 0.00459    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 337000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 165    |
|    time_elapsed    | 34576  |
|    total_timesteps | 337920 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 338000      |
| train/                  |             |
|    approx_kl            | 0.016879676 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.23       |
|    explained_variance   | 0.512       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0234     |
|    n_updates            | 1650        |
|    policy_gradient_loss | -0.0141     |
|    value_loss           | 0.00391     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 339000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 166    |
|    time_elapsed    | 34782  |
|    total_timesteps | 339968 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 340000      |
| train/                  |             |
|    approx_kl            | 0.013919791 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.09       |
|    explained_variance   | 0.486       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0378     |
|    n_updates            | 1660        |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.00445     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 341000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 342000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 167    |
|    time_elapsed    | 35087  |
|    total_timesteps | 342016 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 343000      |
| train/                  |             |
|    approx_kl            | 0.017569505 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.37       |
|    explained_variance   | 0.564       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00606     |
|    n_updates            | 1670        |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 0.00471     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 344000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 168    |
|    time_elapsed    | 35292  |
|    total_timesteps | 344064 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.7        |
| time/                   |            |
|    total_timesteps      | 345000     |
| train/                  |            |
|    approx_kl            | 0.01759576 |
|    clip_fraction        | 0.264      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.37      |
|    explained_variance   | 0.663      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0229    |
|    n_updates            | 1680       |
|    policy_gradient_loss | -0.0218    |
|    value_loss           | 0.00364    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 346000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 169    |
|    time_elapsed    | 35497  |
|    total_timesteps | 346112 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 347000      |
| train/                  |             |
|    approx_kl            | 0.104456395 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.29       |
|    explained_variance   | 0.726       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0465     |
|    n_updates            | 1690        |
|    policy_gradient_loss | -0.0265     |
|    value_loss           | 0.00263     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 348000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 170    |
|    time_elapsed    | 35703  |
|    total_timesteps | 348160 |
-------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+04     |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 349000    |
| train/                  |           |
|    approx_kl            | 0.0687572 |
|    clip_fraction        | 0.282     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.2      |
|    explained_variance   | 0.877     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0653   |
|    n_updates            | 1700      |
|    policy_gradient_loss | -0.0309   |
|    value_loss           | 0.00259   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 350000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 171    |
|    time_elapsed    | 35908  |
|    total_timesteps | 350208 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 351000      |
| train/                  |             |
|    approx_kl            | 0.021167185 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.09       |
|    explained_variance   | 0.847       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0286     |
|    n_updates            | 1710        |
|    policy_gradient_loss | -0.0197     |
|    value_loss           | 0.00321     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 352000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 172    |
|    time_elapsed    | 36113  |
|    total_timesteps | 352256 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.7        |
| time/                   |            |
|    total_timesteps      | 353000     |
| train/                  |            |
|    approx_kl            | 0.04779007 |
|    clip_fraction        | 0.316      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.11      |
|    explained_variance   | 0.789      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0862    |
|    n_updates            | 1720       |
|    policy_gradient_loss | -0.0291    |
|    value_loss           | 0.00408    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 354000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 173    |
|    time_elapsed    | 36319  |
|    total_timesteps | 354304 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 355000      |
| train/                  |             |
|    approx_kl            | 0.016928619 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.23       |
|    explained_variance   | 0.686       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0385     |
|    n_updates            | 1730        |
|    policy_gradient_loss | -0.0145     |
|    value_loss           | 0.00379     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 356000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 174    |
|    time_elapsed    | 36524  |
|    total_timesteps | 356352 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.6        |
| time/                   |            |
|    total_timesteps      | 357000     |
| train/                  |            |
|    approx_kl            | 0.02414921 |
|    clip_fraction        | 0.258      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.78      |
|    explained_variance   | 0.623      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.000197   |
|    n_updates            | 1740       |
|    policy_gradient_loss | -0.0193    |
|    value_loss           | 0.0047     |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 358000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 175    |
|    time_elapsed    | 36729  |
|    total_timesteps | 358400 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.6        |
| time/                   |            |
|    total_timesteps      | 359000     |
| train/                  |            |
|    approx_kl            | 0.02196719 |
|    clip_fraction        | 0.255      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.34      |
|    explained_variance   | 0.516      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0209    |
|    n_updates            | 1750       |
|    policy_gradient_loss | -0.019     |
|    value_loss           | 0.00436    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 360000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 176    |
|    time_elapsed    | 36935  |
|    total_timesteps | 360448 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.8        |
| time/                   |            |
|    total_timesteps      | 361000     |
| train/                  |            |
|    approx_kl            | 0.02011748 |
|    clip_fraction        | 0.242      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.91      |
|    explained_variance   | 0.767      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0455    |
|    n_updates            | 1760       |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 0.00391    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 362000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 177    |
|    time_elapsed    | 37140  |
|    total_timesteps | 362496 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 363000      |
| train/                  |             |
|    approx_kl            | 0.023852184 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.3        |
|    explained_variance   | 0.72        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0176     |
|    n_updates            | 1770        |
|    policy_gradient_loss | -0.0196     |
|    value_loss           | 0.0044      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 364000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 178    |
|    time_elapsed    | 37345  |
|    total_timesteps | 364544 |
-------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+04     |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 365000    |
| train/                  |           |
|    approx_kl            | 0.0225487 |
|    clip_fraction        | 0.261     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.82     |
|    explained_variance   | 0.629     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.00141   |
|    n_updates            | 1780      |
|    policy_gradient_loss | -0.0179   |
|    value_loss           | 0.00525   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 366000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 179    |
|    time_elapsed    | 37551  |
|    total_timesteps | 366592 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 367000      |
| train/                  |             |
|    approx_kl            | 0.025987804 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.84       |
|    explained_variance   | 0.669       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00999    |
|    n_updates            | 1790        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.00446     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 368000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 180    |
|    time_elapsed    | 37756  |
|    total_timesteps | 368640 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 369000      |
| train/                  |             |
|    approx_kl            | 0.016196653 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.533       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0354     |
|    n_updates            | 1800        |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 0.00522     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 370000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 181    |
|    time_elapsed    | 37961  |
|    total_timesteps | 370688 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 371000      |
| train/                  |             |
|    approx_kl            | 0.026791291 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.88       |
|    explained_variance   | 0.415       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0516     |
|    n_updates            | 1810        |
|    policy_gradient_loss | -0.0201     |
|    value_loss           | 0.00467     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 372000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 182    |
|    time_elapsed    | 38167  |
|    total_timesteps | 372736 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 373000      |
| train/                  |             |
|    approx_kl            | 0.017764822 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.01       |
|    explained_variance   | 0.545       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00463     |
|    n_updates            | 1820        |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.0048      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 374000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 183    |
|    time_elapsed    | 38372  |
|    total_timesteps | 374784 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 375000      |
| train/                  |             |
|    approx_kl            | 0.019273387 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.05       |
|    explained_variance   | 0.6         |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0281     |
|    n_updates            | 1830        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.00443     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 376000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 184    |
|    time_elapsed    | 38577  |
|    total_timesteps | 376832 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 377000      |
| train/                  |             |
|    approx_kl            | 0.020948146 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.31       |
|    explained_variance   | 0.628       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00648     |
|    n_updates            | 1840        |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 0.00658     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 378000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 185    |
|    time_elapsed    | 38783  |
|    total_timesteps | 378880 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 379000      |
| train/                  |             |
|    approx_kl            | 0.018748296 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.08       |
|    explained_variance   | 0.677       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00221     |
|    n_updates            | 1850        |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 0.00461     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 380000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 186    |
|    time_elapsed    | 38988  |
|    total_timesteps | 380928 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 381000      |
| train/                  |             |
|    approx_kl            | 0.015350586 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.25       |
|    explained_variance   | 0.639       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0323     |
|    n_updates            | 1860        |
|    policy_gradient_loss | -0.0128     |
|    value_loss           | 0.00494     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 382000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 187    |
|    time_elapsed    | 39193  |
|    total_timesteps | 382976 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 383000      |
| train/                  |             |
|    approx_kl            | 0.019660974 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.17       |
|    explained_variance   | 0.373       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00698    |
|    n_updates            | 1870        |
|    policy_gradient_loss | -0.0142     |
|    value_loss           | 0.00529     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 384000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 385000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 188    |
|    time_elapsed    | 39499  |
|    total_timesteps | 385024 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 386000      |
| train/                  |             |
|    approx_kl            | 0.014247453 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.04       |
|    explained_variance   | 0.452       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0281     |
|    n_updates            | 1880        |
|    policy_gradient_loss | -0.00995    |
|    value_loss           | 0.0082      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 387000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 189    |
|    time_elapsed    | 39704  |
|    total_timesteps | 387072 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 388000      |
| train/                  |             |
|    approx_kl            | 0.017088886 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.01       |
|    explained_variance   | 0.548       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0214      |
|    n_updates            | 1890        |
|    policy_gradient_loss | -0.0143     |
|    value_loss           | 0.00466     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 389000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 190    |
|    time_elapsed    | 39909  |
|    total_timesteps | 389120 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 390000      |
| train/                  |             |
|    approx_kl            | 0.018511478 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.9        |
|    explained_variance   | 0.414       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0313     |
|    n_updates            | 1900        |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 0.00646     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 391000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 191    |
|    time_elapsed    | 40115  |
|    total_timesteps | 391168 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 392000      |
| train/                  |             |
|    approx_kl            | 0.019478615 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.59       |
|    explained_variance   | 0.476       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0101     |
|    n_updates            | 1910        |
|    policy_gradient_loss | -0.0134     |
|    value_loss           | 0.00578     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 393000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 192    |
|    time_elapsed    | 40320  |
|    total_timesteps | 393216 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 394000      |
| train/                  |             |
|    approx_kl            | 0.018890651 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.59       |
|    explained_variance   | 0.387       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0185      |
|    n_updates            | 1920        |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 0.00714     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 395000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 193    |
|    time_elapsed    | 40525  |
|    total_timesteps | 395264 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 396000      |
| train/                  |             |
|    approx_kl            | 0.017935611 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.408       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00255    |
|    n_updates            | 1930        |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 0.00655     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 397000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 194    |
|    time_elapsed    | 40731  |
|    total_timesteps | 397312 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 398000      |
| train/                  |             |
|    approx_kl            | 0.020042816 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.535       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0277     |
|    n_updates            | 1940        |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 0.0071      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 399000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 195    |
|    time_elapsed    | 40936  |
|    total_timesteps | 399360 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 400000      |
| train/                  |             |
|    approx_kl            | 0.018018557 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.496       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0334     |
|    n_updates            | 1950        |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 0.0079      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 401000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 196    |
|    time_elapsed    | 41141  |
|    total_timesteps | 401408 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 402000      |
| train/                  |             |
|    approx_kl            | 0.020096174 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.04       |
|    explained_variance   | 0.474       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00125     |
|    n_updates            | 1960        |
|    policy_gradient_loss | -0.0086     |
|    value_loss           | 0.00838     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 403000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 197    |
|    time_elapsed    | 41347  |
|    total_timesteps | 403456 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 404000      |
| train/                  |             |
|    approx_kl            | 0.018597981 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.73       |
|    explained_variance   | 0.459       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00805    |
|    n_updates            | 1970        |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 0.00876     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 405000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 198    |
|    time_elapsed    | 41552  |
|    total_timesteps | 405504 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 406000      |
| train/                  |             |
|    approx_kl            | 0.014382165 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.54       |
|    explained_variance   | 0.457       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00265     |
|    n_updates            | 1980        |
|    policy_gradient_loss | -0.00652    |
|    value_loss           | 0.00893     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 407000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 199    |
|    time_elapsed    | 41757  |
|    total_timesteps | 407552 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 408000      |
| train/                  |             |
|    approx_kl            | 0.025213704 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.3        |
|    explained_variance   | 0.485       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0323     |
|    n_updates            | 1990        |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 0.00923     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 409000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 200    |
|    time_elapsed    | 41963  |
|    total_timesteps | 409600 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 1          |
| time/                   |            |
|    total_timesteps      | 410000     |
| train/                  |            |
|    approx_kl            | 0.05026427 |
|    clip_fraction        | 0.293      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.31      |
|    explained_variance   | 0.4        |
|    learning_rate        | 0.0003     |
|    loss                 | -0.021     |
|    n_updates            | 2000       |
|    policy_gradient_loss | -0.0135    |
|    value_loss           | 0.00972    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 411000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 201    |
|    time_elapsed    | 42168  |
|    total_timesteps | 411648 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 412000      |
| train/                  |             |
|    approx_kl            | 0.016877955 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.39       |
|    explained_variance   | 0.575       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00143     |
|    n_updates            | 2010        |
|    policy_gradient_loss | -0.00838    |
|    value_loss           | 0.00841     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 413000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 202    |
|    time_elapsed    | 42373  |
|    total_timesteps | 413696 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 414000      |
| train/                  |             |
|    approx_kl            | 0.029274404 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.58       |
|    explained_variance   | 0.482       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0303      |
|    n_updates            | 2020        |
|    policy_gradient_loss | -0.00375    |
|    value_loss           | 0.0103      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 415000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 203    |
|    time_elapsed    | 42579  |
|    total_timesteps | 415744 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 416000      |
| train/                  |             |
|    approx_kl            | 0.032265738 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.32       |
|    explained_variance   | 0.458       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.016      |
|    n_updates            | 2030        |
|    policy_gradient_loss | -0.00928    |
|    value_loss           | 0.0102      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 417000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 204    |
|    time_elapsed    | 42784  |
|    total_timesteps | 417792 |
-------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+04     |
|    mean_reward          | 1         |
| time/                   |           |
|    total_timesteps      | 418000    |
| train/                  |           |
|    approx_kl            | 0.0473323 |
|    clip_fraction        | 0.261     |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.32     |
|    explained_variance   | 0.44      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0182   |
|    n_updates            | 2040      |
|    policy_gradient_loss | -0.00848  |
|    value_loss           | 0.00969   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 419000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 205    |
|    time_elapsed    | 42989  |
|    total_timesteps | 419840 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 420000      |
| train/                  |             |
|    approx_kl            | 0.014760637 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.61       |
|    explained_variance   | 0.517       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0308     |
|    n_updates            | 2050        |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 0.0088      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 421000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 206    |
|    time_elapsed    | 43195  |
|    total_timesteps | 421888 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1            |
| time/                   |              |
|    total_timesteps      | 422000       |
| train/                  |              |
|    approx_kl            | 0.0073511065 |
|    clip_fraction        | 0.106        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.49        |
|    explained_variance   | 0.492        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000125    |
|    n_updates            | 2060         |
|    policy_gradient_loss | -0.00468     |
|    value_loss           | 0.00923      |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 423000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 207    |
|    time_elapsed    | 43400  |
|    total_timesteps | 423936 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 424000      |
| train/                  |             |
|    approx_kl            | 0.009110443 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.399       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.028       |
|    n_updates            | 2070        |
|    policy_gradient_loss | -0.00445    |
|    value_loss           | 0.0105      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 425000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 208    |
|    time_elapsed    | 43605  |
|    total_timesteps | 425984 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 426000      |
| train/                  |             |
|    approx_kl            | 0.008573779 |
|    clip_fraction        | 0.0992      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.454       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00603    |
|    n_updates            | 2080        |
|    policy_gradient_loss | -0.0062     |
|    value_loss           | 0.0101      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 427000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 428000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 209    |
|    time_elapsed    | 43911  |
|    total_timesteps | 428032 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 429000      |
| train/                  |             |
|    approx_kl            | 0.008999385 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.448       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00633    |
|    n_updates            | 2090        |
|    policy_gradient_loss | -0.00207    |
|    value_loss           | 0.0148      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 430000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 210    |
|    time_elapsed    | 44116  |
|    total_timesteps | 430080 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 431000      |
| train/                  |             |
|    approx_kl            | 0.009086586 |
|    clip_fraction        | 0.0962      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.474       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00953    |
|    n_updates            | 2100        |
|    policy_gradient_loss | -0.00376    |
|    value_loss           | 0.0105      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 432000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 211    |
|    time_elapsed    | 44321  |
|    total_timesteps | 432128 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 433000      |
| train/                  |             |
|    approx_kl            | 0.006789485 |
|    clip_fraction        | 0.0867      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.794      |
|    explained_variance   | 0.399       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00299    |
|    n_updates            | 2110        |
|    policy_gradient_loss | -0.00425    |
|    value_loss           | 0.011       |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 434000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 212    |
|    time_elapsed    | 44527  |
|    total_timesteps | 434176 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.1          |
| time/                   |              |
|    total_timesteps      | 435000       |
| train/                  |              |
|    approx_kl            | 0.0047601797 |
|    clip_fraction        | 0.0583       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.753       |
|    explained_variance   | 0.361        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00839     |
|    n_updates            | 2120         |
|    policy_gradient_loss | -0.00371     |
|    value_loss           | 0.0113       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 436000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 213    |
|    time_elapsed    | 44732  |
|    total_timesteps | 436224 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.1          |
| time/                   |              |
|    total_timesteps      | 437000       |
| train/                  |              |
|    approx_kl            | 0.0046144202 |
|    clip_fraction        | 0.0593       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.562       |
|    explained_variance   | 0.458        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00292      |
|    n_updates            | 2130         |
|    policy_gradient_loss | -0.00367     |
|    value_loss           | 0.011        |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 438000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 214    |
|    time_elapsed    | 44937  |
|    total_timesteps | 438272 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.1          |
| time/                   |              |
|    total_timesteps      | 439000       |
| train/                  |              |
|    approx_kl            | 0.0033274188 |
|    clip_fraction        | 0.0477       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.54        |
|    explained_variance   | 0.392        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00382     |
|    n_updates            | 2140         |
|    policy_gradient_loss | -0.00193     |
|    value_loss           | 0.0115       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 440000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 215    |
|    time_elapsed    | 45143  |
|    total_timesteps | 440320 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.2          |
| time/                   |              |
|    total_timesteps      | 441000       |
| train/                  |              |
|    approx_kl            | 0.0029099907 |
|    clip_fraction        | 0.0425       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.451       |
|    explained_variance   | 0.38         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0076       |
|    n_updates            | 2150         |
|    policy_gradient_loss | -0.00294     |
|    value_loss           | 0.0119       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.2      |
| time/              |          |
|    total_timesteps | 442000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 216    |
|    time_elapsed    | 45348  |
|    total_timesteps | 442368 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 443000      |
| train/                  |             |
|    approx_kl            | 0.002060909 |
|    clip_fraction        | 0.0386      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.319      |
|    explained_variance   | 0.364       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00811     |
|    n_updates            | 2160        |
|    policy_gradient_loss | -0.00273    |
|    value_loss           | 0.0125      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 444000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 217    |
|    time_elapsed    | 45553  |
|    total_timesteps | 444416 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.1          |
| time/                   |              |
|    total_timesteps      | 445000       |
| train/                  |              |
|    approx_kl            | 0.0013854704 |
|    clip_fraction        | 0.0259       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.312       |
|    explained_variance   | 0.382        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0073       |
|    n_updates            | 2170         |
|    policy_gradient_loss | -0.000846    |
|    value_loss           | 0.0128       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 446000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 218    |
|    time_elapsed    | 45759  |
|    total_timesteps | 446464 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.1          |
| time/                   |              |
|    total_timesteps      | 447000       |
| train/                  |              |
|    approx_kl            | 0.0020775972 |
|    clip_fraction        | 0.0316       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.264       |
|    explained_variance   | 0.355        |
|    learning_rate        | 0.0003       |
|    loss                 | -5.59e-05    |
|    n_updates            | 2180         |
|    policy_gradient_loss | -0.00151     |
|    value_loss           | 0.013        |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 448000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 219    |
|    time_elapsed    | 45964  |
|    total_timesteps | 448512 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.1          |
| time/                   |              |
|    total_timesteps      | 449000       |
| train/                  |              |
|    approx_kl            | 0.0018026081 |
|    clip_fraction        | 0.0217       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.234       |
|    explained_variance   | 0.346        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0013       |
|    n_updates            | 2190         |
|    policy_gradient_loss | -0.000978    |
|    value_loss           | 0.013        |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 450000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 220    |
|    time_elapsed    | 46169  |
|    total_timesteps | 450560 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.4          |
| time/                   |              |
|    total_timesteps      | 451000       |
| train/                  |              |
|    approx_kl            | 0.0023142437 |
|    clip_fraction        | 0.0206       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.274       |
|    explained_variance   | 0.351        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00362     |
|    n_updates            | 2200         |
|    policy_gradient_loss | -0.00135     |
|    value_loss           | 0.0128       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 452000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 221    |
|    time_elapsed    | 46375  |
|    total_timesteps | 452608 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.4          |
| time/                   |              |
|    total_timesteps      | 453000       |
| train/                  |              |
|    approx_kl            | 0.0011146017 |
|    clip_fraction        | 0.0228       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.215       |
|    explained_variance   | 0.395        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00555      |
|    n_updates            | 2210         |
|    policy_gradient_loss | -0.001       |
|    value_loss           | 0.0125       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 454000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 222    |
|    time_elapsed    | 46580  |
|    total_timesteps | 454656 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.4          |
| time/                   |              |
|    total_timesteps      | 455000       |
| train/                  |              |
|    approx_kl            | 0.0017618468 |
|    clip_fraction        | 0.0229       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.176       |
|    explained_variance   | 0.341        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00329      |
|    n_updates            | 2220         |
|    policy_gradient_loss | -0.000815    |
|    value_loss           | 0.013        |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 456000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 223    |
|    time_elapsed    | 46785  |
|    total_timesteps | 456704 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.4          |
| time/                   |              |
|    total_timesteps      | 457000       |
| train/                  |              |
|    approx_kl            | 0.0011582633 |
|    clip_fraction        | 0.0154       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.172       |
|    explained_variance   | 0.371        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00108      |
|    n_updates            | 2230         |
|    policy_gradient_loss | -0.00115     |
|    value_loss           | 0.0132       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 458000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 224    |
|    time_elapsed    | 46991  |
|    total_timesteps | 458752 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 1.4           |
| time/                   |               |
|    total_timesteps      | 459000        |
| train/                  |               |
|    approx_kl            | 0.00083095464 |
|    clip_fraction        | 0.0168        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.143        |
|    explained_variance   | 0.344         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00561       |
|    n_updates            | 2240          |
|    policy_gradient_loss | -0.000896     |
|    value_loss           | 0.0134        |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 460000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 225    |
|    time_elapsed    | 47196  |
|    total_timesteps | 460800 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.4         |
| time/                   |             |
|    total_timesteps      | 461000      |
| train/                  |             |
|    approx_kl            | 0.000441575 |
|    clip_fraction        | 0.00981     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.148      |
|    explained_variance   | 0.346       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00303     |
|    n_updates            | 2250        |
|    policy_gradient_loss | -0.00077    |
|    value_loss           | 0.0135      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 462000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 226    |
|    time_elapsed    | 47401  |
|    total_timesteps | 462848 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.4          |
| time/                   |              |
|    total_timesteps      | 463000       |
| train/                  |              |
|    approx_kl            | 0.0009665906 |
|    clip_fraction        | 0.0125       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.12        |
|    explained_variance   | 0.338        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0191       |
|    n_updates            | 2260         |
|    policy_gradient_loss | -0.000515    |
|    value_loss           | 0.0137       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 464000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 227    |
|    time_elapsed    | 47607  |
|    total_timesteps | 464896 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1            |
| time/                   |              |
|    total_timesteps      | 465000       |
| train/                  |              |
|    approx_kl            | 0.0046804473 |
|    clip_fraction        | 0.0196       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.108       |
|    explained_variance   | 0.328        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00771      |
|    n_updates            | 2270         |
|    policy_gradient_loss | -0.00135     |
|    value_loss           | 0.0133       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 466000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 228    |
|    time_elapsed    | 47812  |
|    total_timesteps | 466944 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.4          |
| time/                   |              |
|    total_timesteps      | 467000       |
| train/                  |              |
|    approx_kl            | 0.0018652868 |
|    clip_fraction        | 0.0138       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.102       |
|    explained_variance   | 0.342        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0185       |
|    n_updates            | 2280         |
|    policy_gradient_loss | -0.000648    |
|    value_loss           | 0.0129       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 468000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 229    |
|    time_elapsed    | 48017  |
|    total_timesteps | 468992 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.4          |
| time/                   |              |
|    total_timesteps      | 469000       |
| train/                  |              |
|    approx_kl            | 0.0016053256 |
|    clip_fraction        | 0.00996      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0779      |
|    explained_variance   | 0.376        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00372      |
|    n_updates            | 2290         |
|    policy_gradient_loss | -0.000944    |
|    value_loss           | 0.013        |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 470000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 471000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 230    |
|    time_elapsed    | 48323  |
|    total_timesteps | 471040 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.4          |
| time/                   |              |
|    total_timesteps      | 472000       |
| train/                  |              |
|    approx_kl            | 0.0011350375 |
|    clip_fraction        | 0.012        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0863      |
|    explained_variance   | 0.4          |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00814     |
|    n_updates            | 2300         |
|    policy_gradient_loss | -0.000485    |
|    value_loss           | 0.0176       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 473000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 231    |
|    time_elapsed    | 48528  |
|    total_timesteps | 473088 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 1.4           |
| time/                   |               |
|    total_timesteps      | 474000        |
| train/                  |               |
|    approx_kl            | 0.00046309372 |
|    clip_fraction        | 0.00322       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0562       |
|    explained_variance   | 0.352         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000353      |
|    n_updates            | 2310          |
|    policy_gradient_loss | -0.000336     |
|    value_loss           | 0.0135        |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 475000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 232    |
|    time_elapsed    | 48733  |
|    total_timesteps | 475136 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 1.4           |
| time/                   |               |
|    total_timesteps      | 476000        |
| train/                  |               |
|    approx_kl            | 0.00014394661 |
|    clip_fraction        | 0.00322       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.05         |
|    explained_variance   | 0.329         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00639       |
|    n_updates            | 2320          |
|    policy_gradient_loss | -0.000125     |
|    value_loss           | 0.0137        |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 477000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 233    |
|    time_elapsed    | 48939  |
|    total_timesteps | 477184 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.4          |
| time/                   |              |
|    total_timesteps      | 478000       |
| train/                  |              |
|    approx_kl            | 0.0031023014 |
|    clip_fraction        | 0.00566      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0463      |
|    explained_variance   | 0.343        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0041       |
|    n_updates            | 2330         |
|    policy_gradient_loss | -2.33e-05    |
|    value_loss           | 0.0136       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 479000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 234    |
|    time_elapsed    | 49144  |
|    total_timesteps | 479232 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 1.4           |
| time/                   |               |
|    total_timesteps      | 480000        |
| train/                  |               |
|    approx_kl            | 0.00051594136 |
|    clip_fraction        | 0.00454       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0403       |
|    explained_variance   | 0.346         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0246        |
|    n_updates            | 2340          |
|    policy_gradient_loss | -0.000262     |
|    value_loss           | 0.0138        |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 481000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 235    |
|    time_elapsed    | 49349  |
|    total_timesteps | 481280 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 1.4           |
| time/                   |               |
|    total_timesteps      | 482000        |
| train/                  |               |
|    approx_kl            | 0.00022372042 |
|    clip_fraction        | 0.00327       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0359       |
|    explained_variance   | 0.342         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000233     |
|    n_updates            | 2350          |
|    policy_gradient_loss | -0.000289     |
|    value_loss           | 0.0137        |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 483000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 236    |
|    time_elapsed    | 49555  |
|    total_timesteps | 483328 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 1.4           |
| time/                   |               |
|    total_timesteps      | 484000        |
| train/                  |               |
|    approx_kl            | 0.00031592997 |
|    clip_fraction        | 0.0043        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0316       |
|    explained_variance   | 0.363         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00156       |
|    n_updates            | 2360          |
|    policy_gradient_loss | -0.000145     |
|    value_loss           | 0.0135        |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 485000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 237    |
|    time_elapsed    | 49760  |
|    total_timesteps | 485376 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.4         |
| time/                   |             |
|    total_timesteps      | 486000      |
| train/                  |             |
|    approx_kl            | 0.001893979 |
|    clip_fraction        | 0.00337     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0274     |
|    explained_variance   | 0.351       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00847     |
|    n_updates            | 2370        |
|    policy_gradient_loss | -5.36e-05   |
|    value_loss           | 0.0135      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 487000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 238    |
|    time_elapsed    | 49965  |
|    total_timesteps | 487424 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.4          |
| time/                   |              |
|    total_timesteps      | 488000       |
| train/                  |              |
|    approx_kl            | 9.890311e-05 |
|    clip_fraction        | 0.00127      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0227      |
|    explained_variance   | 0.328        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00108      |
|    n_updates            | 2380         |
|    policy_gradient_loss | -7.88e-05    |
|    value_loss           | 0.0137       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 489000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 239    |
|    time_elapsed    | 50170  |
|    total_timesteps | 489472 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.4          |
| time/                   |              |
|    total_timesteps      | 490000       |
| train/                  |              |
|    approx_kl            | 0.0003196062 |
|    clip_fraction        | 0.00259      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0201      |
|    explained_variance   | 0.328        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0014       |
|    n_updates            | 2390         |
|    policy_gradient_loss | -0.000117    |
|    value_loss           | 0.0139       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 491000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 240    |
|    time_elapsed    | 50376  |
|    total_timesteps | 491520 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.4         |
| time/                   |             |
|    total_timesteps      | 492000      |
| train/                  |             |
|    approx_kl            | 0.000426854 |
|    clip_fraction        | 0.0021      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0193     |
|    explained_variance   | 0.328       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000491    |
|    n_updates            | 2400        |
|    policy_gradient_loss | -7.06e-05   |
|    value_loss           | 0.0139      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 493000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 241    |
|    time_elapsed    | 50581  |
|    total_timesteps | 493568 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.4          |
| time/                   |              |
|    total_timesteps      | 494000       |
| train/                  |              |
|    approx_kl            | 0.0003865385 |
|    clip_fraction        | 0.00225      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0176      |
|    explained_variance   | 0.329        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00763      |
|    n_updates            | 2410         |
|    policy_gradient_loss | -4.32e-05    |
|    value_loss           | 0.0138       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 495000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 242    |
|    time_elapsed    | 50786  |
|    total_timesteps | 495616 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 1.4           |
| time/                   |               |
|    total_timesteps      | 496000        |
| train/                  |               |
|    approx_kl            | 0.00012655676 |
|    clip_fraction        | 0.0021        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.016        |
|    explained_variance   | 0.331         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000957      |
|    n_updates            | 2420          |
|    policy_gradient_loss | 5.87e-06      |
|    value_loss           | 0.0138        |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 497000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 243    |
|    time_elapsed    | 50992  |
|    total_timesteps | 497664 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.4          |
| time/                   |              |
|    total_timesteps      | 498000       |
| train/                  |              |
|    approx_kl            | 0.0026392865 |
|    clip_fraction        | 0.00225      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.015       |
|    explained_variance   | 0.344        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0123       |
|    n_updates            | 2430         |
|    policy_gradient_loss | 6.82e-05     |
|    value_loss           | 0.0138       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 499000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 244    |
|    time_elapsed    | 51197  |
|    total_timesteps | 499712 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 1.4           |
| time/                   |               |
|    total_timesteps      | 500000        |
| train/                  |               |
|    approx_kl            | 0.00028351488 |
|    clip_fraction        | 0.00132       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0133       |
|    explained_variance   | 0.331         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.001         |
|    n_updates            | 2440          |
|    policy_gradient_loss | -8.42e-05     |
|    value_loss           | 0.0137        |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 501000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 245    |
|    time_elapsed    | 51402  |
|    total_timesteps | 501760 |
-------------------------------
