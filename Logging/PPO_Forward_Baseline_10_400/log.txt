Logging to ./Logging/PPO_Forward_Baseline_10_400
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.65     |
| time/              |          |
|    fps             | 84       |
|    iterations      | 1        |
|    time_elapsed    | 24       |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 3000        |
| train/                  |             |
|    approx_kl            | 0.017780222 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.54       |
|    explained_variance   | 0.847       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0479     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0273     |
|    value_loss           | 0.012       |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 980      |
|    ep_rew_mean     | 0.7      |
| time/              |          |
|    fps             | 82       |
|    iterations      | 2        |
|    time_elapsed    | 49       |
|    total_timesteps | 4096     |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.013875651 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.52       |
|    explained_variance   | 0.866       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0443     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0268     |
|    value_loss           | 0.00599     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.75     |
| time/              |          |
|    fps             | 82       |
|    iterations      | 3        |
|    time_elapsed    | 74       |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 7000        |
| train/                  |             |
|    approx_kl            | 0.014758365 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.5        |
|    explained_variance   | 0.868       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.052      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0227     |
|    value_loss           | 0.00378     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.688    |
| time/              |          |
|    fps             | 81       |
|    iterations      | 4        |
|    time_elapsed    | 100      |
|    total_timesteps | 8192     |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 9000        |
| train/                  |             |
|    approx_kl            | 0.014202252 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.51       |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0334     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0232     |
|    value_loss           | 0.00524     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.66     |
| time/              |          |
|    fps             | 81       |
|    iterations      | 5        |
|    time_elapsed    | 125      |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 11000       |
| train/                  |             |
|    approx_kl            | 0.019679952 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.49       |
|    explained_variance   | 0.899       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0515     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0252     |
|    value_loss           | 0.00268     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.65     |
| time/              |          |
|    fps             | 81       |
|    iterations      | 6        |
|    time_elapsed    | 150      |
|    total_timesteps | 12288    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 13000       |
| train/                  |             |
|    approx_kl            | 0.020295447 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.48       |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0764     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.028      |
|    value_loss           | 0.00291     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.671    |
| time/              |          |
|    fps             | 81       |
|    iterations      | 7        |
|    time_elapsed    | 176      |
|    total_timesteps | 14336    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.01562713 |
|    clip_fraction        | 0.184      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.46      |
|    explained_variance   | 0.828      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0239    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0302    |
|    value_loss           | 0.00232    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.662    |
| time/              |          |
|    fps             | 81       |
|    iterations      | 8        |
|    time_elapsed    | 201      |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 17000       |
| train/                  |             |
|    approx_kl            | 0.017148996 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.45       |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0424     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0265     |
|    value_loss           | 0.00167     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.678    |
| time/              |          |
|    fps             | 81       |
|    iterations      | 9        |
|    time_elapsed    | 226      |
|    total_timesteps | 18432    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 19000       |
| train/                  |             |
|    approx_kl            | 0.013310635 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.44       |
|    explained_variance   | 0.841       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0392     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0225     |
|    value_loss           | 0.00227     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.67     |
| time/              |          |
|    fps             | 81       |
|    iterations      | 10       |
|    time_elapsed    | 252      |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 21000       |
| train/                  |             |
|    approx_kl            | 0.013705027 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.45       |
|    explained_variance   | 0.734       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0148      |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0212     |
|    value_loss           | 0.00107     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 988      |
|    ep_rew_mean     | 0.682    |
| time/              |          |
|    fps             | 81       |
|    iterations      | 11       |
|    time_elapsed    | 277      |
|    total_timesteps | 22528    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 23000       |
| train/                  |             |
|    approx_kl            | 0.017700851 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.45       |
|    explained_variance   | 0.882       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00426    |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0284     |
|    value_loss           | 0.00312     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.704    |
| time/              |          |
|    fps             | 81       |
|    iterations      | 12       |
|    time_elapsed    | 302      |
|    total_timesteps | 24576    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.014104016 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.4        |
|    explained_variance   | 0.804       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0136     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0201     |
|    value_loss           | 0.00145     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.704    |
| time/              |          |
|    fps             | 81       |
|    iterations      | 13       |
|    time_elapsed    | 328      |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 27000       |
| train/                  |             |
|    approx_kl            | 0.017885882 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.42       |
|    explained_variance   | 0.657       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0301     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0276     |
|    value_loss           | 0.00115     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.696    |
| time/              |          |
|    fps             | 81       |
|    iterations      | 14       |
|    time_elapsed    | 353      |
|    total_timesteps | 28672    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 29000       |
| train/                  |             |
|    approx_kl            | 0.016454935 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.37       |
|    explained_variance   | 0.81        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0573     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0218     |
|    value_loss           | 0.000945    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.693    |
| time/              |          |
|    fps             | 81       |
|    iterations      | 15       |
|    time_elapsed    | 378      |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 31000       |
| train/                  |             |
|    approx_kl            | 0.014732955 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.34       |
|    explained_variance   | 0.692       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0356     |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0225     |
|    value_loss           | 0.00163     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.7      |
| time/              |          |
|    fps             | 81       |
|    iterations      | 16       |
|    time_elapsed    | 404      |
|    total_timesteps | 32768    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 33000       |
| train/                  |             |
|    approx_kl            | 0.014408044 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.36       |
|    explained_variance   | 0.841       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0514     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0237     |
|    value_loss           | 0.00101     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.7      |
| time/              |          |
|    fps             | 81       |
|    iterations      | 17       |
|    time_elapsed    | 429      |
|    total_timesteps | 34816    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.012970004 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.31       |
|    explained_variance   | 0.73        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0249     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0216     |
|    value_loss           | 0.0005      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.703    |
| time/              |          |
|    fps             | 81       |
|    iterations      | 18       |
|    time_elapsed    | 454      |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 37000       |
| train/                  |             |
|    approx_kl            | 0.020323329 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.37       |
|    explained_variance   | 0.838       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0337     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0285     |
|    value_loss           | 0.00489     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.697    |
| time/              |          |
|    fps             | 81       |
|    iterations      | 19       |
|    time_elapsed    | 479      |
|    total_timesteps | 38912    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 39000       |
| train/                  |             |
|    approx_kl            | 0.017701292 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.32       |
|    explained_variance   | 0.571       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0086     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.021      |
|    value_loss           | 0.00129     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.693    |
| time/              |          |
|    fps             | 81       |
|    iterations      | 20       |
|    time_elapsed    | 505      |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 41000       |
| train/                  |             |
|    approx_kl            | 0.016210385 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.3        |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.047      |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.027      |
|    value_loss           | 0.000555    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.691    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 21       |
|    time_elapsed    | 540      |
|    total_timesteps | 43008    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 44000       |
| train/                  |             |
|    approx_kl            | 0.014773893 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.32       |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0137     |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0238     |
|    value_loss           | 0.0015      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 995      |
|    ep_rew_mean     | 0.696    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 22       |
|    time_elapsed    | 565      |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 46000      |
| train/                  |            |
|    approx_kl            | 0.01732585 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.27      |
|    explained_variance   | 0.766      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.013     |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0287    |
|    value_loss           | 0.00172    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.7      |
| time/              |          |
|    fps             | 79       |
|    iterations      | 23       |
|    time_elapsed    | 591      |
|    total_timesteps | 47104    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 48000      |
| train/                  |            |
|    approx_kl            | 0.01414233 |
|    clip_fraction        | 0.16       |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.27      |
|    explained_variance   | 0.815      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0113    |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.0245    |
|    value_loss           | 0.000472   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.692    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 24       |
|    time_elapsed    | 616      |
|    total_timesteps | 49152    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.019743161 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.23       |
|    explained_variance   | 0.837       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0291     |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0261     |
|    value_loss           | 0.000458    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.692    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 25       |
|    time_elapsed    | 641      |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 52000       |
| train/                  |             |
|    approx_kl            | 0.015723854 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.26       |
|    explained_variance   | 0.904       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0307     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0248     |
|    value_loss           | 0.000834    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.691    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 26       |
|    time_elapsed    | 667      |
|    total_timesteps | 53248    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 54000       |
| train/                  |             |
|    approx_kl            | 0.013316343 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.2        |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0581     |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0238     |
|    value_loss           | 0.000399    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.693    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 27       |
|    time_elapsed    | 692      |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 56000       |
| train/                  |             |
|    approx_kl            | 0.019832142 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.22       |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0289     |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0296     |
|    value_loss           | 0.00051     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.689    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 28       |
|    time_elapsed    | 717      |
|    total_timesteps | 57344    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 58000       |
| train/                  |             |
|    approx_kl            | 0.020364758 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.23       |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0236     |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0264     |
|    value_loss           | 0.00039     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.69     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 29       |
|    time_elapsed    | 743      |
|    total_timesteps | 59392    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.01729507 |
|    clip_fraction        | 0.177      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.24      |
|    explained_variance   | 0.905      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0174    |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0274    |
|    value_loss           | 0.00188    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.69     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 30       |
|    time_elapsed    | 768      |
|    total_timesteps | 61440    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 62000       |
| train/                  |             |
|    approx_kl            | 0.014016289 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.21       |
|    explained_variance   | 0.85        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0131     |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.0255     |
|    value_loss           | 0.00135     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.69     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 31       |
|    time_elapsed    | 793      |
|    total_timesteps | 63488    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 64000       |
| train/                  |             |
|    approx_kl            | 0.015792847 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.21       |
|    explained_variance   | 0.896       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0125      |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.0203     |
|    value_loss           | 0.00057     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.691    |
| time/              |          |
|    fps             | 80       |
|    iterations      | 32       |
|    time_elapsed    | 819      |
|    total_timesteps | 65536    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 66000       |
| train/                  |             |
|    approx_kl            | 0.015475154 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.2        |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0285     |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0251     |
|    value_loss           | 0.00039     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.691    |
| time/              |          |
|    fps             | 80       |
|    iterations      | 33       |
|    time_elapsed    | 844      |
|    total_timesteps | 67584    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 68000       |
| train/                  |             |
|    approx_kl            | 0.018534143 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.23       |
|    explained_variance   | 0.898       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0341     |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.0239     |
|    value_loss           | 0.00126     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.691    |
| time/              |          |
|    fps             | 80       |
|    iterations      | 34       |
|    time_elapsed    | 869      |
|    total_timesteps | 69632    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 70000      |
| train/                  |            |
|    approx_kl            | 0.01599126 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.12      |
|    explained_variance   | 0.8        |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0323    |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.0234    |
|    value_loss           | 0.000676   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.694    |
| time/              |          |
|    fps             | 80       |
|    iterations      | 35       |
|    time_elapsed    | 894      |
|    total_timesteps | 71680    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 72000       |
| train/                  |             |
|    approx_kl            | 0.018591616 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.12       |
|    explained_variance   | 0.897       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0494     |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.0291     |
|    value_loss           | 0.00069     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.696    |
| time/              |          |
|    fps             | 80       |
|    iterations      | 36       |
|    time_elapsed    | 920      |
|    total_timesteps | 73728    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 74000       |
| train/                  |             |
|    approx_kl            | 0.016268766 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.08       |
|    explained_variance   | 0.66        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00176    |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.0266     |
|    value_loss           | 0.00106     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 997      |
|    ep_rew_mean     | 0.699    |
| time/              |          |
|    fps             | 80       |
|    iterations      | 37       |
|    time_elapsed    | 945      |
|    total_timesteps | 75776    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 76000       |
| train/                  |             |
|    approx_kl            | 0.017467603 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6          |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0396     |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.0271     |
|    value_loss           | 0.000311    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.703    |
| time/              |          |
|    fps             | 80       |
|    iterations      | 38       |
|    time_elapsed    | 970      |
|    total_timesteps | 77824    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 78000       |
| train/                  |             |
|    approx_kl            | 0.015947483 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.16       |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0271     |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.0229     |
|    value_loss           | 0.000533    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.701    |
| time/              |          |
|    fps             | 80       |
|    iterations      | 39       |
|    time_elapsed    | 996      |
|    total_timesteps | 79872    |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | 0.1          |
| time/                   |              |
|    total_timesteps      | 80000        |
| train/                  |              |
|    approx_kl            | 0.0154820625 |
|    clip_fraction        | 0.171        |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.14        |
|    explained_variance   | 0.895        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0299      |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.0219      |
|    value_loss           | 0.000227     |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.701    |
| time/              |          |
|    fps             | 80       |
|    iterations      | 40       |
|    time_elapsed    | 1021     |
|    total_timesteps | 81920    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 82000       |
| train/                  |             |
|    approx_kl            | 0.022431785 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.94       |
|    explained_variance   | 0.87        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0467     |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.0261     |
|    value_loss           | 0.000361    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.701    |
| time/              |          |
|    fps             | 80       |
|    iterations      | 41       |
|    time_elapsed    | 1046     |
|    total_timesteps | 83968    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 84000      |
| train/                  |            |
|    approx_kl            | 0.01301104 |
|    clip_fraction        | 0.151      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.87      |
|    explained_variance   | 0.93       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0524    |
|    n_updates            | 410        |
|    policy_gradient_loss | -0.0191    |
|    value_loss           | 0.000185   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.705    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 42       |
|    time_elapsed    | 1082     |
|    total_timesteps | 86016    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 87000       |
| train/                  |             |
|    approx_kl            | 0.017261667 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.03       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0265     |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0244     |
|    value_loss           | 0.000641    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.703    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 43       |
|    time_elapsed    | 1107     |
|    total_timesteps | 88064    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 89000       |
| train/                  |             |
|    approx_kl            | 0.018458541 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.11       |
|    explained_variance   | 0.83        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0432     |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.0284     |
|    value_loss           | 0.000663    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 998      |
|    ep_rew_mean     | 0.706    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 44       |
|    time_elapsed    | 1132     |
|    total_timesteps | 90112    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 91000      |
| train/                  |            |
|    approx_kl            | 0.01316196 |
|    clip_fraction        | 0.144      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.93      |
|    explained_variance   | 0.888      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0166    |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.0224    |
|    value_loss           | 0.000371   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 998      |
|    ep_rew_mean     | 0.712    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 45       |
|    time_elapsed    | 1158     |
|    total_timesteps | 92160    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 93000       |
| train/                  |             |
|    approx_kl            | 0.022117084 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.05       |
|    explained_variance   | 0.861       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0297     |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.0274     |
|    value_loss           | 0.000605    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.718    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 46       |
|    time_elapsed    | 1183     |
|    total_timesteps | 94208    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 95000       |
| train/                  |             |
|    approx_kl            | 0.018522872 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.97       |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0581     |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.0278     |
|    value_loss           | 0.000653    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.717    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 47       |
|    time_elapsed    | 1208     |
|    total_timesteps | 96256    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 97000       |
| train/                  |             |
|    approx_kl            | 0.023524279 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.95       |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0563     |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.0292     |
|    value_loss           | 0.000276    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.717    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 48       |
|    time_elapsed    | 1234     |
|    total_timesteps | 98304    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 99000       |
| train/                  |             |
|    approx_kl            | 0.017193489 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.94       |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.014      |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.0297     |
|    value_loss           | 0.000246    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 997      |
|    ep_rew_mean     | 0.719    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 49       |
|    time_elapsed    | 1259     |
|    total_timesteps | 100352   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 101000      |
| train/                  |             |
|    approx_kl            | 0.018203855 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.93       |
|    explained_variance   | 0.903       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0208     |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.0251     |
|    value_loss           | 0.000829    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.729    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 50       |
|    time_elapsed    | 1284     |
|    total_timesteps | 102400   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 103000     |
| train/                  |            |
|    approx_kl            | 0.01840027 |
|    clip_fraction        | 0.205      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.94      |
|    explained_variance   | 0.939      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0454    |
|    n_updates            | 500        |
|    policy_gradient_loss | -0.0275    |
|    value_loss           | 0.000532   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.729    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 51       |
|    time_elapsed    | 1310     |
|    total_timesteps | 104448   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 105000     |
| train/                  |            |
|    approx_kl            | 0.01874035 |
|    clip_fraction        | 0.225      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.96      |
|    explained_variance   | 0.92       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0215    |
|    n_updates            | 510        |
|    policy_gradient_loss | -0.0269    |
|    value_loss           | 0.000483   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.727    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 52       |
|    time_elapsed    | 1335     |
|    total_timesteps | 106496   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 107000      |
| train/                  |             |
|    approx_kl            | 0.012305519 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.89       |
|    explained_variance   | 0.886       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0393     |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.0192     |
|    value_loss           | 0.000367    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.73     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 53       |
|    time_elapsed    | 1360     |
|    total_timesteps | 108544   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 109000      |
| train/                  |             |
|    approx_kl            | 0.024735626 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.97       |
|    explained_variance   | 0.634       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0185     |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.0257     |
|    value_loss           | 0.000639    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.733    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 54       |
|    time_elapsed    | 1385     |
|    total_timesteps | 110592   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 111000      |
| train/                  |             |
|    approx_kl            | 0.020177517 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.92       |
|    explained_variance   | 0.904       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0727     |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.0259     |
|    value_loss           | 0.00019     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.736    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 55       |
|    time_elapsed    | 1411     |
|    total_timesteps | 112640   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 113000      |
| train/                  |             |
|    approx_kl            | 0.022832604 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.81       |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0758     |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.0264     |
|    value_loss           | 0.000207    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.734    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 56       |
|    time_elapsed    | 1436     |
|    total_timesteps | 114688   |
---------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+03     |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 115000    |
| train/                  |           |
|    approx_kl            | 0.0181417 |
|    clip_fraction        | 0.21      |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.76     |
|    explained_variance   | 0.913     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0319   |
|    n_updates            | 560       |
|    policy_gradient_loss | -0.0244   |
|    value_loss           | 0.0004    |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.737    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 57       |
|    time_elapsed    | 1461     |
|    total_timesteps | 116736   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 117000      |
| train/                  |             |
|    approx_kl            | 0.017787566 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.71       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0337     |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.0256     |
|    value_loss           | 0.000182    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 999      |
|    ep_rew_mean     | 0.737    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 58       |
|    time_elapsed    | 1487     |
|    total_timesteps | 118784   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 119000     |
| train/                  |            |
|    approx_kl            | 0.02169379 |
|    clip_fraction        | 0.189      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.61      |
|    explained_variance   | 0.938      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0299    |
|    n_updates            | 580        |
|    policy_gradient_loss | -0.0224    |
|    value_loss           | 0.000378   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.742    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 59       |
|    time_elapsed    | 1512     |
|    total_timesteps | 120832   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 121000      |
| train/                  |             |
|    approx_kl            | 0.023326077 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.61       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0712      |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.0211     |
|    value_loss           | 0.000152    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.74     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 60       |
|    time_elapsed    | 1537     |
|    total_timesteps | 122880   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 123000      |
| train/                  |             |
|    approx_kl            | 0.023995975 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.54       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0505     |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.0238     |
|    value_loss           | 8.76e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.737    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 61       |
|    time_elapsed    | 1563     |
|    total_timesteps | 124928   |
---------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+03     |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 125000    |
| train/                  |           |
|    approx_kl            | 0.0217091 |
|    clip_fraction        | 0.236     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.6      |
|    explained_variance   | 0.937     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.051    |
|    n_updates            | 610       |
|    policy_gradient_loss | -0.0232   |
|    value_loss           | 0.000286  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 999      |
|    ep_rew_mean     | 0.741    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 62       |
|    time_elapsed    | 1588     |
|    total_timesteps | 126976   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 127000      |
| train/                  |             |
|    approx_kl            | 0.017177563 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.49       |
|    explained_variance   | 0.897       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0301     |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.0264     |
|    value_loss           | 0.000343    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 996      |
|    ep_rew_mean     | 0.751    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 63       |
|    time_elapsed    | 1623     |
|    total_timesteps | 129024   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 130000      |
| train/                  |             |
|    approx_kl            | 0.017670266 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.43       |
|    explained_variance   | 0.893       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0276     |
|    n_updates            | 630         |
|    policy_gradient_loss | -0.0258     |
|    value_loss           | 0.000838    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.761    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 64       |
|    time_elapsed    | 1649     |
|    total_timesteps | 131072   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 132000     |
| train/                  |            |
|    approx_kl            | 0.01986641 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.37      |
|    explained_variance   | 0.953      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0628    |
|    n_updates            | 640        |
|    policy_gradient_loss | -0.0243    |
|    value_loss           | 0.000308   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 998      |
|    ep_rew_mean     | 0.762    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 65       |
|    time_elapsed    | 1674     |
|    total_timesteps | 133120   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 134000     |
| train/                  |            |
|    approx_kl            | 0.01969016 |
|    clip_fraction        | 0.165      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.46      |
|    explained_variance   | 0.887      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0032     |
|    n_updates            | 650        |
|    policy_gradient_loss | -0.027     |
|    value_loss           | 0.000463   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 998      |
|    ep_rew_mean     | 0.78     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 66       |
|    time_elapsed    | 1699     |
|    total_timesteps | 135168   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 136000      |
| train/                  |             |
|    approx_kl            | 0.016973253 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.48       |
|    explained_variance   | 0.795       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0186     |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 0.00155     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.786    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 67       |
|    time_elapsed    | 1725     |
|    total_timesteps | 137216   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 138000      |
| train/                  |             |
|    approx_kl            | 0.016468663 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.62       |
|    explained_variance   | 0.855       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0287     |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.000337    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.791    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 68       |
|    time_elapsed    | 1750     |
|    total_timesteps | 139264   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 140000      |
| train/                  |             |
|    approx_kl            | 0.018093439 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.45       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0423     |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.028      |
|    value_loss           | 0.000104    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.801    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 69       |
|    time_elapsed    | 1775     |
|    total_timesteps | 141312   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.3        |
| time/                   |            |
|    total_timesteps      | 142000     |
| train/                  |            |
|    approx_kl            | 0.01646172 |
|    clip_fraction        | 0.191      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.27      |
|    explained_variance   | 0.858      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0254    |
|    n_updates            | 690        |
|    policy_gradient_loss | -0.0216    |
|    value_loss           | 0.00096    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 993      |
|    ep_rew_mean     | 0.813    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 70       |
|    time_elapsed    | 1801     |
|    total_timesteps | 143360   |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | 0.3          |
| time/                   |              |
|    total_timesteps      | 144000       |
| train/                  |              |
|    approx_kl            | 0.0136812115 |
|    clip_fraction        | 0.166        |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.27        |
|    explained_variance   | 0.879        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0304      |
|    n_updates            | 700          |
|    policy_gradient_loss | -0.018       |
|    value_loss           | 0.000902     |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.82     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 71       |
|    time_elapsed    | 1826     |
|    total_timesteps | 145408   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 146000      |
| train/                  |             |
|    approx_kl            | 0.012873851 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.19       |
|    explained_variance   | 0.749       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.035      |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.0145     |
|    value_loss           | 0.00125     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 992      |
|    ep_rew_mean     | 0.821    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 72       |
|    time_elapsed    | 1851     |
|    total_timesteps | 147456   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 148000      |
| train/                  |             |
|    approx_kl            | 0.019096298 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.1        |
|    explained_variance   | 0.879       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0558     |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.0229     |
|    value_loss           | 0.000802    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.831    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 73       |
|    time_elapsed    | 1877     |
|    total_timesteps | 149504   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.046974026 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.27       |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0782     |
|    n_updates            | 730         |
|    policy_gradient_loss | -0.0199     |
|    value_loss           | 0.000199    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 151000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.833    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 74       |
|    time_elapsed    | 1902     |
|    total_timesteps | 151552   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 152000      |
| train/                  |             |
|    approx_kl            | 0.017306644 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.08       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.012      |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.0215     |
|    value_loss           | 0.000178    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 153000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.836    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 75       |
|    time_elapsed    | 1927     |
|    total_timesteps | 153600   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 154000      |
| train/                  |             |
|    approx_kl            | 0.019048028 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.23       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0203     |
|    n_updates            | 750         |
|    policy_gradient_loss | -0.0262     |
|    value_loss           | 0.000163    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 985      |
|    ep_rew_mean     | 0.84     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 76       |
|    time_elapsed    | 1953     |
|    total_timesteps | 155648   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 156000      |
| train/                  |             |
|    approx_kl            | 0.015955644 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.05       |
|    explained_variance   | 0.867       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0522     |
|    n_updates            | 760         |
|    policy_gradient_loss | -0.022      |
|    value_loss           | 0.000815    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 990      |
|    ep_rew_mean     | 0.85     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 77       |
|    time_elapsed    | 1978     |
|    total_timesteps | 157696   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 158000      |
| train/                  |             |
|    approx_kl            | 0.013840981 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.77       |
|    explained_variance   | 0.834       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00572     |
|    n_updates            | 770         |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.000262    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 159000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 990      |
|    ep_rew_mean     | 0.852    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 78       |
|    time_elapsed    | 2003     |
|    total_timesteps | 159744   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 160000      |
| train/                  |             |
|    approx_kl            | 0.014117888 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.37       |
|    explained_variance   | 0.888       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0418     |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.0184     |
|    value_loss           | 0.000186    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 161000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 990      |
|    ep_rew_mean     | 0.852    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 79       |
|    time_elapsed    | 2028     |
|    total_timesteps | 161792   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 162000      |
| train/                  |             |
|    approx_kl            | 0.021617316 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.03       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000703    |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.0249     |
|    value_loss           | 9.35e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 163000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 983      |
|    ep_rew_mean     | 0.857    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 80       |
|    time_elapsed    | 2054     |
|    total_timesteps | 163840   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.3        |
| time/                   |            |
|    total_timesteps      | 164000     |
| train/                  |            |
|    approx_kl            | 0.01676426 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.96      |
|    explained_variance   | 0.814      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0235    |
|    n_updates            | 800        |
|    policy_gradient_loss | -0.0142    |
|    value_loss           | 0.00183    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 165000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 981      |
|    ep_rew_mean     | 0.868    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 81       |
|    time_elapsed    | 2079     |
|    total_timesteps | 165888   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.3        |
| time/                   |            |
|    total_timesteps      | 166000     |
| train/                  |            |
|    approx_kl            | 0.02196542 |
|    clip_fraction        | 0.243      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.07      |
|    explained_variance   | 0.936      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.041      |
|    n_updates            | 810        |
|    policy_gradient_loss | -0.0244    |
|    value_loss           | 0.000199   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 167000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 981      |
|    ep_rew_mean     | 0.878    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 82       |
|    time_elapsed    | 2104     |
|    total_timesteps | 167936   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 168000      |
| train/                  |             |
|    approx_kl            | 0.016859971 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.36       |
|    explained_variance   | 0.785       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0518     |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.0194     |
|    value_loss           | 0.000926    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 169000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 968      |
|    ep_rew_mean     | 0.889    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 83       |
|    time_elapsed    | 2130     |
|    total_timesteps | 169984   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 170000      |
| train/                  |             |
|    approx_kl            | 0.019187985 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.776       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00523    |
|    n_updates            | 830         |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.00222     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 930      |
|    ep_rew_mean     | 0.91     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 84       |
|    time_elapsed    | 2165     |
|    total_timesteps | 172032   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 173000      |
| train/                  |             |
|    approx_kl            | 0.019320082 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.779       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0128     |
|    n_updates            | 840         |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.000867    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 883      |
|    ep_rew_mean     | 0.929    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 85       |
|    time_elapsed    | 2190     |
|    total_timesteps | 174080   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.6        |
| time/                   |            |
|    total_timesteps      | 175000     |
| train/                  |            |
|    approx_kl            | 0.02089968 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.35      |
|    explained_variance   | 0.82       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0195    |
|    n_updates            | 850        |
|    policy_gradient_loss | -0.0207    |
|    value_loss           | 0.00277    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 871      |
|    ep_rew_mean     | 0.95     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 86       |
|    time_elapsed    | 2216     |
|    total_timesteps | 176128   |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | 0.6          |
| time/                   |              |
|    total_timesteps      | 177000       |
| train/                  |              |
|    approx_kl            | 0.0152023295 |
|    clip_fraction        | 0.179        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.37        |
|    explained_variance   | 0.793        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0336      |
|    n_updates            | 860          |
|    policy_gradient_loss | -0.0166      |
|    value_loss           | 0.0027       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 882      |
|    ep_rew_mean     | 0.958    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 87       |
|    time_elapsed    | 2241     |
|    total_timesteps | 178176   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 179000      |
| train/                  |             |
|    approx_kl            | 0.022698306 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.023      |
|    n_updates            | 870         |
|    policy_gradient_loss | -0.0209     |
|    value_loss           | 9.02e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 873      |
|    ep_rew_mean     | 0.955    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 88       |
|    time_elapsed    | 2266     |
|    total_timesteps | 180224   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 181000      |
| train/                  |             |
|    approx_kl            | 0.015177941 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.789       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.024       |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.00907    |
|    value_loss           | 0.00143     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 871      |
|    ep_rew_mean     | 0.96     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 89       |
|    time_elapsed    | 2292     |
|    total_timesteps | 182272   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 183000      |
| train/                  |             |
|    approx_kl            | 0.013294915 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.729       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00128    |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 0.00187     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 851      |
|    ep_rew_mean     | 0.976    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 90       |
|    time_elapsed    | 2317     |
|    total_timesteps | 184320   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 185000      |
| train/                  |             |
|    approx_kl            | 0.024456257 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0365     |
|    n_updates            | 900         |
|    policy_gradient_loss | -0.0217     |
|    value_loss           | 6.48e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 841      |
|    ep_rew_mean     | 0.987    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 91       |
|    time_elapsed    | 2342     |
|    total_timesteps | 186368   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 187000      |
| train/                  |             |
|    approx_kl            | 0.018352874 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.42       |
|    explained_variance   | 0.616       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00717    |
|    n_updates            | 910         |
|    policy_gradient_loss | -0.00824    |
|    value_loss           | 0.00451     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 840      |
|    ep_rew_mean     | 0.997    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 92       |
|    time_elapsed    | 2368     |
|    total_timesteps | 188416   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 189000      |
| train/                  |             |
|    approx_kl            | 0.021719787 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.675       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0347     |
|    n_updates            | 920         |
|    policy_gradient_loss | -0.00985    |
|    value_loss           | 0.00209     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 850      |
|    ep_rew_mean     | 1.01     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 93       |
|    time_elapsed    | 2393     |
|    total_timesteps | 190464   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.6        |
| time/                   |            |
|    total_timesteps      | 191000     |
| train/                  |            |
|    approx_kl            | 0.01025736 |
|    clip_fraction        | 0.146      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.02      |
|    explained_variance   | 0.677      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00567    |
|    n_updates            | 930        |
|    policy_gradient_loss | -0.00764   |
|    value_loss           | 0.00201    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 850      |
|    ep_rew_mean     | 1.01     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 94       |
|    time_elapsed    | 2418     |
|    total_timesteps | 192512   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 193000      |
| train/                  |             |
|    approx_kl            | 0.025339846 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.02       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0262     |
|    n_updates            | 940         |
|    policy_gradient_loss | -0.0184     |
|    value_loss           | 2.43e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 831      |
|    ep_rew_mean     | 1.02     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 95       |
|    time_elapsed    | 2444     |
|    total_timesteps | 194560   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 195000      |
| train/                  |             |
|    approx_kl            | 0.016446171 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.605       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0158     |
|    n_updates            | 950         |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 0.0044      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 830      |
|    ep_rew_mean     | 1.03     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 96       |
|    time_elapsed    | 2469     |
|    total_timesteps | 196608   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 197000      |
| train/                  |             |
|    approx_kl            | 0.012995442 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.673       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000356    |
|    n_updates            | 960         |
|    policy_gradient_loss | -0.00362    |
|    value_loss           | 0.00226     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 802      |
|    ep_rew_mean     | 1.05     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 97       |
|    time_elapsed    | 2494     |
|    total_timesteps | 198656   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 199000      |
| train/                  |             |
|    approx_kl            | 0.017357726 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.835       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0178      |
|    n_updates            | 970         |
|    policy_gradient_loss | -0.00985    |
|    value_loss           | 0.00228     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 791      |
|    ep_rew_mean     | 1.08     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 98       |
|    time_elapsed    | 2520     |
|    total_timesteps | 200704   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.6        |
| time/                   |            |
|    total_timesteps      | 201000     |
| train/                  |            |
|    approx_kl            | 0.01008061 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.07      |
|    explained_variance   | 0.685      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0112    |
|    n_updates            | 980        |
|    policy_gradient_loss | -0.00471   |
|    value_loss           | 0.00431    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 794      |
|    ep_rew_mean     | 1.1      |
| time/              |          |
|    fps             | 79       |
|    iterations      | 99       |
|    time_elapsed    | 2545     |
|    total_timesteps | 202752   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 203000      |
| train/                  |             |
|    approx_kl            | 0.018839099 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.07       |
|    explained_variance   | 0.602       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0431     |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.0197     |
|    value_loss           | 0.00301     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 772      |
|    ep_rew_mean     | 1.1      |
| time/              |          |
|    fps             | 79       |
|    iterations      | 100      |
|    time_elapsed    | 2570     |
|    total_timesteps | 204800   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 205000      |
| train/                  |             |
|    approx_kl            | 0.013572605 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.74       |
|    explained_variance   | 0.645       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0269      |
|    n_updates            | 1000        |
|    policy_gradient_loss | -0.00571    |
|    value_loss           | 0.00427     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 751      |
|    ep_rew_mean     | 1.1      |
| time/              |          |
|    fps             | 79       |
|    iterations      | 101      |
|    time_elapsed    | 2596     |
|    total_timesteps | 206848   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 207000      |
| train/                  |             |
|    approx_kl            | 0.014109432 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.57       |
|    explained_variance   | 0.765       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00254    |
|    n_updates            | 1010        |
|    policy_gradient_loss | -0.00922    |
|    value_loss           | 0.00234     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 686      |
|    ep_rew_mean     | 1.11     |
| time/              |          |
|    fps             | 80       |
|    iterations      | 102      |
|    time_elapsed    | 2601     |
|    total_timesteps | 208896   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 209000      |
| train/                  |             |
|    approx_kl            | 0.015978374 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.46       |
|    explained_variance   | 0.863       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0245     |
|    n_updates            | 1020        |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 0.0025      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 513      |
|    ep_rew_mean     | 1.11     |
| time/              |          |
|    fps             | 80       |
|    iterations      | 103      |
|    time_elapsed    | 2607     |
|    total_timesteps | 210944   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 211000      |
| train/                  |             |
|    approx_kl            | 0.011325717 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.35       |
|    explained_variance   | 0.846       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00883    |
|    n_updates            | 1030        |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 0.00509     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 293      |
|    ep_rew_mean     | 1.05     |
| time/              |          |
|    fps             | 81       |
|    iterations      | 104      |
|    time_elapsed    | 2612     |
|    total_timesteps | 212992   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 213000      |
| train/                  |             |
|    approx_kl            | 0.014264628 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.13       |
|    explained_variance   | 0.882       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0298     |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.00822    |
|    value_loss           | 0.0052      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.3     |
|    ep_rew_mean     | 0.927    |
| time/              |          |
|    fps             | 82       |
|    iterations      | 105      |
|    time_elapsed    | 2618     |
|    total_timesteps | 215040   |
---------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 9         |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 216000    |
| train/                  |           |
|    approx_kl            | 0.0372649 |
|    clip_fraction        | 0.303     |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.44     |
|    explained_variance   | 0.967     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0772   |
|    n_updates            | 1050      |
|    policy_gradient_loss | -0.0299   |
|    value_loss           | 0.00055   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.8     |
|    ep_rew_mean     | 0.908    |
| time/              |          |
|    fps             | 82       |
|    iterations      | 106      |
|    time_elapsed    | 2624     |
|    total_timesteps | 217088   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 218000      |
| train/                  |             |
|    approx_kl            | 0.032797962 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.22       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0529     |
|    n_updates            | 1060        |
|    policy_gradient_loss | -0.0329     |
|    value_loss           | 0.00103     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.1     |
|    ep_rew_mean     | 0.911    |
| time/              |          |
|    fps             | 83       |
|    iterations      | 107      |
|    time_elapsed    | 2629     |
|    total_timesteps | 219136   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 220000      |
| train/                  |             |
|    approx_kl            | 0.015206522 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.71       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0293     |
|    n_updates            | 1070        |
|    policy_gradient_loss | -0.0129     |
|    value_loss           | 0.00106     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.4     |
|    ep_rew_mean     | 0.924    |
| time/              |          |
|    fps             | 83       |
|    iterations      | 108      |
|    time_elapsed    | 2635     |
|    total_timesteps | 221184   |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 9            |
|    mean_reward          | 0.9          |
| time/                   |              |
|    total_timesteps      | 222000       |
| train/                  |              |
|    approx_kl            | 0.0077282907 |
|    clip_fraction        | 0.103        |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.76        |
|    explained_variance   | 0.855        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00891     |
|    n_updates            | 1080         |
|    policy_gradient_loss | -0.00846     |
|    value_loss           | 0.00221      |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.3     |
|    ep_rew_mean     | 0.931    |
| time/              |          |
|    fps             | 84       |
|    iterations      | 109      |
|    time_elapsed    | 2640     |
|    total_timesteps | 223232   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 9          |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 224000     |
| train/                  |            |
|    approx_kl            | 0.01198089 |
|    clip_fraction        | 0.171      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.21      |
|    explained_variance   | 0.667      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00818   |
|    n_updates            | 1090       |
|    policy_gradient_loss | -0.00873   |
|    value_loss           | 0.00297    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 56.7     |
|    ep_rew_mean     | 0.939    |
| time/              |          |
|    fps             | 85       |
|    iterations      | 110      |
|    time_elapsed    | 2646     |
|    total_timesteps | 225280   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 226000      |
| train/                  |             |
|    approx_kl            | 0.008162782 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.19       |
|    explained_variance   | 0.505       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.012      |
|    n_updates            | 1100        |
|    policy_gradient_loss | -0.00275    |
|    value_loss           | 0.00363     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.8     |
|    ep_rew_mean     | 0.945    |
| time/              |          |
|    fps             | 85       |
|    iterations      | 111      |
|    time_elapsed    | 2651     |
|    total_timesteps | 227328   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 228000      |
| train/                  |             |
|    approx_kl            | 0.028528895 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.27       |
|    explained_variance   | 0.819       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00864    |
|    n_updates            | 1110        |
|    policy_gradient_loss | -0.00873    |
|    value_loss           | 0.000624    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 0.961    |
| time/              |          |
|    fps             | 86       |
|    iterations      | 112      |
|    time_elapsed    | 2657     |
|    total_timesteps | 229376   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 230000      |
| train/                  |             |
|    approx_kl            | 0.019377388 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.21       |
|    explained_variance   | 0.682       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0647      |
|    n_updates            | 1120        |
|    policy_gradient_loss | -0.00744    |
|    value_loss           | 0.0038      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 0.969    |
| time/              |          |
|    fps             | 86       |
|    iterations      | 113      |
|    time_elapsed    | 2682     |
|    total_timesteps | 231424   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 232000      |
| train/                  |             |
|    approx_kl            | 0.008947598 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.11       |
|    explained_variance   | 0.576       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0124     |
|    n_updates            | 1130        |
|    policy_gradient_loss | -0.00571    |
|    value_loss           | 0.00371     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 130      |
|    ep_rew_mean     | 0.977    |
| time/              |          |
|    fps             | 86       |
|    iterations      | 114      |
|    time_elapsed    | 2707     |
|    total_timesteps | 233472   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 234000      |
| train/                  |             |
|    approx_kl            | 0.007349587 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.02       |
|    explained_variance   | 0.358       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0135      |
|    n_updates            | 1140        |
|    policy_gradient_loss | -0.00164    |
|    value_loss           | 0.00433     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 159      |
|    ep_rew_mean     | 0.983    |
| time/              |          |
|    fps             | 86       |
|    iterations      | 115      |
|    time_elapsed    | 2733     |
|    total_timesteps | 235520   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 236000      |
| train/                  |             |
|    approx_kl            | 0.017470714 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.09       |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0291     |
|    n_updates            | 1150        |
|    policy_gradient_loss | -0.00466    |
|    value_loss           | 0.000225    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 237000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | 0.981    |
| time/              |          |
|    fps             | 86       |
|    iterations      | 116      |
|    time_elapsed    | 2758     |
|    total_timesteps | 237568   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 238000      |
| train/                  |             |
|    approx_kl            | 0.015173059 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.21       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0126     |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.00222    |
|    value_loss           | 0.00015     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 239000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 205      |
|    ep_rew_mean     | 0.979    |
| time/              |          |
|    fps             | 86       |
|    iterations      | 117      |
|    time_elapsed    | 2783     |
|    total_timesteps | 239616   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 0.021131633 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.42       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0304      |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.0028     |
|    value_loss           | 0.000102    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 241000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 219      |
|    ep_rew_mean     | 0.986    |
| time/              |          |
|    fps             | 86       |
|    iterations      | 118      |
|    time_elapsed    | 2809     |
|    total_timesteps | 241664   |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | 0.8          |
| time/                   |              |
|    total_timesteps      | 242000       |
| train/                  |              |
|    approx_kl            | 0.0084500685 |
|    clip_fraction        | 0.138        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.39        |
|    explained_variance   | 0.549        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.024       |
|    n_updates            | 1180         |
|    policy_gradient_loss | -0.0011      |
|    value_loss           | 0.0025       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 243000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 228      |
|    ep_rew_mean     | 0.994    |
| time/              |          |
|    fps             | 85       |
|    iterations      | 119      |
|    time_elapsed    | 2834     |
|    total_timesteps | 243712   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 244000      |
| train/                  |             |
|    approx_kl            | 0.011538299 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.27       |
|    explained_variance   | 0.371       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0186      |
|    n_updates            | 1190        |
|    policy_gradient_loss | -0.00493    |
|    value_loss           | 0.00485     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 258      |
|    ep_rew_mean     | 1        |
| time/              |          |
|    fps             | 85       |
|    iterations      | 120      |
|    time_elapsed    | 2859     |
|    total_timesteps | 245760   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.8        |
| time/                   |            |
|    total_timesteps      | 246000     |
| train/                  |            |
|    approx_kl            | 0.02457313 |
|    clip_fraction        | 0.285      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.49      |
|    explained_variance   | 0.974      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00608    |
|    n_updates            | 1200       |
|    policy_gradient_loss | -0.00667   |
|    value_loss           | 6.22e-05   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 247000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 268      |
|    ep_rew_mean     | 0.999    |
| time/              |          |
|    fps             | 85       |
|    iterations      | 121      |
|    time_elapsed    | 2885     |
|    total_timesteps | 247808   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 248000      |
| train/                  |             |
|    approx_kl            | 0.009865278 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.36       |
|    explained_variance   | 0.546       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00105    |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.00173    |
|    value_loss           | 0.00248     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 1.01     |
| time/              |          |
|    fps             | 85       |
|    iterations      | 122      |
|    time_elapsed    | 2910     |
|    total_timesteps | 249856   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.024228793 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.5        |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0167     |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.00472    |
|    value_loss           | 4.58e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 317      |
|    ep_rew_mean     | 1.01     |
| time/              |          |
|    fps             | 85       |
|    iterations      | 123      |
|    time_elapsed    | 2935     |
|    total_timesteps | 251904   |
---------------------------------
