Logging to ../Logging/RPPO_BW_Then_FW_10_400
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 54.6                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 1                    |
|    time_elapsed    | -1683717610063963648 |
|    total_timesteps | 2048                 |
---------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 3000        |
| train/                  |             |
|    approx_kl            | 0.024812475 |
|    clip_fraction        | 0.285       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.53       |
|    explained_variance   | -2.34       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0573     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0648     |
|    value_loss           | 0.04        |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 50.5                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 2                    |
|    time_elapsed    | -1683717610063963392 |
|    total_timesteps | 4096                 |
---------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.100130595 |
|    clip_fraction        | 0.546       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.48       |
|    explained_variance   | -0.122      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0456     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0325     |
|    value_loss           | 0.00819     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 48.1                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 3                    |
|    time_elapsed    | -1683717610063963392 |
|    total_timesteps | 6144                 |
---------------------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 7000       |
| train/                  |            |
|    approx_kl            | 0.23545201 |
|    clip_fraction        | 0.664      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.39      |
|    explained_variance   | 0.401      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0523    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0349    |
|    value_loss           | 0.00578    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 46.5                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 4                    |
|    time_elapsed    | -1683717610063963136 |
|    total_timesteps | 8192                 |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+03     |
|    mean_reward          | 0         |
| time/                   |           |
|    total_timesteps      | 9000      |
| train/                  |           |
|    approx_kl            | 0.4784602 |
|    clip_fraction        | 0.77      |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.26     |
|    explained_variance   | 0.612     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.012    |
|    n_updates            | 80        |
|    policy_gradient_loss | -0.0174   |
|    value_loss           | 0.00344   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 42.4                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 5                    |
|    time_elapsed    | -1683717610063963136 |
|    total_timesteps | 10240                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+03     |
|    mean_reward          | 0         |
| time/                   |           |
|    total_timesteps      | 11000     |
| train/                  |           |
|    approx_kl            | 1.0606139 |
|    clip_fraction        | 0.856     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.04     |
|    explained_variance   | 0.588     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0515   |
|    n_updates            | 100       |
|    policy_gradient_loss | -0.00994  |
|    value_loss           | 0.00376   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 38.8                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 6                    |
|    time_elapsed    | -1683717610063962880 |
|    total_timesteps | 12288                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+03     |
|    mean_reward          | 0         |
| time/                   |           |
|    total_timesteps      | 13000     |
| train/                  |           |
|    approx_kl            | 2.1010108 |
|    clip_fraction        | 0.906     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.79     |
|    explained_variance   | 0.554     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.00793   |
|    n_updates            | 120       |
|    policy_gradient_loss | -0.00845  |
|    value_loss           | 0.00132   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 34                   |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 7                    |
|    time_elapsed    | -1683717610063962880 |
|    total_timesteps | 14336                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 1e+03    |
|    mean_reward          | 0        |
| time/                   |          |
|    total_timesteps      | 15000    |
| train/                  |          |
|    approx_kl            | 2.990575 |
|    clip_fraction        | 0.929    |
|    clip_range           | 0.2      |
|    entropy_loss         | -5.76    |
|    explained_variance   | 0.482    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.026   |
|    n_updates            | 140      |
|    policy_gradient_loss | -0.00278 |
|    value_loss           | 0.00181  |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 29.1                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 8                    |
|    time_elapsed    | -1683717610063962880 |
|    total_timesteps | 16384                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 1e+03    |
|    mean_reward          | 0        |
| time/                   |          |
|    total_timesteps      | 17000    |
| train/                  |          |
|    approx_kl            | 5.504555 |
|    clip_fraction        | 0.946    |
|    clip_range           | 0.2      |
|    entropy_loss         | -5.6     |
|    explained_variance   | 0.514    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.00614 |
|    n_updates            | 160      |
|    policy_gradient_loss | -0.025   |
|    value_loss           | 0.000696 |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 23.2                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 9                    |
|    time_elapsed    | -1683717610063962624 |
|    total_timesteps | 18432                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 1e+03    |
|    mean_reward          | 0        |
| time/                   |          |
|    total_timesteps      | 19000    |
| train/                  |          |
|    approx_kl            | 9.386473 |
|    clip_fraction        | 0.962    |
|    clip_range           | 0.2      |
|    entropy_loss         | -5.31    |
|    explained_variance   | 0.435    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0614  |
|    n_updates            | 180      |
|    policy_gradient_loss | -0.041   |
|    value_loss           | 0.00225  |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 20.1                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 10                   |
|    time_elapsed    | -1683717610063962624 |
|    total_timesteps | 20480                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+03     |
|    mean_reward          | 0         |
| time/                   |           |
|    total_timesteps      | 21000     |
| train/                  |           |
|    approx_kl            | 13.752752 |
|    clip_fraction        | 0.956     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.07     |
|    explained_variance   | 0.364     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0179   |
|    n_updates            | 200       |
|    policy_gradient_loss | -0.0196   |
|    value_loss           | 0.000755  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 19                   |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 11                   |
|    time_elapsed    | -1683717610063962368 |
|    total_timesteps | 22528                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 23000     |
| train/                  |           |
|    approx_kl            | 19.679077 |
|    clip_fraction        | 0.961     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.7      |
|    explained_variance   | 0.347     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0856   |
|    n_updates            | 220       |
|    policy_gradient_loss | -0.0377   |
|    value_loss           | 0.00383   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 16.9                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 12                   |
|    time_elapsed    | -1683717610063962368 |
|    total_timesteps | 24576                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 25000     |
| train/                  |           |
|    approx_kl            | 32.205093 |
|    clip_fraction        | 0.969     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.31     |
|    explained_variance   | 0.284     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0576   |
|    n_updates            | 240       |
|    policy_gradient_loss | -0.0598   |
|    value_loss           | 0.00187   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 14.8                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 13                   |
|    time_elapsed    | -1683717610063962368 |
|    total_timesteps | 26624                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 27000     |
| train/                  |           |
|    approx_kl            | 109.08828 |
|    clip_fraction        | 0.962     |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.89     |
|    explained_variance   | 0.162     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0485   |
|    n_updates            | 260       |
|    policy_gradient_loss | -0.0535   |
|    value_loss           | 0.00289   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 15.2                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 14                   |
|    time_elapsed    | -1683717610063962368 |
|    total_timesteps | 28672                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 10       |
|    mean_reward          | 0.9      |
| time/                   |          |
|    total_timesteps      | 29000    |
| train/                  |          |
|    approx_kl            | 37.62509 |
|    clip_fraction        | 0.958    |
|    clip_range           | 0.2      |
|    entropy_loss         | -3.6     |
|    explained_variance   | 0.278    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0432  |
|    n_updates            | 280      |
|    policy_gradient_loss | -0.0576  |
|    value_loss           | 0.000474 |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 14.2                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 15                   |
|    time_elapsed    | -1683717610063962368 |
|    total_timesteps | 30720                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 31000     |
| train/                  |           |
|    approx_kl            | 55.395454 |
|    clip_fraction        | 0.949     |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.33     |
|    explained_variance   | 0.165     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0674   |
|    n_updates            | 300       |
|    policy_gradient_loss | -0.0615   |
|    value_loss           | 0.00222   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 13                   |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 16                   |
|    time_elapsed    | -1683717610063962112 |
|    total_timesteps | 32768                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 10       |
|    mean_reward          | 0.9      |
| time/                   |          |
|    total_timesteps      | 33000    |
| train/                  |          |
|    approx_kl            | 64.03832 |
|    clip_fraction        | 0.937    |
|    clip_range           | 0.2      |
|    entropy_loss         | -3.03    |
|    explained_variance   | 0.118    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0587  |
|    n_updates            | 320      |
|    policy_gradient_loss | -0.0336  |
|    value_loss           | 0.000495 |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 12.3                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 17                   |
|    time_elapsed    | -1683717610063962112 |
|    total_timesteps | 34816                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 79.018295 |
|    clip_fraction        | 0.931     |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.75     |
|    explained_variance   | 0.0947    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0581   |
|    n_updates            | 340       |
|    policy_gradient_loss | 0.135     |
|    value_loss           | 0.00383   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 11.6                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 18                   |
|    time_elapsed    | -1683717610063962112 |
|    total_timesteps | 36864                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 10       |
|    mean_reward          | 0.9      |
| time/                   |          |
|    total_timesteps      | 37000    |
| train/                  |          |
|    approx_kl            | 67.52125 |
|    clip_fraction        | 0.932    |
|    clip_range           | 0.2      |
|    entropy_loss         | -2.42    |
|    explained_variance   | 0.0176   |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0773  |
|    n_updates            | 360      |
|    policy_gradient_loss | -0.0635  |
|    value_loss           | 0.00387  |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 11.6                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 19                   |
|    time_elapsed    | -1683717610063962112 |
|    total_timesteps | 38912                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 39000     |
| train/                  |           |
|    approx_kl            | 202.58151 |
|    clip_fraction        | 0.898     |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.27     |
|    explained_variance   | 0.154     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0758   |
|    n_updates            | 380       |
|    policy_gradient_loss | -0.0496   |
|    value_loss           | 0.000486  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 12.2                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 20                   |
|    time_elapsed    | -1683717610063962112 |
|    total_timesteps | 40960                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 41000     |
| train/                  |           |
|    approx_kl            | 375.64227 |
|    clip_fraction        | 0.902     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.93     |
|    explained_variance   | 0.00717   |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0554   |
|    n_updates            | 400       |
|    policy_gradient_loss | -0.0474   |
|    value_loss           | 0.000333  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 11.2                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 21                   |
|    time_elapsed    | -1683717610063961856 |
|    total_timesteps | 43008                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 10       |
|    mean_reward          | 0.9      |
| time/                   |          |
|    total_timesteps      | 44000    |
| train/                  |          |
|    approx_kl            | 67.63028 |
|    clip_fraction        | 0.814    |
|    clip_range           | 0.2      |
|    entropy_loss         | -1.52    |
|    explained_variance   | -0.0813  |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0763  |
|    n_updates            | 420      |
|    policy_gradient_loss | -0.0016  |
|    value_loss           | 0.00401  |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 10.9                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 22                   |
|    time_elapsed    | -1683717610063961856 |
|    total_timesteps | 45056                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 10       |
|    mean_reward          | 0.9      |
| time/                   |          |
|    total_timesteps      | 46000    |
| train/                  |          |
|    approx_kl            | 73.57585 |
|    clip_fraction        | 0.651    |
|    clip_range           | 0.2      |
|    entropy_loss         | -1.06    |
|    explained_variance   | -0.0589  |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0339  |
|    n_updates            | 440      |
|    policy_gradient_loss | -0.0228  |
|    value_loss           | 0.00213  |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 10.8                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 23                   |
|    time_elapsed    | -1683717610063961856 |
|    total_timesteps | 47104                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 10       |
|    mean_reward          | 0.9      |
| time/                   |          |
|    total_timesteps      | 48000    |
| train/                  |          |
|    approx_kl            | 291.8843 |
|    clip_fraction        | 0.524    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.692   |
|    explained_variance   | -0.0503  |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0769  |
|    n_updates            | 460      |
|    policy_gradient_loss | -0.0342  |
|    value_loss           | 0.00103  |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 10.5                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 24                   |
|    time_elapsed    | -1683717610063961856 |
|    total_timesteps | 49152                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 10       |
|    mean_reward          | 0.9      |
| time/                   |          |
|    total_timesteps      | 50000    |
| train/                  |          |
|    approx_kl            | 33.56971 |
|    clip_fraction        | 0.325    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.468   |
|    explained_variance   | -0.125   |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0806  |
|    n_updates            | 480      |
|    policy_gradient_loss | -0.0553  |
|    value_loss           | 0.00243  |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 10.8                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 25                   |
|    time_elapsed    | -1683717610063961856 |
|    total_timesteps | 51200                |
---------------------------------------------
{'reward': [0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9], 'std': [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]}
{'reward': [0.9000000134110451, 0.9000000134110451, 0.9000000134110451, 0.9000000134110451, 0.9000000134110451, 0.9000000134110451, 0.9000000134110451, 0.9000000134110451, 0.9000000134110451, 0.9000000134110451], 'std': [9, 9, 9, 9, 9, 9, 9, 9, 9, 9]}
