Logging to ../Logging/PPO_Forward_Baseline_5_600
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 19       |
|    iterations      | 1        |
|    time_elapsed    | 102      |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 3000        |
| train/                  |             |
|    approx_kl            | 0.014609481 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.16       |
|    explained_variance   | -0.353      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0417     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 0.0214      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.0914   |
| time/              |          |
|    fps             | 19       |
|    iterations      | 2        |
|    time_elapsed    | 208      |
|    total_timesteps | 4096     |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.019808564 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.13       |
|    explained_variance   | -0.752      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0308     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0202     |
|    value_loss           | 0.00832     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.103    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 3        |
|    time_elapsed    | 315      |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 7000        |
| train/                  |             |
|    approx_kl            | 0.019106813 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.11       |
|    explained_variance   | -0.431      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0242      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0211     |
|    value_loss           | 0.00282     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.115    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 4        |
|    time_elapsed    | 421      |
|    total_timesteps | 8192     |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 9000        |
| train/                  |             |
|    approx_kl            | 0.015238524 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.08       |
|    explained_variance   | -0.284      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00287    |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 0.00173     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.133    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 5        |
|    time_elapsed    | 526      |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 11000       |
| train/                  |             |
|    approx_kl            | 0.013162866 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.06       |
|    explained_variance   | 0.337       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00527    |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 0.00148     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.134    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 6        |
|    time_elapsed    | 632      |
|    total_timesteps | 12288    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 13000       |
| train/                  |             |
|    approx_kl            | 0.011222183 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.03       |
|    explained_variance   | 0.348       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0177     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0145     |
|    value_loss           | 0.00148     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.135    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 7        |
|    time_elapsed    | 737      |
|    total_timesteps | 14336    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50         |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.00957916 |
|    clip_fraction        | 0.12       |
|    clip_range           | 0.2        |
|    entropy_loss         | -5         |
|    explained_variance   | 0.429      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0144    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0125    |
|    value_loss           | 0.0014     |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.145    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 8        |
|    time_elapsed    | 843      |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 17000       |
| train/                  |             |
|    approx_kl            | 0.013242496 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.95       |
|    explained_variance   | 0.439       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0152      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.00135     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.178    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 9        |
|    time_elapsed    | 948      |
|    total_timesteps | 18432    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 19000       |
| train/                  |             |
|    approx_kl            | 0.014049372 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.91       |
|    explained_variance   | 0.389       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0187     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.00175     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 19       |
|    iterations      | 10       |
|    time_elapsed    | 1053     |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50         |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 21000      |
| train/                  |            |
|    approx_kl            | 0.01281168 |
|    clip_fraction        | 0.138      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.83      |
|    explained_variance   | 0.528      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0283    |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0148    |
|    value_loss           | 0.00191    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.204    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 11       |
|    time_elapsed    | 1158     |
|    total_timesteps | 22528    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 23000       |
| train/                  |             |
|    approx_kl            | 0.011148328 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.8        |
|    explained_variance   | 0.573       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0107      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0185     |
|    value_loss           | 0.00194     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.9     |
|    ep_rew_mean     | 0.206    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 12       |
|    time_elapsed    | 1264     |
|    total_timesteps | 24576    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.015992578 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.72       |
|    explained_variance   | 0.585       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0322     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0228     |
|    value_loss           | 0.00185     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | 0.216    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 13       |
|    time_elapsed    | 1369     |
|    total_timesteps | 26624    |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | 0.1          |
| time/                   |              |
|    total_timesteps      | 27000        |
| train/                  |              |
|    approx_kl            | 0.0125950305 |
|    clip_fraction        | 0.181        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.65        |
|    explained_variance   | 0.461        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0171      |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.0179      |
|    value_loss           | 0.00233      |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.5     |
|    ep_rew_mean     | 0.247    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 14       |
|    time_elapsed    | 1474     |
|    total_timesteps | 28672    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 29000       |
| train/                  |             |
|    approx_kl            | 0.012062396 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.537       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0391     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.00274     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.8     |
|    ep_rew_mean     | 0.268    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 15       |
|    time_elapsed    | 1578     |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 31000       |
| train/                  |             |
|    approx_kl            | 0.013851289 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.634       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00905     |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.00233     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.2     |
|    ep_rew_mean     | 0.288    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 16       |
|    time_elapsed    | 1683     |
|    total_timesteps | 32768    |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | 0.1          |
| time/                   |              |
|    total_timesteps      | 33000        |
| train/                  |              |
|    approx_kl            | 0.0143523365 |
|    clip_fraction        | 0.184        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.49        |
|    explained_variance   | 0.648        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0341      |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.0184      |
|    value_loss           | 0.00248      |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 43.3     |
|    ep_rew_mean     | 0.311    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 17       |
|    time_elapsed    | 1787     |
|    total_timesteps | 34816    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.014530603 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.658       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0205     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.019      |
|    value_loss           | 0.00243     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 40       |
|    ep_rew_mean     | 0.341    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 18       |
|    time_elapsed    | 1889     |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 37000       |
| train/                  |             |
|    approx_kl            | 0.014768653 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | 0.764       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0139      |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0211     |
|    value_loss           | 0.00205     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.9     |
|    ep_rew_mean     | 0.348    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 19       |
|    time_elapsed    | 1992     |
|    total_timesteps | 38912    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 39000       |
| train/                  |             |
|    approx_kl            | 0.017230157 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.817       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0053      |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.02       |
|    value_loss           | 0.00169     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 0.367    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 20       |
|    time_elapsed    | 2094     |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 41000       |
| train/                  |             |
|    approx_kl            | 0.019734614 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.11       |
|    explained_variance   | 0.829       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0309     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.022      |
|    value_loss           | 0.00136     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 31.6     |
|    ep_rew_mean     | 0.382    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 21       |
|    time_elapsed    | 2206     |
|    total_timesteps | 43008    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50         |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 44000      |
| train/                  |            |
|    approx_kl            | 0.01627547 |
|    clip_fraction        | 0.208      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4         |
|    explained_variance   | 0.736      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0253    |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.0182    |
|    value_loss           | 0.00254    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.4     |
|    ep_rew_mean     | 0.382    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 22       |
|    time_elapsed    | 2307     |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 46000       |
| train/                  |             |
|    approx_kl            | 0.027677014 |
|    clip_fraction        | 0.27        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.85       |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0531     |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0241     |
|    value_loss           | 0.000829    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.7     |
|    ep_rew_mean     | 0.396    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 23       |
|    time_elapsed    | 2408     |
|    total_timesteps | 47104    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 48000       |
| train/                  |             |
|    approx_kl            | 0.018534623 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.67       |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00438    |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0198     |
|    value_loss           | 0.000979    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.5     |
|    ep_rew_mean     | 0.402    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 24       |
|    time_elapsed    | 2509     |
|    total_timesteps | 49152    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.021044182 |
|    clip_fraction        | 0.288       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.41       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0139     |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0276     |
|    value_loss           | 0.000375    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.4     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 19       |
|    iterations      | 25       |
|    time_elapsed    | 2609     |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 52000       |
| train/                  |             |
|    approx_kl            | 0.021697111 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.24       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0233     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0249     |
|    value_loss           | 0.000233    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | 0.403    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 26       |
|    time_elapsed    | 2708     |
|    total_timesteps | 53248    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 54000       |
| train/                  |             |
|    approx_kl            | 0.019782651 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.01       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0118     |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0206     |
|    value_loss           | 0.000444    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.5     |
|    ep_rew_mean     | 0.404    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 27       |
|    time_elapsed    | 2789     |
|    total_timesteps | 55296    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 4          |
|    mean_reward          | 0.4        |
| time/                   |            |
|    total_timesteps      | 56000      |
| train/                  |            |
|    approx_kl            | 0.02005912 |
|    clip_fraction        | 0.299      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.78      |
|    explained_variance   | 0.972      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0811    |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.0311    |
|    value_loss           | 0.000274   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 13.1     |
|    ep_rew_mean     | 0.401    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 28       |
|    time_elapsed    | 2868     |
|    total_timesteps | 57344    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 58000       |
| train/                  |             |
|    approx_kl            | 0.031359278 |
|    clip_fraction        | 0.364       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.55       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00554     |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.036      |
|    value_loss           | 0.000111    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.14     |
|    ep_rew_mean     | 0.401    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 29       |
|    time_elapsed    | 2946     |
|    total_timesteps | 59392    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.017092422 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.22       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0272     |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0293     |
|    value_loss           | 0.000192    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 8.09     |
|    ep_rew_mean     | 0.403    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 30       |
|    time_elapsed    | 3023     |
|    total_timesteps | 61440    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 62000       |
| train/                  |             |
|    approx_kl            | 0.027996676 |
|    clip_fraction        | 0.358       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.01       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0441     |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.0387     |
|    value_loss           | 0.000473    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 7.48     |
|    ep_rew_mean     | 0.401    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 31       |
|    time_elapsed    | 3098     |
|    total_timesteps | 63488    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 4          |
|    mean_reward          | 0.4        |
| time/                   |            |
|    total_timesteps      | 64000      |
| train/                  |            |
|    approx_kl            | 0.02315458 |
|    clip_fraction        | 0.318      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.74      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0392    |
|    n_updates            | 310        |
|    policy_gradient_loss | -0.0424    |
|    value_loss           | 4.48e-05   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 6.76     |
|    ep_rew_mean     | 0.401    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 32       |
|    time_elapsed    | 3172     |
|    total_timesteps | 65536    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 66000       |
| train/                  |             |
|    approx_kl            | 0.043366574 |
|    clip_fraction        | 0.468       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.12       |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0603     |
|    value_loss           | 5.51e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.53     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 20       |
|    iterations      | 33       |
|    time_elapsed    | 3244     |
|    total_timesteps | 67584    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 68000       |
| train/                  |             |
|    approx_kl            | 0.018245148 |
|    clip_fraction        | 0.292       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0501     |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.0469     |
|    value_loss           | 8.84e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.14     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 21       |
|    iterations      | 34       |
|    time_elapsed    | 3314     |
|    total_timesteps | 69632    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 4          |
|    mean_reward          | 0.4        |
| time/                   |            |
|    total_timesteps      | 70000      |
| train/                  |            |
|    approx_kl            | 0.09895955 |
|    clip_fraction        | 0.322      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.639     |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.104     |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.0677    |
|    value_loss           | 2.11e-05   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.34     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 21       |
|    iterations      | 35       |
|    time_elapsed    | 3382     |
|    total_timesteps | 71680    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 72000       |
| train/                  |             |
|    approx_kl            | 0.016144557 |
|    clip_fraction        | 0.0653      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.507      |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.031      |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.000173    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.3      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 21       |
|    iterations      | 36       |
|    time_elapsed    | 3451     |
|    total_timesteps | 73728    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 74000       |
| train/                  |             |
|    approx_kl            | 0.038590483 |
|    clip_fraction        | 0.0451      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.309      |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0333     |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 6.48e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.18     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 21       |
|    iterations      | 37       |
|    time_elapsed    | 3519     |
|    total_timesteps | 75776    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 76000       |
| train/                  |             |
|    approx_kl            | 0.044899188 |
|    clip_fraction        | 0.0915      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.245      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.087      |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.0375     |
|    value_loss           | 8.55e-07    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.93     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 21       |
|    iterations      | 38       |
|    time_elapsed    | 3588     |
|    total_timesteps | 77824    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 78000       |
| train/                  |             |
|    approx_kl            | 0.038948584 |
|    clip_fraction        | 0.354       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.297      |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.064      |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.0539     |
|    value_loss           | 1.37e-06    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.26     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 21       |
|    iterations      | 39       |
|    time_elapsed    | 3657     |
|    total_timesteps | 79872    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.056102116 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.132      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0486     |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.0393     |
|    value_loss           | 2.77e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.06     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 21       |
|    iterations      | 40       |
|    time_elapsed    | 3725     |
|    total_timesteps | 81920    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 4          |
|    mean_reward          | 0.4        |
| time/                   |            |
|    total_timesteps      | 82000      |
| train/                  |            |
|    approx_kl            | 0.03541881 |
|    clip_fraction        | 0.111      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.199     |
|    explained_variance   | 0.995      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0629    |
|    n_updates            | 400        |
|    policy_gradient_loss | -0.0288    |
|    value_loss           | 6.42e-05   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.9      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 22       |
|    iterations      | 41       |
|    time_elapsed    | 3796     |
|    total_timesteps | 83968    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 84000       |
| train/                  |             |
|    approx_kl            | 0.011501167 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.449      |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0243     |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.0141     |
|    value_loss           | 0.000369    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.39     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 22       |
|    iterations      | 42       |
|    time_elapsed    | 3866     |
|    total_timesteps | 86016    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 87000       |
| train/                  |             |
|    approx_kl            | 0.026210172 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.353      |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0943     |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0393     |
|    value_loss           | 0.000171    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.61     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 22       |
|    iterations      | 43       |
|    time_elapsed    | 3935     |
|    total_timesteps | 88064    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 89000       |
| train/                  |             |
|    approx_kl            | 0.054859962 |
|    clip_fraction        | 0.51        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.246      |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.096      |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.0688     |
|    value_loss           | 5.71e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.24     |
|    ep_rew_mean     | 0.401    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 44       |
|    time_elapsed    | 4002     |
|    total_timesteps | 90112    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 91000       |
| train/                  |             |
|    approx_kl            | 0.056111597 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.253      |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0865     |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 2.21e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.79     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 22       |
|    iterations      | 45       |
|    time_elapsed    | 4072     |
|    total_timesteps | 92160    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 93000       |
| train/                  |             |
|    approx_kl            | 0.004144787 |
|    clip_fraction        | 0.0615      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.319      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | 8.44e-05    |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.0186     |
|    value_loss           | 2.82e-06    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.54     |
|    ep_rew_mean     | 0.401    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 46       |
|    time_elapsed    | 4140     |
|    total_timesteps | 94208    |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 4            |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 95000        |
| train/                  |              |
|    approx_kl            | 0.0049379794 |
|    clip_fraction        | 0.079        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.258       |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0338      |
|    n_updates            | 460          |
|    policy_gradient_loss | -0.0308      |
|    value_loss           | 1.19e-05     |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.28     |
|    ep_rew_mean     | 0.402    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 47       |
|    time_elapsed    | 4208     |
|    total_timesteps | 96256    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 97000       |
| train/                  |             |
|    approx_kl            | 0.033990055 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.124      |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.042      |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.0298     |
|    value_loss           | 6.29e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.08     |
|    ep_rew_mean     | 0.402    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 48       |
|    time_elapsed    | 4275     |
|    total_timesteps | 98304    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 99000       |
| train/                  |             |
|    approx_kl            | 0.029265733 |
|    clip_fraction        | 0.0198      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0598     |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0181     |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 7.67e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.09     |
|    ep_rew_mean     | 0.402    |
| time/              |          |
|    fps             | 23       |
|    iterations      | 49       |
|    time_elapsed    | 4342     |
|    total_timesteps | 100352   |
---------------------------------
{'reward': [0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645], 'std': [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]}
