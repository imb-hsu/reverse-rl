Logging to ../Logging/PPO_Forward_Baseline_5_700
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.095    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 1        |
|    time_elapsed    | 102      |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 3000        |
| train/                  |             |
|    approx_kl            | 0.018565536 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.15       |
|    explained_variance   | -0.712      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0136     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0201     |
|    value_loss           | 0.0168      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.0889   |
| time/              |          |
|    fps             | 19       |
|    iterations      | 2        |
|    time_elapsed    | 208      |
|    total_timesteps | 4096     |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.019162185 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.12       |
|    explained_variance   | -0.8        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0009     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0243     |
|    value_loss           | 0.00511     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.107    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 3        |
|    time_elapsed    | 314      |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 7000        |
| train/                  |             |
|    approx_kl            | 0.017032577 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.09       |
|    explained_variance   | -0.204      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0192     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 0.0025      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.129    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 4        |
|    time_elapsed    | 420      |
|    total_timesteps | 8192     |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50         |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 9000       |
| train/                  |            |
|    approx_kl            | 0.01723092 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.07      |
|    explained_variance   | -0.12      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00479    |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0184    |
|    value_loss           | 0.00247    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.13     |
| time/              |          |
|    fps             | 19       |
|    iterations      | 5        |
|    time_elapsed    | 526      |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50         |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 11000      |
| train/                  |            |
|    approx_kl            | 0.01161987 |
|    clip_fraction        | 0.144      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.03      |
|    explained_variance   | 0.0869     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0526    |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.014     |
|    value_loss           | 0.00194    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.131    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 6        |
|    time_elapsed    | 632      |
|    total_timesteps | 12288    |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | 0            |
| time/                   |              |
|    total_timesteps      | 13000        |
| train/                  |              |
|    approx_kl            | 0.0150680095 |
|    clip_fraction        | 0.167        |
|    clip_range           | 0.2          |
|    entropy_loss         | -5           |
|    explained_variance   | 0.16         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0168       |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.0178      |
|    value_loss           | 0.00136      |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.134    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 7        |
|    time_elapsed    | 737      |
|    total_timesteps | 14336    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.011174428 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.98       |
|    explained_variance   | 0.344       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0399     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0142     |
|    value_loss           | 0.00163     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.155    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 8        |
|    time_elapsed    | 842      |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 17000       |
| train/                  |             |
|    approx_kl            | 0.018328633 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.92       |
|    explained_variance   | 0.452       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0636     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0203     |
|    value_loss           | 0.00177     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.161    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 9        |
|    time_elapsed    | 948      |
|    total_timesteps | 18432    |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | 0            |
| time/                   |              |
|    total_timesteps      | 19000        |
| train/                  |              |
|    approx_kl            | 0.0141012445 |
|    clip_fraction        | 0.179        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.87        |
|    explained_variance   | 0.581        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0026      |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.0184      |
|    value_loss           | 0.00138      |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.177    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 10       |
|    time_elapsed    | 1053     |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 21000       |
| train/                  |             |
|    approx_kl            | 0.011274666 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.83       |
|    explained_variance   | 0.728       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0101     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0143     |
|    value_loss           | 0.000888    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.196    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 11       |
|    time_elapsed    | 1158     |
|    total_timesteps | 22528    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 23000       |
| train/                  |             |
|    approx_kl            | 0.014635356 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.79       |
|    explained_variance   | 0.625       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0108      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.00164     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.208    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 12       |
|    time_elapsed    | 1264     |
|    total_timesteps | 24576    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.014185516 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.78       |
|    explained_variance   | 0.639       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0444     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0204     |
|    value_loss           | 0.00131     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.2     |
|    ep_rew_mean     | 0.222    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 13       |
|    time_elapsed    | 1369     |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 27000       |
| train/                  |             |
|    approx_kl            | 0.013235778 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.72       |
|    explained_variance   | 0.619       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00904    |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0201     |
|    value_loss           | 0.00185     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.7     |
|    ep_rew_mean     | 0.232    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 14       |
|    time_elapsed    | 1474     |
|    total_timesteps | 28672    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 29000       |
| train/                  |             |
|    approx_kl            | 0.016306669 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.7        |
|    explained_variance   | 0.638       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00383     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0206     |
|    value_loss           | 0.00149     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.7     |
|    ep_rew_mean     | 0.262    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 15       |
|    time_elapsed    | 1579     |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 31000       |
| train/                  |             |
|    approx_kl            | 0.014125399 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0.795       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00276    |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0204     |
|    value_loss           | 0.00139     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.8     |
|    ep_rew_mean     | 0.288    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 16       |
|    time_elapsed    | 1684     |
|    total_timesteps | 32768    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 33000       |
| train/                  |             |
|    approx_kl            | 0.014136355 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.59       |
|    explained_variance   | 0.82        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0478     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.023      |
|    value_loss           | 0.00138     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.3     |
|    ep_rew_mean     | 0.303    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 17       |
|    time_elapsed    | 1788     |
|    total_timesteps | 34816    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.016624803 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.58       |
|    explained_variance   | 0.777       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00734    |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0209     |
|    value_loss           | 0.0018      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.1     |
|    ep_rew_mean     | 0.309    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 18       |
|    time_elapsed    | 1893     |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 37000       |
| train/                  |             |
|    approx_kl            | 0.018542603 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.821       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0468     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0219     |
|    value_loss           | 0.00111     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.5     |
|    ep_rew_mean     | 0.325    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 19       |
|    time_elapsed    | 1996     |
|    total_timesteps | 38912    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 39000       |
| train/                  |             |
|    approx_kl            | 0.013529127 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.814       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0185     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0248     |
|    value_loss           | 0.0016      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.3     |
|    ep_rew_mean     | 0.349    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 20       |
|    time_elapsed    | 2100     |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 41000       |
| train/                  |             |
|    approx_kl            | 0.014048701 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | 0.82        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0019     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0233     |
|    value_loss           | 0.00149     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 38.3     |
|    ep_rew_mean     | 0.363    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 21       |
|    time_elapsed    | 2213     |
|    total_timesteps | 43008    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 44000       |
| train/                  |             |
|    approx_kl            | 0.013509052 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.879       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0115     |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0199     |
|    value_loss           | 0.0013      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.6     |
|    ep_rew_mean     | 0.376    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 22       |
|    time_elapsed    | 2296     |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 46000       |
| train/                  |             |
|    approx_kl            | 0.015175116 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.902       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0094      |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0206     |
|    value_loss           | 0.00121     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 32.5     |
|    ep_rew_mean     | 0.386    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 23       |
|    time_elapsed    | 2379     |
|    total_timesteps | 47104    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 48000       |
| train/                  |             |
|    approx_kl            | 0.018395172 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.05       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0372     |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0214     |
|    value_loss           | 0.000859    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.6     |
|    ep_rew_mean     | 0.392    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 24       |
|    time_elapsed    | 2462     |
|    total_timesteps | 49152    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.017589778 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00392     |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0253     |
|    value_loss           | 0.00079     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.8     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 20       |
|    iterations      | 25       |
|    time_elapsed    | 2544     |
|    total_timesteps | 51200    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 4          |
|    mean_reward          | 0.4        |
| time/                   |            |
|    total_timesteps      | 52000      |
| train/                  |            |
|    approx_kl            | 0.01770015 |
|    clip_fraction        | 0.218      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.8       |
|    explained_variance   | 0.893      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0298    |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.0175    |
|    value_loss           | 0.00117    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.7     |
|    ep_rew_mean     | 0.401    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 26       |
|    time_elapsed    | 2625     |
|    total_timesteps | 53248    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 54000       |
| train/                  |             |
|    approx_kl            | 0.021573849 |
|    clip_fraction        | 0.295       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.54       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0261     |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0277     |
|    value_loss           | 0.000489    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 13.9     |
|    ep_rew_mean     | 0.402    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 27       |
|    time_elapsed    | 2705     |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 56000       |
| train/                  |             |
|    approx_kl            | 0.031897068 |
|    clip_fraction        | 0.429       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.1        |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0499     |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0395     |
|    value_loss           | 0.000249    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 10.9     |
|    ep_rew_mean     | 0.403    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 28       |
|    time_elapsed    | 2784     |
|    total_timesteps | 57344    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 58000       |
| train/                  |             |
|    approx_kl            | 0.025742445 |
|    clip_fraction        | 0.308       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.9        |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0573     |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0264     |
|    value_loss           | 0.000639    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 10.9     |
|    ep_rew_mean     | 0.401    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 29       |
|    time_elapsed    | 2862     |
|    total_timesteps | 59392    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.027252264 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.76       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0534     |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0274     |
|    value_loss           | 0.000201    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.45     |
|    ep_rew_mean     | 0.403    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 30       |
|    time_elapsed    | 2940     |
|    total_timesteps | 61440    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 4          |
|    mean_reward          | 0.4        |
| time/                   |            |
|    total_timesteps      | 62000      |
| train/                  |            |
|    approx_kl            | 0.03414877 |
|    clip_fraction        | 0.288      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.51      |
|    explained_variance   | 0.965      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0643    |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.0212    |
|    value_loss           | 0.000483   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 7.99     |
|    ep_rew_mean     | 0.403    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 31       |
|    time_elapsed    | 3015     |
|    total_timesteps | 63488    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 64000       |
| train/                  |             |
|    approx_kl            | 0.072610825 |
|    clip_fraction        | 0.411       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.93       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0459     |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.0377     |
|    value_loss           | 0.000241    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.96     |
|    ep_rew_mean     | 0.401    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 32       |
|    time_elapsed    | 3089     |
|    total_timesteps | 65536    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 66000       |
| train/                  |             |
|    approx_kl            | 0.027596949 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.42       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0606     |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0269     |
|    value_loss           | 0.000121    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.36     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 21       |
|    iterations      | 33       |
|    time_elapsed    | 3160     |
|    total_timesteps | 67584    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 68000       |
| train/                  |             |
|    approx_kl            | 0.035310116 |
|    clip_fraction        | 0.393       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.817      |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.056      |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.03       |
|    value_loss           | 0.000226    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.61     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 21       |
|    iterations      | 34       |
|    time_elapsed    | 3229     |
|    total_timesteps | 69632    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 4          |
|    mean_reward          | 0.4        |
| time/                   |            |
|    total_timesteps      | 70000      |
| train/                  |            |
|    approx_kl            | 0.05541686 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.464     |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0697    |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.0323    |
|    value_loss           | 1.04e-05   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.33     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 21       |
|    iterations      | 35       |
|    time_elapsed    | 3297     |
|    total_timesteps | 71680    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 72000       |
| train/                  |             |
|    approx_kl            | 0.014169818 |
|    clip_fraction        | 0.0582      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.284      |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.034      |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.0216     |
|    value_loss           | 0.000209    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.19     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 21       |
|    iterations      | 36       |
|    time_elapsed    | 3364     |
|    total_timesteps | 73728    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 74000       |
| train/                  |             |
|    approx_kl            | 0.009843853 |
|    clip_fraction        | 0.0303      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.177      |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0194     |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 0.000181    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.12     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 22       |
|    iterations      | 37       |
|    time_elapsed    | 3432     |
|    total_timesteps | 75776    |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 4            |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 76000        |
| train/                  |              |
|    approx_kl            | 0.0023837073 |
|    clip_fraction        | 0.016        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.124       |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00122     |
|    n_updates            | 370          |
|    policy_gradient_loss | -0.00418     |
|    value_loss           | 0.000117     |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.11     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 22       |
|    iterations      | 38       |
|    time_elapsed    | 3499     |
|    total_timesteps | 77824    |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 4            |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 78000        |
| train/                  |              |
|    approx_kl            | 0.0008639224 |
|    clip_fraction        | 0.00811      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.109       |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00628     |
|    n_updates            | 380          |
|    policy_gradient_loss | -0.00213     |
|    value_loss           | 0.000227     |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.06     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 22       |
|    iterations      | 39       |
|    time_elapsed    | 3566     |
|    total_timesteps | 79872    |
---------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 4             |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 80000         |
| train/                  |               |
|    approx_kl            | 0.00024302345 |
|    clip_fraction        | 0.00186       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0927       |
|    explained_variance   | 0.992         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00175      |
|    n_updates            | 390           |
|    policy_gradient_loss | -0.00119      |
|    value_loss           | 2.25e-05      |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.06     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 22       |
|    iterations      | 40       |
|    time_elapsed    | 3633     |
|    total_timesteps | 81920    |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 4            |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 82000        |
| train/                  |              |
|    approx_kl            | 0.0032267612 |
|    clip_fraction        | 0.0107       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0852      |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0143      |
|    n_updates            | 400          |
|    policy_gradient_loss | -0.00559     |
|    value_loss           | 0.000327     |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.08     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 22       |
|    iterations      | 41       |
|    time_elapsed    | 3700     |
|    total_timesteps | 83968    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 84000       |
| train/                  |             |
|    approx_kl            | 0.016103514 |
|    clip_fraction        | 0.0146      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0684     |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0169     |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 7.3e-05     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.05     |
|    ep_rew_mean     | 0.402    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 42       |
|    time_elapsed    | 3767     |
|    total_timesteps | 86016    |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 4            |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 87000        |
| train/                  |              |
|    approx_kl            | 0.0013889838 |
|    clip_fraction        | 0.00874      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0618      |
|    explained_variance   | 0.973        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000195     |
|    n_updates            | 420          |
|    policy_gradient_loss | -0.00271     |
|    value_loss           | 0.000384     |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.13     |
|    ep_rew_mean     | 0.403    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 43       |
|    time_elapsed    | 3834     |
|    total_timesteps | 88064    |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 4            |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 89000        |
| train/                  |              |
|    approx_kl            | 0.0021353276 |
|    clip_fraction        | 0.0137       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0795      |
|    explained_variance   | 0.973        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00206     |
|    n_updates            | 430          |
|    policy_gradient_loss | -0.00182     |
|    value_loss           | 0.000347     |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.07     |
|    ep_rew_mean     | 0.401    |
| time/              |          |
|    fps             | 23       |
|    iterations      | 44       |
|    time_elapsed    | 3901     |
|    total_timesteps | 90112    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 4          |
|    mean_reward          | 0.4        |
| time/                   |            |
|    total_timesteps      | 91000      |
| train/                  |            |
|    approx_kl            | 0.04686572 |
|    clip_fraction        | 0.0778     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.148     |
|    explained_variance   | 0.997      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0793    |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.0239    |
|    value_loss           | 9.78e-06   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.76     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 23       |
|    iterations      | 45       |
|    time_elapsed    | 3971     |
|    total_timesteps | 92160    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 4          |
|    mean_reward          | 0.4        |
| time/                   |            |
|    total_timesteps      | 93000      |
| train/                  |            |
|    approx_kl            | 0.01484075 |
|    clip_fraction        | 0.366      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.41      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0863    |
|    n_updates            | 450        |
|    policy_gradient_loss | -0.0725    |
|    value_loss           | 0.000187   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.96     |
|    ep_rew_mean     | 0.398    |
| time/              |          |
|    fps             | 23       |
|    iterations      | 46       |
|    time_elapsed    | 4041     |
|    total_timesteps | 94208    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 95000       |
| train/                  |             |
|    approx_kl            | 0.035955474 |
|    clip_fraction        | 0.298       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.312      |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0576     |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.0209     |
|    value_loss           | 0.000173    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.25     |
|    ep_rew_mean     | 0.402    |
| time/              |          |
|    fps             | 23       |
|    iterations      | 47       |
|    time_elapsed    | 4109     |
|    total_timesteps | 96256    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 4          |
|    mean_reward          | 0.4        |
| time/                   |            |
|    total_timesteps      | 97000      |
| train/                  |            |
|    approx_kl            | 0.05168646 |
|    clip_fraction        | 0.179      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.122     |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0232    |
|    n_updates            | 470        |
|    policy_gradient_loss | -0.015     |
|    value_loss           | 0.000231   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.1      |
|    ep_rew_mean     | 0.402    |
| time/              |          |
|    fps             | 23       |
|    iterations      | 48       |
|    time_elapsed    | 4176     |
|    total_timesteps | 98304    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50         |
|    mean_reward          | 0.3        |
| time/                   |            |
|    total_timesteps      | 99000      |
| train/                  |            |
|    approx_kl            | 0.60361624 |
|    clip_fraction        | 0.171      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.194     |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0914    |
|    n_updates            | 480        |
|    policy_gradient_loss | -0.0497    |
|    value_loss           | 0.000118   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.3     |
|    ep_rew_mean     | 0.392    |
| time/              |          |
|    fps             | 23       |
|    iterations      | 49       |
|    time_elapsed    | 4278     |
|    total_timesteps | 100352   |
---------------------------------
{'reward': [0.30000000447034836, 0.30000000447034836, 0.30000000447034836, 0.30000000447034836, 0.30000000447034836, 0.30000000447034836, 0.30000000447034836, 0.30000000447034836, 0.30000000447034836, 0.30000000447034836], 'std': [50, 50, 50, 50, 50, 50, 50, 50, 50, 50]}
