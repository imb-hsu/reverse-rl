Logging to ../Logging/RPPO_BW_Then_FW_5_200
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 14.7                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 1                    |
|    time_elapsed    | -1683715397694830848 |
|    total_timesteps | 2048                 |
---------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 3000        |
| train/                  |             |
|    approx_kl            | 0.036615957 |
|    clip_fraction        | 0.531       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.12       |
|    explained_variance   | -5.19       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.106      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.108      |
|    value_loss           | 0.045       |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 12.1                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 2                    |
|    time_elapsed    | -1683715397694830848 |
|    total_timesteps | 4096                 |
---------------------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50         |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.20376393 |
|    clip_fraction        | 0.64       |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.07      |
|    explained_variance   | -1.45      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00584    |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0265    |
|    value_loss           | 0.00135    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 14.8                 |
|    ep_rew_mean     | 0.396                |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 3                    |
|    time_elapsed    | -1683715397694830848 |
|    total_timesteps | 6144                 |
---------------------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50         |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 7000       |
| train/                  |            |
|    approx_kl            | 0.42568064 |
|    clip_fraction        | 0.744      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.01      |
|    explained_variance   | -0.161     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0213    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0223    |
|    value_loss           | 0.000286   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 15.2                 |
|    ep_rew_mean     | 0.399                |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 4                    |
|    time_elapsed    | -1683715397694830592 |
|    total_timesteps | 8192                 |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50        |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 9000      |
| train/                  |           |
|    approx_kl            | 0.790503  |
|    clip_fraction        | 0.775     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.89     |
|    explained_variance   | -0.328    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.062    |
|    n_updates            | 80        |
|    policy_gradient_loss | -0.000127 |
|    value_loss           | 0.000512  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 12.1                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 5                    |
|    time_elapsed    | -1683715397694830592 |
|    total_timesteps | 10240                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 11000     |
| train/                  |           |
|    approx_kl            | 2.3671706 |
|    clip_fraction        | 0.902     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.6      |
|    explained_variance   | -0.525    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0722   |
|    n_updates            | 100       |
|    policy_gradient_loss | 0.0462    |
|    value_loss           | 0.000138  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 10.2                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 6                    |
|    time_elapsed    | -1683715397694830336 |
|    total_timesteps | 12288                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5        |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 13000    |
| train/                  |          |
|    approx_kl            | 5.668124 |
|    clip_fraction        | 0.939    |
|    clip_range           | 0.2      |
|    entropy_loss         | -4.2     |
|    explained_variance   | -0.526   |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0724  |
|    n_updates            | 120      |
|    policy_gradient_loss | 0.0384   |
|    value_loss           | 0.000174 |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 7.73                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 7                    |
|    time_elapsed    | -1683715397694830336 |
|    total_timesteps | 14336                |
---------------------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 5          |
|    mean_reward          | 0.4        |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 15.1688385 |
|    clip_fraction        | 0.94       |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.66      |
|    explained_variance   | -0.701     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0606     |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0398    |
|    value_loss           | 0.000554   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 7.11                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 8                    |
|    time_elapsed    | -1683715397694830336 |
|    total_timesteps | 16384                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 17000     |
| train/                  |           |
|    approx_kl            | 15.357706 |
|    clip_fraction        | 0.954     |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.98     |
|    explained_variance   | -0.714    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.112    |
|    n_updates            | 160       |
|    policy_gradient_loss | -0.0776   |
|    value_loss           | 0.00028   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 6.94                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 9                    |
|    time_elapsed    | -1683715397694830080 |
|    total_timesteps | 18432                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5        |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 19000    |
| train/                  |          |
|    approx_kl            | 23.7328  |
|    clip_fraction        | 0.965    |
|    clip_range           | 0.2      |
|    entropy_loss         | -2.43    |
|    explained_variance   | -0.786   |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0882  |
|    n_updates            | 180      |
|    policy_gradient_loss | -0.0994  |
|    value_loss           | 0.000174 |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 8.56                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 10                   |
|    time_elapsed    | -1683715397694830080 |
|    total_timesteps | 20480                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5        |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 21000    |
| train/                  |          |
|    approx_kl            | 56.24443 |
|    clip_fraction        | 0.978    |
|    clip_range           | 0.2      |
|    entropy_loss         | -1.8     |
|    explained_variance   | -0.728   |
|    learning_rate        | 0.0003   |
|    loss                 | -0.119   |
|    n_updates            | 200      |
|    policy_gradient_loss | -0.013   |
|    value_loss           | 0.000443 |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 6.57                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 11                   |
|    time_elapsed    | -1683715397694830080 |
|    total_timesteps | 22528                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5        |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 23000    |
| train/                  |          |
|    approx_kl            | 36.69717 |
|    clip_fraction        | 0.959    |
|    clip_range           | 0.2      |
|    entropy_loss         | -1.42    |
|    explained_variance   | -0.879   |
|    learning_rate        | 0.0003   |
|    loss                 | -0.133   |
|    n_updates            | 220      |
|    policy_gradient_loss | -0.116   |
|    value_loss           | 0.000123 |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 7.46                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 12                   |
|    time_elapsed    | -1683715397694830080 |
|    total_timesteps | 24576                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5        |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 25000    |
| train/                  |          |
|    approx_kl            | 37.98207 |
|    clip_fraction        | 0.931    |
|    clip_range           | 0.2      |
|    entropy_loss         | -1.16    |
|    explained_variance   | -0.8     |
|    learning_rate        | 0.0003   |
|    loss                 | -0.127   |
|    n_updates            | 240      |
|    policy_gradient_loss | -0.0842  |
|    value_loss           | 0.000513 |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 7.03                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 13                   |
|    time_elapsed    | -1683715397694829824 |
|    total_timesteps | 26624                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5        |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 27000    |
| train/                  |          |
|    approx_kl            | 53.05085 |
|    clip_fraction        | 0.885    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.979   |
|    explained_variance   | -0.922   |
|    learning_rate        | 0.0003   |
|    loss                 | -0.121   |
|    n_updates            | 260      |
|    policy_gradient_loss | -0.0133  |
|    value_loss           | 0.00057  |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 6.14                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 14                   |
|    time_elapsed    | -1683715397694829824 |
|    total_timesteps | 28672                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 29000     |
| train/                  |           |
|    approx_kl            | 30.090408 |
|    clip_fraction        | 0.835     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.848    |
|    explained_variance   | -0.892    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.125    |
|    n_updates            | 280       |
|    policy_gradient_loss | -0.118    |
|    value_loss           | 0.000298  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.29                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 15                   |
|    time_elapsed    | -1683715397694829824 |
|    total_timesteps | 30720                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 31000     |
| train/                  |           |
|    approx_kl            | 33.370575 |
|    clip_fraction        | 0.741     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.721    |
|    explained_variance   | -1.02     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.151    |
|    n_updates            | 300       |
|    policy_gradient_loss | -0.129    |
|    value_loss           | 3.82e-05  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.7                  |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 16                   |
|    time_elapsed    | -1683715397694829568 |
|    total_timesteps | 32768                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 33000     |
| train/                  |           |
|    approx_kl            | 28.414228 |
|    clip_fraction        | 0.677     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.664    |
|    explained_variance   | -0.95     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.115    |
|    n_updates            | 320       |
|    policy_gradient_loss | -0.115    |
|    value_loss           | 0.000145  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.88                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 17                   |
|    time_elapsed    | -1683715397694829568 |
|    total_timesteps | 34816                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 36.490063 |
|    clip_fraction        | 0.706     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.605    |
|    explained_variance   | -0.923    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.118    |
|    n_updates            | 340       |
|    policy_gradient_loss | -0.107    |
|    value_loss           | 0.000362  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.51                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 18                   |
|    time_elapsed    | -1683715397694829568 |
|    total_timesteps | 36864                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5        |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 37000    |
| train/                  |          |
|    approx_kl            | 65.71134 |
|    clip_fraction        | 0.697    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.659   |
|    explained_variance   | -0.918   |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0955  |
|    n_updates            | 360      |
|    policy_gradient_loss | -0.102   |
|    value_loss           | 0.000501 |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.18                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 19                   |
|    time_elapsed    | -1683715397694829312 |
|    total_timesteps | 38912                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 39000     |
| train/                  |           |
|    approx_kl            | 40.796875 |
|    clip_fraction        | 0.657     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.707    |
|    explained_variance   | -1.04     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.097    |
|    n_updates            | 380       |
|    policy_gradient_loss | -0.124    |
|    value_loss           | 0.000339  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.34                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 20                   |
|    time_elapsed    | -1683715397694829312 |
|    total_timesteps | 40960                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 41000     |
| train/                  |           |
|    approx_kl            | 45.722214 |
|    clip_fraction        | 0.665     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.681    |
|    explained_variance   | -1.08     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.148    |
|    n_updates            | 400       |
|    policy_gradient_loss | -0.128    |
|    value_loss           | 0.000295  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.09                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 21                   |
|    time_elapsed    | -1683715397694829312 |
|    total_timesteps | 43008                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5        |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 44000    |
| train/                  |          |
|    approx_kl            | 60.74133 |
|    clip_fraction        | 0.657    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.687   |
|    explained_variance   | -1.03    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.122   |
|    n_updates            | 420      |
|    policy_gradient_loss | -0.12    |
|    value_loss           | 0.000213 |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.23                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 22                   |
|    time_elapsed    | -1683715397694829312 |
|    total_timesteps | 45056                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 46000     |
| train/                  |           |
|    approx_kl            | 28.569405 |
|    clip_fraction        | 0.662     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.694    |
|    explained_variance   | -0.996    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0979   |
|    n_updates            | 440       |
|    policy_gradient_loss | -0.118    |
|    value_loss           | 0.000255  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.23                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 23                   |
|    time_elapsed    | -1683715397694829056 |
|    total_timesteps | 47104                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5        |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 48000    |
| train/                  |          |
|    approx_kl            | 45.82931 |
|    clip_fraction        | 0.689    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.68    |
|    explained_variance   | -0.887   |
|    learning_rate        | 0.0003   |
|    loss                 | -0.114   |
|    n_updates            | 460      |
|    policy_gradient_loss | -0.104   |
|    value_loss           | 0.000428 |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.57                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 24                   |
|    time_elapsed    | -1683715397694829056 |
|    total_timesteps | 49152                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 121.33504 |
|    clip_fraction        | 0.765     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.595    |
|    explained_variance   | -0.952    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0492   |
|    n_updates            | 480       |
|    policy_gradient_loss | -0.0506   |
|    value_loss           | 0.000655  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.95                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 25                   |
|    time_elapsed    | -1683715397694829056 |
|    total_timesteps | 51200                |
---------------------------------------------
{'reward': [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4], 'std': [5, 5, 5, 5, 5, 5, 5, 5, 5, 5]}
{'reward': [0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645], 'std': [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]}
