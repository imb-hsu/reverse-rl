Logging to ./Logging/TRPO_Forward_Baseline_5_400
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.0925   |
| time/              |          |
|    fps             | 20       |
|    iterations      | 1        |
|    time_elapsed    | 102      |
|    total_timesteps | 2048     |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 50       |
|    mean_reward            | 0        |
| time/                     |          |
|    total_timesteps        | 3000     |
| train/                    |          |
|    explained_variance     | -0.121   |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00687  |
|    learning_rate          | 0.001    |
|    n_updates              | 1        |
|    policy_objective       | 0.0282   |
|    value_loss             | 0.0378   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.0914   |
| time/              |          |
|    fps             | 20       |
|    iterations      | 2        |
|    time_elapsed    | 204      |
|    total_timesteps | 4096     |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 50       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 5000     |
| train/                    |          |
|    explained_variance     | -0.501   |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0057   |
|    learning_rate          | 0.001    |
|    n_updates              | 2        |
|    policy_objective       | 0.0333   |
|    value_loss             | 0.00869  |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.098    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 3        |
|    time_elapsed    | 306      |
|    total_timesteps | 6144     |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 50       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 7000     |
| train/                    |          |
|    explained_variance     | -0.669   |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00728  |
|    learning_rate          | 0.001    |
|    n_updates              | 3        |
|    policy_objective       | 0.0284   |
|    value_loss             | 0.00369  |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.8     |
|    ep_rew_mean     | 0.113    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 4        |
|    time_elapsed    | 408      |
|    total_timesteps | 8192     |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 50       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 9000     |
| train/                    |          |
|    explained_variance     | -0.353   |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00769  |
|    learning_rate          | 0.001    |
|    n_updates              | 4        |
|    policy_objective       | 0.0221   |
|    value_loss             | 0.002    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 20       |
|    iterations      | 5        |
|    time_elapsed    | 510      |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 50       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 11000    |
| train/                    |          |
|    explained_variance     | 0.259    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00779  |
|    learning_rate          | 0.001    |
|    n_updates              | 5        |
|    policy_objective       | 0.0223   |
|    value_loss             | 0.00133  |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.121    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 6        |
|    time_elapsed    | 613      |
|    total_timesteps | 12288    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 50       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 13000    |
| train/                    |          |
|    explained_variance     | 0.5      |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00811  |
|    learning_rate          | 0.001    |
|    n_updates              | 6        |
|    policy_objective       | 0.0259   |
|    value_loss             | 0.001    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.137    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 7        |
|    time_elapsed    | 715      |
|    total_timesteps | 14336    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 50       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 15000    |
| train/                    |          |
|    explained_variance     | 0.493    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00774  |
|    learning_rate          | 0.001    |
|    n_updates              | 7        |
|    policy_objective       | 0.0278   |
|    value_loss             | 0.00137  |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.145    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 8        |
|    time_elapsed    | 817      |
|    total_timesteps | 16384    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 50       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 17000    |
| train/                    |          |
|    explained_variance     | 0.469    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00695  |
|    learning_rate          | 0.001    |
|    n_updates              | 8        |
|    policy_objective       | 0.0303   |
|    value_loss             | 0.00138  |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.139    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 9        |
|    time_elapsed    | 919      |
|    total_timesteps | 18432    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 50       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 19000    |
| train/                    |          |
|    explained_variance     | 0.446    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00751  |
|    learning_rate          | 0.001    |
|    n_updates              | 9        |
|    policy_objective       | 0.0235   |
|    value_loss             | 0.00144  |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.161    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 10       |
|    time_elapsed    | 1021     |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 50       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 21000    |
| train/                    |          |
|    explained_variance     | 0.536    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00723  |
|    learning_rate          | 0.001    |
|    n_updates              | 10       |
|    policy_objective       | 0.0301   |
|    value_loss             | 0.00112  |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 20       |
|    iterations      | 11       |
|    time_elapsed    | 1123     |
|    total_timesteps | 22528    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 50       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 23000    |
| train/                    |          |
|    explained_variance     | 0.437    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0071   |
|    learning_rate          | 0.001    |
|    n_updates              | 11       |
|    policy_objective       | 0.0308   |
|    value_loss             | 0.00127  |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.1     |
|    ep_rew_mean     | 0.174    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 12       |
|    time_elapsed    | 1226     |
|    total_timesteps | 24576    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 50       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 25000    |
| train/                    |          |
|    explained_variance     | 0.423    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00698  |
|    learning_rate          | 0.001    |
|    n_updates              | 12       |
|    policy_objective       | 0.029    |
|    value_loss             | 0.00178  |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.7     |
|    ep_rew_mean     | 0.178    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 13       |
|    time_elapsed    | 1328     |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 50       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 27000    |
| train/                    |          |
|    explained_variance     | 0.566    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00719  |
|    learning_rate          | 0.001    |
|    n_updates              | 13       |
|    policy_objective       | 0.0287   |
|    value_loss             | 0.00153  |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | 0.195    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 14       |
|    time_elapsed    | 1430     |
|    total_timesteps | 28672    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 50       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 29000    |
| train/                    |          |
|    explained_variance     | 0.521    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00771  |
|    learning_rate          | 0.001    |
|    n_updates              | 14       |
|    policy_objective       | 0.0277   |
|    value_loss             | 0.0018   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | 0.206    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 15       |
|    time_elapsed    | 1532     |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 50       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 31000    |
| train/                    |          |
|    explained_variance     | 0.57     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00753  |
|    learning_rate          | 0.001    |
|    n_updates              | 15       |
|    policy_objective       | 0.0274   |
|    value_loss             | 0.00152  |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.1     |
|    ep_rew_mean     | 0.189    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 16       |
|    time_elapsed    | 1634     |
|    total_timesteps | 32768    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 50       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 33000    |
| train/                    |          |
|    explained_variance     | 0.549    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00763  |
|    learning_rate          | 0.001    |
|    n_updates              | 16       |
|    policy_objective       | 0.0294   |
|    value_loss             | 0.00181  |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.8     |
|    ep_rew_mean     | 0.205    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 17       |
|    time_elapsed    | 1736     |
|    total_timesteps | 34816    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 50       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 35000    |
| train/                    |          |
|    explained_variance     | 0.622    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00683  |
|    learning_rate          | 0.001    |
|    n_updates              | 17       |
|    policy_objective       | 0.0287   |
|    value_loss             | 0.00151  |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.3     |
|    ep_rew_mean     | 0.228    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 18       |
|    time_elapsed    | 1838     |
|    total_timesteps | 36864    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 50       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 37000    |
| train/                    |          |
|    explained_variance     | 0.659    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00675  |
|    learning_rate          | 0.001    |
|    n_updates              | 18       |
|    policy_objective       | 0.0323   |
|    value_loss             | 0.0016   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.1     |
|    ep_rew_mean     | 0.253    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 19       |
|    time_elapsed    | 1940     |
|    total_timesteps | 38912    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 50       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 39000    |
| train/                    |          |
|    explained_variance     | 0.549    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00766  |
|    learning_rate          | 0.001    |
|    n_updates              | 19       |
|    policy_objective       | 0.0296   |
|    value_loss             | 0.00234  |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.7     |
|    ep_rew_mean     | 0.261    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 20       |
|    time_elapsed    | 2042     |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 50       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 41000    |
| train/                    |          |
|    explained_variance     | 0.569    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00729  |
|    learning_rate          | 0.001    |
|    n_updates              | 20       |
|    policy_objective       | 0.0282   |
|    value_loss             | 0.00225  |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.6     |
|    ep_rew_mean     | 0.282    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 21       |
|    time_elapsed    | 2153     |
|    total_timesteps | 43008    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 50       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 44000    |
| train/                    |          |
|    explained_variance     | 0.663    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00756  |
|    learning_rate          | 0.001    |
|    n_updates              | 21       |
|    policy_objective       | 0.0284   |
|    value_loss             | 0.00242  |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47       |
|    ep_rew_mean     | 0.294    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 22       |
|    time_elapsed    | 2255     |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 50       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 46000    |
| train/                    |          |
|    explained_variance     | 0.769    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00677  |
|    learning_rate          | 0.001    |
|    n_updates              | 22       |
|    policy_objective       | 0.0271   |
|    value_loss             | 0.00165  |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.3     |
|    ep_rew_mean     | 0.305    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 23       |
|    time_elapsed    | 2357     |
|    total_timesteps | 47104    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 50       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 48000    |
| train/                    |          |
|    explained_variance     | 0.687    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00751  |
|    learning_rate          | 0.001    |
|    n_updates              | 23       |
|    policy_objective       | 0.0282   |
|    value_loss             | 0.00232  |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44       |
|    ep_rew_mean     | 0.318    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 24       |
|    time_elapsed    | 2458     |
|    total_timesteps | 49152    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 50       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 50000    |
| train/                    |          |
|    explained_variance     | 0.645    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00774  |
|    learning_rate          | 0.001    |
|    n_updates              | 24       |
|    policy_objective       | 0.0304   |
|    value_loss             | 0.00249  |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 0.34     |
| time/              |          |
|    fps             | 20       |
|    iterations      | 25       |
|    time_elapsed    | 2559     |
|    total_timesteps | 51200    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 50       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 52000    |
| train/                    |          |
|    explained_variance     | 0.718    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0075   |
|    learning_rate          | 0.001    |
|    n_updates              | 25       |
|    policy_objective       | 0.0299   |
|    value_loss             | 0.00223  |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 39.8     |
|    ep_rew_mean     | 0.344    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 26       |
|    time_elapsed    | 2660     |
|    total_timesteps | 53248    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 50       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 54000    |
| train/                    |          |
|    explained_variance     | 0.693    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00741  |
|    learning_rate          | 0.001    |
|    n_updates              | 26       |
|    policy_objective       | 0.0247   |
|    value_loss             | 0.00208  |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.9     |
|    ep_rew_mean     | 0.357    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 27       |
|    time_elapsed    | 2761     |
|    total_timesteps | 55296    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 50       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 56000    |
| train/                    |          |
|    explained_variance     | 0.621    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00758  |
|    learning_rate          | 0.001    |
|    n_updates              | 27       |
|    policy_objective       | 0.0299   |
|    value_loss             | 0.00245  |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 0.373    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 28       |
|    time_elapsed    | 2861     |
|    total_timesteps | 57344    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 50       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 58000    |
| train/                    |          |
|    explained_variance     | 0.586    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00769  |
|    learning_rate          | 0.001    |
|    n_updates              | 28       |
|    policy_objective       | 0.0306   |
|    value_loss             | 0.00293  |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 32.3     |
|    ep_rew_mean     | 0.379    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 29       |
|    time_elapsed    | 2961     |
|    total_timesteps | 59392    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 50       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 60000    |
| train/                    |          |
|    explained_variance     | 0.807    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00759  |
|    learning_rate          | 0.001    |
|    n_updates              | 29       |
|    policy_objective       | 0.0294   |
|    value_loss             | 0.00129  |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.4     |
|    ep_rew_mean     | 0.396    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 30       |
|    time_elapsed    | 3061     |
|    total_timesteps | 61440    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 50       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 62000    |
| train/                    |          |
|    explained_variance     | 0.877    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00769  |
|    learning_rate          | 0.001    |
|    n_updates              | 30       |
|    policy_objective       | 0.0323   |
|    value_loss             | 0.00088  |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.1     |
|    ep_rew_mean     | 0.394    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 31       |
|    time_elapsed    | 3160     |
|    total_timesteps | 63488    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 50       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 64000    |
| train/                    |          |
|    explained_variance     | 0.845    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00708  |
|    learning_rate          | 0.001    |
|    n_updates              | 31       |
|    policy_objective       | 0.0299   |
|    value_loss             | 0.00108  |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.4     |
|    ep_rew_mean     | 0.399    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 32       |
|    time_elapsed    | 3259     |
|    total_timesteps | 65536    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 50       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 66000    |
| train/                    |          |
|    explained_variance     | 0.952    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00785  |
|    learning_rate          | 0.001    |
|    n_updates              | 32       |
|    policy_objective       | 0.0398   |
|    value_loss             | 0.000302 |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.3     |
|    ep_rew_mean     | 0.399    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 33       |
|    time_elapsed    | 3358     |
|    total_timesteps | 67584    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 50       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 68000    |
| train/                    |          |
|    explained_variance     | 0.905    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00743  |
|    learning_rate          | 0.001    |
|    n_updates              | 33       |
|    policy_objective       | 0.029    |
|    value_loss             | 0.000782 |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.6     |
|    ep_rew_mean     | 0.401    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 34       |
|    time_elapsed    | 3457     |
|    total_timesteps | 69632    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 50       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 70000    |
| train/                    |          |
|    explained_variance     | 0.966    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00789  |
|    learning_rate          | 0.001    |
|    n_updates              | 34       |
|    policy_objective       | 0.0358   |
|    value_loss             | 0.000229 |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.3     |
|    ep_rew_mean     | 0.404    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 35       |
|    time_elapsed    | 3555     |
|    total_timesteps | 71680    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 50       |
|    mean_reward            | 0.1      |
| time/                     |          |
|    total_timesteps        | 72000    |
| train/                    |          |
|    explained_variance     | 0.956    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00876  |
|    learning_rate          | 0.001    |
|    n_updates              | 35       |
|    policy_objective       | 0.0289   |
|    value_loss             | 0.000417 |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | 0.402    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 36       |
|    time_elapsed    | 3653     |
|    total_timesteps | 73728    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 4        |
|    mean_reward            | 0.4      |
| time/                     |          |
|    total_timesteps        | 74000    |
| train/                    |          |
|    explained_variance     | 0.956    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00838  |
|    learning_rate          | 0.001    |
|    n_updates              | 36       |
|    policy_objective       | 0.0281   |
|    value_loss             | 0.00047  |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.4     |
|    ep_rew_mean     | 0.403    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 37       |
|    time_elapsed    | 3731     |
|    total_timesteps | 75776    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 4        |
|    mean_reward            | 0.4      |
| time/                     |          |
|    total_timesteps        | 76000    |
| train/                    |          |
|    explained_variance     | 0.97     |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00893  |
|    learning_rate          | 0.001    |
|    n_updates              | 37       |
|    policy_objective       | 0.0329   |
|    value_loss             | 0.000283 |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.8     |
|    ep_rew_mean     | 0.402    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 38       |
|    time_elapsed    | 3809     |
|    total_timesteps | 77824    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 4        |
|    mean_reward            | 0.4      |
| time/                     |          |
|    total_timesteps        | 78000    |
| train/                    |          |
|    explained_variance     | 0.965    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00858  |
|    learning_rate          | 0.001    |
|    n_updates              | 38       |
|    policy_objective       | 0.0326   |
|    value_loss             | 0.000387 |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 10.9     |
|    ep_rew_mean     | 0.403    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 39       |
|    time_elapsed    | 3886     |
|    total_timesteps | 79872    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 4        |
|    mean_reward            | 0.4      |
| time/                     |          |
|    total_timesteps        | 80000    |
| train/                    |          |
|    explained_variance     | 0.953    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0082   |
|    learning_rate          | 0.001    |
|    n_updates              | 39       |
|    policy_objective       | 0.0222   |
|    value_loss             | 0.000529 |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 11.3     |
|    ep_rew_mean     | 0.401    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 40       |
|    time_elapsed    | 3963     |
|    total_timesteps | 81920    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 4        |
|    mean_reward            | 0.4      |
| time/                     |          |
|    total_timesteps        | 82000    |
| train/                    |          |
|    explained_variance     | 0.971    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00801  |
|    learning_rate          | 0.001    |
|    n_updates              | 40       |
|    policy_objective       | 0.0262   |
|    value_loss             | 8.01e-05 |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 11.9     |
|    ep_rew_mean     | 0.403    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 41       |
|    time_elapsed    | 4040     |
|    total_timesteps | 83968    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 4        |
|    mean_reward            | 0.4      |
| time/                     |          |
|    total_timesteps        | 84000    |
| train/                    |          |
|    explained_variance     | 0.976    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0082   |
|    learning_rate          | 0.001    |
|    n_updates              | 41       |
|    policy_objective       | 0.0262   |
|    value_loss             | 0.000309 |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 10.1     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 20       |
|    iterations      | 42       |
|    time_elapsed    | 4117     |
|    total_timesteps | 86016    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 4        |
|    mean_reward            | 0.4      |
| time/                     |          |
|    total_timesteps        | 87000    |
| train/                    |          |
|    explained_variance     | 0.982    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00847  |
|    learning_rate          | 0.001    |
|    n_updates              | 42       |
|    policy_objective       | 0.0331   |
|    value_loss             | 0.000214 |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.24     |
|    ep_rew_mean     | 0.401    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 43       |
|    time_elapsed    | 4193     |
|    total_timesteps | 88064    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 4        |
|    mean_reward            | 0.4      |
| time/                     |          |
|    total_timesteps        | 89000    |
| train/                    |          |
|    explained_variance     | 0.994    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0077   |
|    learning_rate          | 0.001    |
|    n_updates              | 43       |
|    policy_objective       | 0.0328   |
|    value_loss             | 3.84e-05 |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 8.09     |
|    ep_rew_mean     | 0.401    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 44       |
|    time_elapsed    | 4267     |
|    total_timesteps | 90112    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 4        |
|    mean_reward            | 0.4      |
| time/                     |          |
|    total_timesteps        | 91000    |
| train/                    |          |
|    explained_variance     | 0.976    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00689  |
|    learning_rate          | 0.001    |
|    n_updates              | 44       |
|    policy_objective       | 0.0256   |
|    value_loss             | 0.000356 |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 7.88     |
|    ep_rew_mean     | 0.403    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 45       |
|    time_elapsed    | 4341     |
|    total_timesteps | 92160    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 4        |
|    mean_reward            | 0.4      |
| time/                     |          |
|    total_timesteps        | 93000    |
| train/                    |          |
|    explained_variance     | 0.981    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00842  |
|    learning_rate          | 0.001    |
|    n_updates              | 45       |
|    policy_objective       | 0.0234   |
|    value_loss             | 0.000271 |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 7.41     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 21       |
|    iterations      | 46       |
|    time_elapsed    | 4416     |
|    total_timesteps | 94208    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 4        |
|    mean_reward            | 0.4      |
| time/                     |          |
|    total_timesteps        | 95000    |
| train/                    |          |
|    explained_variance     | 0.994    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.00848  |
|    learning_rate          | 0.001    |
|    n_updates              | 46       |
|    policy_objective       | 0.0345   |
|    value_loss             | 8.18e-05 |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 6.78     |
|    ep_rew_mean     | 0.401    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 47       |
|    time_elapsed    | 4489     |
|    total_timesteps | 96256    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 4        |
|    mean_reward            | 0.4      |
| time/                     |          |
|    total_timesteps        | 97000    |
| train/                    |          |
|    explained_variance     | 0.977    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0079   |
|    learning_rate          | 0.001    |
|    n_updates              | 47       |
|    policy_objective       | 0.0311   |
|    value_loss             | 0.000322 |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 7.05     |
|    ep_rew_mean     | 0.403    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 48       |
|    time_elapsed    | 4562     |
|    total_timesteps | 98304    |
---------------------------------
----------------------------------------
| eval/                     |          |
|    mean_ep_length         | 4        |
|    mean_reward            | 0.4      |
| time/                     |          |
|    total_timesteps        | 99000    |
| train/                    |          |
|    explained_variance     | 0.985    |
|    is_line_search_success | 1        |
|    kl_divergence_loss     | 0.0077   |
|    learning_rate          | 0.001    |
|    n_updates              | 48       |
|    policy_objective       | 0.0177   |
|    value_loss             | 0.000191 |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 6.76     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 21       |
|    iterations      | 49       |
|    time_elapsed    | 4634     |
|    total_timesteps | 100352   |
---------------------------------
