Logging to ./Logging/PPO_Forward_Baseline_5_500
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.09     |
| time/              |          |
|    fps             | 20       |
|    iterations      | 1        |
|    time_elapsed    | 102      |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 3000        |
| train/                  |             |
|    approx_kl            | 0.018744634 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.15       |
|    explained_variance   | -0.161      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00626     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.0437      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.0877   |
| time/              |          |
|    fps             | 19       |
|    iterations      | 2        |
|    time_elapsed    | 205      |
|    total_timesteps | 4096     |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.016757213 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.13       |
|    explained_variance   | -0.113      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0338     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 0.0124      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.097    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 3        |
|    time_elapsed    | 308      |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 7000        |
| train/                  |             |
|    approx_kl            | 0.016453046 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.1        |
|    explained_variance   | -0.405      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000698    |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.0071      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.104    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 4        |
|    time_elapsed    | 411      |
|    total_timesteps | 8192     |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 9000        |
| train/                  |             |
|    approx_kl            | 0.016361773 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.06       |
|    explained_variance   | -0.624      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0021      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0193     |
|    value_loss           | 0.00243     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.124    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 5        |
|    time_elapsed    | 514      |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 11000       |
| train/                  |             |
|    approx_kl            | 0.013708441 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.05       |
|    explained_variance   | -0.0562     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0329     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0187     |
|    value_loss           | 0.00159     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.143    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 6        |
|    time_elapsed    | 618      |
|    total_timesteps | 12288    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50         |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 13000      |
| train/                  |            |
|    approx_kl            | 0.01500237 |
|    clip_fraction        | 0.158      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5         |
|    explained_variance   | 0.414      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0372    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0166    |
|    value_loss           | 0.00118    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.155    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 7        |
|    time_elapsed    | 721      |
|    total_timesteps | 14336    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.011375154 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.97       |
|    explained_variance   | 0.412       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0235     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 0.00166     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.163    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 8        |
|    time_elapsed    | 824      |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 17000       |
| train/                  |             |
|    approx_kl            | 0.016401313 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.96       |
|    explained_variance   | 0.392       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00341    |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0201     |
|    value_loss           | 0.00141     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.167    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 9        |
|    time_elapsed    | 927      |
|    total_timesteps | 18432    |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | 0.1          |
| time/                   |              |
|    total_timesteps      | 19000        |
| train/                  |              |
|    approx_kl            | 0.0123658255 |
|    clip_fraction        | 0.135        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.93        |
|    explained_variance   | 0.501        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00746      |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.0146      |
|    value_loss           | 0.00128      |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.178    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 10       |
|    time_elapsed    | 1030     |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50         |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 21000      |
| train/                  |            |
|    approx_kl            | 0.01243134 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.89      |
|    explained_variance   | 0.456      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00654   |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0189    |
|    value_loss           | 0.00169    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.189    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 11       |
|    time_elapsed    | 1134     |
|    total_timesteps | 22528    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 23000       |
| train/                  |             |
|    approx_kl            | 0.013032309 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.86       |
|    explained_variance   | 0.474       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0412     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0185     |
|    value_loss           | 0.00153     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | 0.204    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 12       |
|    time_elapsed    | 1237     |
|    total_timesteps | 24576    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.016411858 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.82       |
|    explained_variance   | 0.424       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00318    |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0203     |
|    value_loss           | 0.00196     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | 0.219    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 13       |
|    time_elapsed    | 1340     |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50         |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 27000      |
| train/                  |            |
|    approx_kl            | 0.01777532 |
|    clip_fraction        | 0.2        |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.75      |
|    explained_variance   | 0.533      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0539    |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0226    |
|    value_loss           | 0.00175    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | 0.219    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 14       |
|    time_elapsed    | 1443     |
|    total_timesteps | 28672    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 29000       |
| train/                  |             |
|    approx_kl            | 0.014433819 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.71       |
|    explained_variance   | 0.558       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00614    |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0205     |
|    value_loss           | 0.00165     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.1     |
|    ep_rew_mean     | 0.215    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 15       |
|    time_elapsed    | 1546     |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50         |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 31000      |
| train/                  |            |
|    approx_kl            | 0.01495631 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.67      |
|    explained_variance   | 0.582      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.05      |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 0.00148    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.7     |
|    ep_rew_mean     | 0.252    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 16       |
|    time_elapsed    | 1649     |
|    total_timesteps | 32768    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 33000       |
| train/                  |             |
|    approx_kl            | 0.013993659 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.59       |
|    explained_variance   | 0.562       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0267     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0215     |
|    value_loss           | 0.00218     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.1     |
|    ep_rew_mean     | 0.269    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 17       |
|    time_elapsed    | 1752     |
|    total_timesteps | 34816    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.014531584 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.53       |
|    explained_variance   | 0.64        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0211      |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0188     |
|    value_loss           | 0.00183     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.1     |
|    ep_rew_mean     | 0.274    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 18       |
|    time_elapsed    | 1836     |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 37000       |
| train/                  |             |
|    approx_kl            | 0.014618873 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.686       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0211     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0216     |
|    value_loss           | 0.00166     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 43.7     |
|    ep_rew_mean     | 0.312    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 19       |
|    time_elapsed    | 1919     |
|    total_timesteps | 38912    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 39000       |
| train/                  |             |
|    approx_kl            | 0.017191544 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.42       |
|    explained_variance   | 0.645       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0446     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0229     |
|    value_loss           | 0.00225     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.4     |
|    ep_rew_mean     | 0.335    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 20       |
|    time_elapsed    | 2002     |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 4          |
|    mean_reward          | 0.4        |
| time/                   |            |
|    total_timesteps      | 41000      |
| train/                  |            |
|    approx_kl            | 0.01602204 |
|    clip_fraction        | 0.207      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.33      |
|    explained_variance   | 0.736      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0169    |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0246    |
|    value_loss           | 0.0017     |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 39.2     |
|    ep_rew_mean     | 0.347    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 21       |
|    time_elapsed    | 2086     |
|    total_timesteps | 43008    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 44000       |
| train/                  |             |
|    approx_kl            | 0.013723845 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.795       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0197     |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0241     |
|    value_loss           | 0.00153     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 0.371    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 22       |
|    time_elapsed    | 2169     |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 46000       |
| train/                  |             |
|    approx_kl            | 0.015521784 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.835       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0532     |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0214     |
|    value_loss           | 0.00136     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29       |
|    ep_rew_mean     | 0.381    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 23       |
|    time_elapsed    | 2251     |
|    total_timesteps | 47104    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 48000       |
| train/                  |             |
|    approx_kl            | 0.018909536 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.851       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0532     |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0313     |
|    value_loss           | 0.00136     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26       |
|    ep_rew_mean     | 0.396    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 24       |
|    time_elapsed    | 2332     |
|    total_timesteps | 49152    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.018783543 |
|    clip_fraction        | 0.27        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.86       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0402     |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.025      |
|    value_loss           | 0.000804    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.8     |
|    ep_rew_mean     | 0.399    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 25       |
|    time_elapsed    | 2414     |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 52000       |
| train/                  |             |
|    approx_kl            | 0.024308674 |
|    clip_fraction        | 0.3         |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.63       |
|    explained_variance   | 0.845       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0408     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0222     |
|    value_loss           | 0.00135     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | 0.403    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 26       |
|    time_elapsed    | 2493     |
|    total_timesteps | 53248    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 54000       |
| train/                  |             |
|    approx_kl            | 0.023664284 |
|    clip_fraction        | 0.366       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.34       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0173     |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0325     |
|    value_loss           | 0.000433    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.5     |
|    ep_rew_mean     | 0.403    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 27       |
|    time_elapsed    | 2573     |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 56000       |
| train/                  |             |
|    approx_kl            | 0.025280926 |
|    clip_fraction        | 0.417       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.12       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0628     |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0493     |
|    value_loss           | 0.000184    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 11.7     |
|    ep_rew_mean     | 0.403    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 28       |
|    time_elapsed    | 2650     |
|    total_timesteps | 57344    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 58000       |
| train/                  |             |
|    approx_kl            | 0.021109538 |
|    clip_fraction        | 0.342       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.75       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0185     |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.038      |
|    value_loss           | 0.000283    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.52     |
|    ep_rew_mean     | 0.403    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 29       |
|    time_elapsed    | 2727     |
|    total_timesteps | 59392    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.037906334 |
|    clip_fraction        | 0.473       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.42       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0158      |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0382     |
|    value_loss           | 0.000543    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 8.2      |
|    ep_rew_mean     | 0.401    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 30       |
|    time_elapsed    | 2802     |
|    total_timesteps | 61440    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 4          |
|    mean_reward          | 0.4        |
| time/                   |            |
|    total_timesteps      | 62000      |
| train/                  |            |
|    approx_kl            | 0.06719081 |
|    clip_fraction        | 0.346      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.19      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00277   |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.0377    |
|    value_loss           | 8.45e-05   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 8.49     |
|    ep_rew_mean     | 0.403    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 31       |
|    time_elapsed    | 2877     |
|    total_timesteps | 63488    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 4          |
|    mean_reward          | 0.4        |
| time/                   |            |
|    total_timesteps      | 64000      |
| train/                  |            |
|    approx_kl            | 0.22869352 |
|    clip_fraction        | 0.591      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.52      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0475    |
|    n_updates            | 310        |
|    policy_gradient_loss | -0.0661    |
|    value_loss           | 0.000257   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.98     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 22       |
|    iterations      | 32       |
|    time_elapsed    | 2947     |
|    total_timesteps | 65536    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 66000       |
| train/                  |             |
|    approx_kl            | 0.027822457 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.879      |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0511     |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0399     |
|    value_loss           | 6.31e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.69     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 22       |
|    iterations      | 33       |
|    time_elapsed    | 3016     |
|    total_timesteps | 67584    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 68000       |
| train/                  |             |
|    approx_kl            | 0.038450394 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.532      |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0195     |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.0411     |
|    value_loss           | 1.22e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.3      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 22       |
|    iterations      | 34       |
|    time_elapsed    | 3083     |
|    total_timesteps | 69632    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.035776854 |
|    clip_fraction        | 0.0605      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.34       |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0457     |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.02       |
|    value_loss           | 0.000104    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.18     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 22       |
|    iterations      | 35       |
|    time_elapsed    | 3150     |
|    total_timesteps | 71680    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 72000       |
| train/                  |             |
|    approx_kl            | 0.027727075 |
|    clip_fraction        | 0.0634      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.293      |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0729     |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 1.12e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.4      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 22       |
|    iterations      | 36       |
|    time_elapsed    | 3217     |
|    total_timesteps | 73728    |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 4            |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 74000        |
| train/                  |              |
|    approx_kl            | 0.0046244585 |
|    clip_fraction        | 0.0635       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.32        |
|    explained_variance   | 1            |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0272      |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.032       |
|    value_loss           | 2.76e-06     |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.39     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 23       |
|    iterations      | 37       |
|    time_elapsed    | 3285     |
|    total_timesteps | 75776    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 76000       |
| train/                  |             |
|    approx_kl            | 0.059080977 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.161      |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0633     |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.0602     |
|    value_loss           | 6.5e-05     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.04     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 23       |
|    iterations      | 38       |
|    time_elapsed    | 3351     |
|    total_timesteps | 77824    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 4          |
|    mean_reward          | 0.4        |
| time/                   |            |
|    total_timesteps      | 78000      |
| train/                  |            |
|    approx_kl            | 0.02454844 |
|    clip_fraction        | 0.0986     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.194     |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0745    |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.00696   |
|    value_loss           | 0.000204   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.44     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 23       |
|    iterations      | 39       |
|    time_elapsed    | 3418     |
|    total_timesteps | 79872    |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 4            |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 80000        |
| train/                  |              |
|    approx_kl            | 0.0030249804 |
|    clip_fraction        | 0.0701       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.226       |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0273      |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.0172      |
|    value_loss           | 0.000118     |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.29     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 23       |
|    iterations      | 40       |
|    time_elapsed    | 3486     |
|    total_timesteps | 81920    |
---------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 4         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 82000     |
| train/                  |           |
|    approx_kl            | 0.1046706 |
|    clip_fraction        | 0.174     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.116    |
|    explained_variance   | 0.984     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0323   |
|    n_updates            | 400       |
|    policy_gradient_loss | -0.0248   |
|    value_loss           | 0.000205  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.07     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 23       |
|    iterations      | 41       |
|    time_elapsed    | 3552     |
|    total_timesteps | 83968    |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 4            |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 84000        |
| train/                  |              |
|    approx_kl            | 0.0003253525 |
|    clip_fraction        | 0.00483      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0955      |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00424     |
|    n_updates            | 410          |
|    policy_gradient_loss | -0.00242     |
|    value_loss           | 0.000181     |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.06     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 23       |
|    iterations      | 42       |
|    time_elapsed    | 3619     |
|    total_timesteps | 86016    |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 4            |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 87000        |
| train/                  |              |
|    approx_kl            | 0.0014062885 |
|    clip_fraction        | 0.0082       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0964      |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0105      |
|    n_updates            | 420          |
|    policy_gradient_loss | -0.00469     |
|    value_loss           | 0.000181     |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.14     |
|    ep_rew_mean     | 0.403    |
| time/              |          |
|    fps             | 23       |
|    iterations      | 43       |
|    time_elapsed    | 3685     |
|    total_timesteps | 88064    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 89000       |
| train/                  |             |
|    approx_kl            | 0.001635656 |
|    clip_fraction        | 0.00776     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.09       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000267   |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.00242    |
|    value_loss           | 0.000218    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.08     |
|    ep_rew_mean     | 0.403    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 44       |
|    time_elapsed    | 3751     |
|    total_timesteps | 90112    |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 4            |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 91000        |
| train/                  |              |
|    approx_kl            | 0.0033652934 |
|    clip_fraction        | 0.00679      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0703      |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00864     |
|    n_updates            | 440          |
|    policy_gradient_loss | -0.00475     |
|    value_loss           | 0.000334     |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.1      |
|    ep_rew_mean     | 0.402    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 45       |
|    time_elapsed    | 3817     |
|    total_timesteps | 92160    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 93000       |
| train/                  |             |
|    approx_kl            | 0.026737414 |
|    clip_fraction        | 0.0614      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.114      |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0707     |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.0193     |
|    value_loss           | 0.000127    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.65     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 24       |
|    iterations      | 46       |
|    time_elapsed    | 3886     |
|    total_timesteps | 94208    |
---------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50        |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 95000     |
| train/                  |           |
|    approx_kl            | 0.5116881 |
|    clip_fraction        | 0.36      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.337    |
|    explained_variance   | 0.993     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0212   |
|    n_updates            | 460       |
|    policy_gradient_loss | -0.0617   |
|    value_loss           | 6.56e-05  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.3     |
|    ep_rew_mean     | 0.392    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 47       |
|    time_elapsed    | 3986     |
|    total_timesteps | 96256    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50         |
|    mean_reward          | 0.3        |
| time/                   |            |
|    total_timesteps      | 97000      |
| train/                  |            |
|    approx_kl            | 0.04906472 |
|    clip_fraction        | 0.182      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.17      |
|    explained_variance   | 0.846      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.000964   |
|    n_updates            | 470        |
|    policy_gradient_loss | -0.00701   |
|    value_loss           | 0.00118    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.3     |
|    ep_rew_mean     | 0.395    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 48       |
|    time_elapsed    | 4087     |
|    total_timesteps | 98304    |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | 0.3          |
| time/                   |              |
|    total_timesteps      | 99000        |
| train/                  |              |
|    approx_kl            | 0.0042651608 |
|    clip_fraction        | 0.0871       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.25        |
|    explained_variance   | 0.831        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0217      |
|    n_updates            | 480          |
|    policy_gradient_loss | -0.00254     |
|    value_loss           | 0.00129      |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.5     |
|    ep_rew_mean     | 0.401    |
| time/              |          |
|    fps             | 23       |
|    iterations      | 49       |
|    time_elapsed    | 4186     |
|    total_timesteps | 100352   |
---------------------------------
