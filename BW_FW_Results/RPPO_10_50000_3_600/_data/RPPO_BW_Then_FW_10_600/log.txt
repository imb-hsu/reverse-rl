Logging to ../Logging/RPPO_BW_Then_FW_10_600
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 52.9                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 1                    |
|    time_elapsed    | -1683717611046681088 |
|    total_timesteps | 2048                 |
---------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 3000        |
| train/                  |             |
|    approx_kl            | 0.022224363 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.53       |
|    explained_variance   | -1.88       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0729     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.067      |
|    value_loss           | 0.0378      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 50.2                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 2                    |
|    time_elapsed    | -1683717611046680832 |
|    total_timesteps | 4096                 |
---------------------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.09723149 |
|    clip_fraction        | 0.536      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.48      |
|    explained_variance   | -0.231     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0952    |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0502    |
|    value_loss           | 0.00809    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 48.9                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 3                    |
|    time_elapsed    | -1683717611046680832 |
|    total_timesteps | 6144                 |
---------------------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 7000       |
| train/                  |            |
|    approx_kl            | 0.17141154 |
|    clip_fraction        | 0.671      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.47      |
|    explained_variance   | 0.31       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0457    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0194    |
|    value_loss           | 0.00587    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 46.2                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 4                    |
|    time_elapsed    | -1683717611046680576 |
|    total_timesteps | 8192                 |
---------------------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 9000       |
| train/                  |            |
|    approx_kl            | 0.29689452 |
|    clip_fraction        | 0.741      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.41      |
|    explained_variance   | 0.49       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0534    |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0385    |
|    value_loss           | 0.00477    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 38.3                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 5                    |
|    time_elapsed    | -1683717611046680576 |
|    total_timesteps | 10240                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 1e+03    |
|    mean_reward          | 0        |
| time/                   |          |
|    total_timesteps      | 11000    |
| train/                  |          |
|    approx_kl            | 0.744982 |
|    clip_fraction        | 0.833    |
|    clip_range           | 0.2      |
|    entropy_loss         | -6.25    |
|    explained_variance   | 0.592    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.045   |
|    n_updates            | 100      |
|    policy_gradient_loss | -0.0471  |
|    value_loss           | 0.00163  |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 34.1                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 6                    |
|    time_elapsed    | -1683717611046680320 |
|    total_timesteps | 12288                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+03     |
|    mean_reward          | 0         |
| time/                   |           |
|    total_timesteps      | 13000     |
| train/                  |           |
|    approx_kl            | 1.0962713 |
|    clip_fraction        | 0.871     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.17     |
|    explained_variance   | 0.548     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.00291  |
|    n_updates            | 120       |
|    policy_gradient_loss | -0.00983  |
|    value_loss           | 0.00462   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 27.9                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 7                    |
|    time_elapsed    | -1683717611046680320 |
|    total_timesteps | 14336                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+03     |
|    mean_reward          | 0         |
| time/                   |           |
|    total_timesteps      | 15000     |
| train/                  |           |
|    approx_kl            | 1.8020349 |
|    clip_fraction        | 0.912     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.07     |
|    explained_variance   | 0.457     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0554   |
|    n_updates            | 140       |
|    policy_gradient_loss | -0.0442   |
|    value_loss           | 0.00448   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 25.1                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 8                    |
|    time_elapsed    | -1683717611046680320 |
|    total_timesteps | 16384                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+03     |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 17000     |
| train/                  |           |
|    approx_kl            | 2.4822335 |
|    clip_fraction        | 0.927     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.87     |
|    explained_variance   | 0.418     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0273   |
|    n_updates            | 160       |
|    policy_gradient_loss | -0.022    |
|    value_loss           | 0.00204   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 22.6                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 9                    |
|    time_elapsed    | -1683717611046680064 |
|    total_timesteps | 18432                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 1e+03    |
|    mean_reward          | 0        |
| time/                   |          |
|    total_timesteps      | 19000    |
| train/                  |          |
|    approx_kl            | 4.150745 |
|    clip_fraction        | 0.936    |
|    clip_range           | 0.2      |
|    entropy_loss         | -5.81    |
|    explained_variance   | 0.39     |
|    learning_rate        | 0.0003   |
|    loss                 | -0.063   |
|    n_updates            | 180      |
|    policy_gradient_loss | -0.0448  |
|    value_loss           | 0.000484 |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 20.6                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 10                   |
|    time_elapsed    | -1683717611046680064 |
|    total_timesteps | 20480                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+03     |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 21000     |
| train/                  |           |
|    approx_kl            | 6.1822367 |
|    clip_fraction        | 0.947     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.66     |
|    explained_variance   | 0.364     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.00288   |
|    n_updates            | 200       |
|    policy_gradient_loss | -0.0313   |
|    value_loss           | 0.00133   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 17.5                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 11                   |
|    time_elapsed    | -1683717611046679808 |
|    total_timesteps | 22528                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 1e+03    |
|    mean_reward          | 0        |
| time/                   |          |
|    total_timesteps      | 23000    |
| train/                  |          |
|    approx_kl            | 9.923145 |
|    clip_fraction        | 0.95     |
|    clip_range           | 0.2      |
|    entropy_loss         | -5.46    |
|    explained_variance   | 0.273    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0783  |
|    n_updates            | 220      |
|    policy_gradient_loss | -0.0601  |
|    value_loss           | 0.000313 |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 17.3                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 12                   |
|    time_elapsed    | -1683717611046679808 |
|    total_timesteps | 24576                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 25000     |
| train/                  |           |
|    approx_kl            | 19.762833 |
|    clip_fraction        | 0.953     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.08     |
|    explained_variance   | 0.258     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0962   |
|    n_updates            | 240       |
|    policy_gradient_loss | -0.0523   |
|    value_loss           | 0.00234   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 14.1                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 13                   |
|    time_elapsed    | -1683717611046679808 |
|    total_timesteps | 26624                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 27000     |
| train/                  |           |
|    approx_kl            | 59.625553 |
|    clip_fraction        | 0.952     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.66     |
|    explained_variance   | 0.156     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.087    |
|    n_updates            | 260       |
|    policy_gradient_loss | -0.0662   |
|    value_loss           | 0.00285   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 13.6                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 14                   |
|    time_elapsed    | -1683717611046679808 |
|    total_timesteps | 28672                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 29000     |
| train/                  |           |
|    approx_kl            | 40.498764 |
|    clip_fraction        | 0.95      |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.25     |
|    explained_variance   | 0.105     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0722   |
|    n_updates            | 280       |
|    policy_gradient_loss | -0.0598   |
|    value_loss           | 0.00174   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 12.4                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 15                   |
|    time_elapsed    | -1683717611046679552 |
|    total_timesteps | 30720                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 31000     |
| train/                  |           |
|    approx_kl            | 57.670876 |
|    clip_fraction        | 0.943     |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.91     |
|    explained_variance   | 0.0351    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0837   |
|    n_updates            | 300       |
|    policy_gradient_loss | -0.0444   |
|    value_loss           | 0.00138   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 11.8                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 16                   |
|    time_elapsed    | -1683717611046679552 |
|    total_timesteps | 32768                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 33000     |
| train/                  |           |
|    approx_kl            | 129.59895 |
|    clip_fraction        | 0.95      |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.74     |
|    explained_variance   | 0.0263    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0836   |
|    n_updates            | 320       |
|    policy_gradient_loss | -0.0491   |
|    value_loss           | 0.00263   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 12.3                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 17                   |
|    time_elapsed    | -1683717611046679552 |
|    total_timesteps | 34816                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 108.15045 |
|    clip_fraction        | 0.95      |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.51     |
|    explained_variance   | 0.0811    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0763   |
|    n_updates            | 340       |
|    policy_gradient_loss | -0.0557   |
|    value_loss           | 0.000393  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 12.7                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 18                   |
|    time_elapsed    | -1683717611046679552 |
|    total_timesteps | 36864                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 37000     |
| train/                  |           |
|    approx_kl            | 92.193344 |
|    clip_fraction        | 0.933     |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.28     |
|    explained_variance   | 0.0343    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0683   |
|    n_updates            | 360       |
|    policy_gradient_loss | 0.151     |
|    value_loss           | 0.00264   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 11.7                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 19                   |
|    time_elapsed    | -1683717611046679552 |
|    total_timesteps | 38912                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 39000     |
| train/                  |           |
|    approx_kl            | 198.66081 |
|    clip_fraction        | 0.937     |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.94     |
|    explained_variance   | -0.0684   |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0682   |
|    n_updates            | 380       |
|    policy_gradient_loss | -0.0662   |
|    value_loss           | 0.00238   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 11.9                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 20                   |
|    time_elapsed    | -1683717611046679296 |
|    total_timesteps | 40960                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 41000     |
| train/                  |           |
|    approx_kl            | 100.30122 |
|    clip_fraction        | 0.915     |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.69     |
|    explained_variance   | -0.0974   |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0761   |
|    n_updates            | 400       |
|    policy_gradient_loss | -0.0589   |
|    value_loss           | 0.000405  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 11.5                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 21                   |
|    time_elapsed    | -1683717611046679296 |
|    total_timesteps | 43008                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 10       |
|    mean_reward          | 0.9      |
| time/                   |          |
|    total_timesteps      | 44000    |
| train/                  |          |
|    approx_kl            | 143.4921 |
|    clip_fraction        | 0.918    |
|    clip_range           | 0.2      |
|    entropy_loss         | -2.32    |
|    explained_variance   | -0.125   |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0683  |
|    n_updates            | 420      |
|    policy_gradient_loss | -0.0549  |
|    value_loss           | 0.00475  |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 11.1                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 22                   |
|    time_elapsed    | -1683717611046679296 |
|    total_timesteps | 45056                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 10       |
|    mean_reward          | 0.9      |
| time/                   |          |
|    total_timesteps      | 46000    |
| train/                  |          |
|    approx_kl            | 494.3689 |
|    clip_fraction        | 0.852    |
|    clip_range           | 0.2      |
|    entropy_loss         | -2.13    |
|    explained_variance   | -0.0138  |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0834  |
|    n_updates            | 440      |
|    policy_gradient_loss | 0.269    |
|    value_loss           | 0.00161  |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 10.8                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 23                   |
|    time_elapsed    | -1683717611046679296 |
|    total_timesteps | 47104                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 10       |
|    mean_reward          | 0.9      |
| time/                   |          |
|    total_timesteps      | 48000    |
| train/                  |          |
|    approx_kl            | 140.818  |
|    clip_fraction        | 0.885    |
|    clip_range           | 0.2      |
|    entropy_loss         | -1.89    |
|    explained_variance   | -0.101   |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0705  |
|    n_updates            | 460      |
|    policy_gradient_loss | 1.01     |
|    value_loss           | 0.000495 |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 10.6                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 24                   |
|    time_elapsed    | -1683717611046679296 |
|    total_timesteps | 49152                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 136.34624 |
|    clip_fraction        | 0.767     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.4      |
|    explained_variance   | -0.143    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0803   |
|    n_updates            | 480       |
|    policy_gradient_loss | -0.0558   |
|    value_loss           | 0.00255   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 10.4                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 25                   |
|    time_elapsed    | -1683717611046679040 |
|    total_timesteps | 51200                |
---------------------------------------------
{'reward': [0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9], 'std': [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]}
{'reward': [0.9000000134110451, 0.9000000134110451, 0.9000000134110451, 0.9000000134110451, 0.9000000134110451, 0.9000000134110451, 0.9000000134110451, 0.9000000134110451, 0.9000000134110451, 0.9000000134110451], 'std': [9, 9, 9, 9, 9, 9, 9, 9, 9, 9]}
