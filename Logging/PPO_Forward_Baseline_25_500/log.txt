Logging to ./Logging/PPO_Forward_Baseline_25_500
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
-----------------------------
| time/              |      |
|    fps             | 10   |
|    iterations      | 1    |
|    time_elapsed    | 204  |
|    total_timesteps | 2048 |
-----------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 3000        |
| train/                  |             |
|    approx_kl            | 0.019985981 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.37       |
|    explained_variance   | 0.654       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0453     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0408     |
|    value_loss           | 0.0238      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
-----------------------------
| time/              |      |
|    fps             | 10   |
|    iterations      | 2    |
|    time_elapsed    | 409  |
|    total_timesteps | 4096 |
-----------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.02302283 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.35      |
|    explained_variance   | 0.741      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0524    |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.0327    |
|    value_loss           | 0.00756    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
-----------------------------
| time/              |      |
|    fps             | 9    |
|    iterations      | 3    |
|    time_elapsed    | 614  |
|    total_timesteps | 6144 |
-----------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 7000        |
| train/                  |             |
|    approx_kl            | 0.019297054 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.34       |
|    explained_variance   | 0.776       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0579     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0343     |
|    value_loss           | 0.0123      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
-----------------------------
| time/              |      |
|    fps             | 9    |
|    iterations      | 4    |
|    time_elapsed    | 820  |
|    total_timesteps | 8192 |
-----------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 9000        |
| train/                  |             |
|    approx_kl            | 0.022213997 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.34       |
|    explained_variance   | 0.678       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0455     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0378     |
|    value_loss           | 0.0125      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 5     |
|    time_elapsed    | 1025  |
|    total_timesteps | 10240 |
------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 11000      |
| train/                  |            |
|    approx_kl            | 0.01834188 |
|    clip_fraction        | 0.21       |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.34      |
|    explained_variance   | 0.916      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0585    |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0447    |
|    value_loss           | 0.00555    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 6     |
|    time_elapsed    | 1230  |
|    total_timesteps | 12288 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 13000       |
| train/                  |             |
|    approx_kl            | 0.019907176 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.29       |
|    explained_variance   | 0.798       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0445     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0351     |
|    value_loss           | 0.0111      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 7     |
|    time_elapsed    | 1436  |
|    total_timesteps | 14336 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.015541013 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.26       |
|    explained_variance   | 0.712       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0117     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0284     |
|    value_loss           | 0.00942     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 8     |
|    time_elapsed    | 1641  |
|    total_timesteps | 16384 |
------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 17000      |
| train/                  |            |
|    approx_kl            | 0.02012692 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.27      |
|    explained_variance   | 0.646      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0483    |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.039     |
|    value_loss           | 0.00245    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 9     |
|    time_elapsed    | 1846  |
|    total_timesteps | 18432 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 19000       |
| train/                  |             |
|    approx_kl            | 0.020431988 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.28       |
|    explained_variance   | 0.784       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0344     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.033      |
|    value_loss           | 0.00459     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 10    |
|    time_elapsed    | 2052  |
|    total_timesteps | 20480 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 21000       |
| train/                  |             |
|    approx_kl            | 0.020448454 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.29       |
|    explained_variance   | 0.886       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0224     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0411     |
|    value_loss           | 0.00568     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 11    |
|    time_elapsed    | 2257  |
|    total_timesteps | 22528 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 23000       |
| train/                  |             |
|    approx_kl            | 0.017178964 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.24       |
|    explained_variance   | 0.765       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0664     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0355     |
|    value_loss           | 0.00424     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 12    |
|    time_elapsed    | 2462  |
|    total_timesteps | 24576 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.024173478 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.23       |
|    explained_variance   | 0.906       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0659     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0412     |
|    value_loss           | 0.00373     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 13    |
|    time_elapsed    | 2668  |
|    total_timesteps | 26624 |
------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 27000      |
| train/                  |            |
|    approx_kl            | 0.02005231 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.18      |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0516    |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0389    |
|    value_loss           | 0.00431    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 14    |
|    time_elapsed    | 2873  |
|    total_timesteps | 28672 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 29000       |
| train/                  |             |
|    approx_kl            | 0.019455737 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.2        |
|    explained_variance   | 0.856       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0286     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0327     |
|    value_loss           | 0.00255     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 15    |
|    time_elapsed    | 3078  |
|    total_timesteps | 30720 |
------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 31000      |
| train/                  |            |
|    approx_kl            | 0.02244336 |
|    clip_fraction        | 0.235      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.15      |
|    explained_variance   | 0.879      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0269    |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0274    |
|    value_loss           | 0.00133    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 16    |
|    time_elapsed    | 3284  |
|    total_timesteps | 32768 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 33000       |
| train/                  |             |
|    approx_kl            | 0.019668661 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.24       |
|    explained_variance   | 0.87        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0407     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0362     |
|    value_loss           | 0.00272     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 17    |
|    time_elapsed    | 3489  |
|    total_timesteps | 34816 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.021086216 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.2        |
|    explained_variance   | 0.742       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0824     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0413     |
|    value_loss           | 0.00419     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 18    |
|    time_elapsed    | 3694  |
|    total_timesteps | 36864 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 37000       |
| train/                  |             |
|    approx_kl            | 0.022008916 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.17       |
|    explained_variance   | 0.828       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0349     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0416     |
|    value_loss           | 0.00212     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 19    |
|    time_elapsed    | 3900  |
|    total_timesteps | 38912 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 39000       |
| train/                  |             |
|    approx_kl            | 0.026216568 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.18       |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0676     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0452     |
|    value_loss           | 0.00563     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 20    |
|    time_elapsed    | 4105  |
|    total_timesteps | 40960 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 41000       |
| train/                  |             |
|    approx_kl            | 0.018178519 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.17       |
|    explained_variance   | 0.719       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0511     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0358     |
|    value_loss           | 0.00311     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 21    |
|    time_elapsed    | 4410  |
|    total_timesteps | 43008 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 44000       |
| train/                  |             |
|    approx_kl            | 0.029751208 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.12       |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0748     |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0384     |
|    value_loss           | 0.000468    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 22    |
|    time_elapsed    | 4616  |
|    total_timesteps | 45056 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 46000       |
| train/                  |             |
|    approx_kl            | 0.020559017 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.1        |
|    explained_variance   | 0.819       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0374     |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0289     |
|    value_loss           | 0.00109     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 23    |
|    time_elapsed    | 4821  |
|    total_timesteps | 47104 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 48000       |
| train/                  |             |
|    approx_kl            | 0.024748107 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.15       |
|    explained_variance   | 0.857       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0343     |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0357     |
|    value_loss           | 0.00331     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 24    |
|    time_elapsed    | 5026  |
|    total_timesteps | 49152 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.018235028 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.03       |
|    explained_variance   | 0.884       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0234     |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0266     |
|    value_loss           | 0.000892    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 25    |
|    time_elapsed    | 5231  |
|    total_timesteps | 51200 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 52000       |
| train/                  |             |
|    approx_kl            | 0.022268025 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.16       |
|    explained_variance   | 0.782       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0631     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.044      |
|    value_loss           | 0.00187     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 26    |
|    time_elapsed    | 5437  |
|    total_timesteps | 53248 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 54000       |
| train/                  |             |
|    approx_kl            | 0.022131052 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.17       |
|    explained_variance   | 0.903       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0681     |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0363     |
|    value_loss           | 0.00176     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 27    |
|    time_elapsed    | 5642  |
|    total_timesteps | 55296 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 56000       |
| train/                  |             |
|    approx_kl            | 0.021880534 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.05       |
|    explained_variance   | 0.861       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0466     |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0354     |
|    value_loss           | 0.00466     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 28    |
|    time_elapsed    | 5847  |
|    total_timesteps | 57344 |
------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 58000      |
| train/                  |            |
|    approx_kl            | 0.02033234 |
|    clip_fraction        | 0.217      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.17      |
|    explained_variance   | 0.843      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0547    |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0353    |
|    value_loss           | 0.00185    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 29    |
|    time_elapsed    | 6053  |
|    total_timesteps | 59392 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.022715189 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.1        |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0667     |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0338     |
|    value_loss           | 0.00227     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 30    |
|    time_elapsed    | 6258  |
|    total_timesteps | 61440 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 62000       |
| train/                  |             |
|    approx_kl            | 0.026198592 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.11       |
|    explained_variance   | 0.85        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0476     |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.0406     |
|    value_loss           | 0.00417     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 31    |
|    time_elapsed    | 6464  |
|    total_timesteps | 63488 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 64000       |
| train/                  |             |
|    approx_kl            | 0.033535205 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.06       |
|    explained_variance   | 0.858       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0799     |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.0374     |
|    value_loss           | 0.00334     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 32    |
|    time_elapsed    | 6669  |
|    total_timesteps | 65536 |
------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 66000      |
| train/                  |            |
|    approx_kl            | 0.02114747 |
|    clip_fraction        | 0.243      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.06      |
|    explained_variance   | 0.781      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0788    |
|    n_updates            | 320        |
|    policy_gradient_loss | -0.0331    |
|    value_loss           | 0.00126    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 33    |
|    time_elapsed    | 6874  |
|    total_timesteps | 67584 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 68000       |
| train/                  |             |
|    approx_kl            | 0.019453073 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.03       |
|    explained_variance   | 0.886       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0789     |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.0417     |
|    value_loss           | 0.00171     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 34    |
|    time_elapsed    | 7079  |
|    total_timesteps | 69632 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.024040524 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.12       |
|    explained_variance   | 0.822       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0172     |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.036      |
|    value_loss           | 0.00194     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 35    |
|    time_elapsed    | 7285  |
|    total_timesteps | 71680 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 72000       |
| train/                  |             |
|    approx_kl            | 0.024051793 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.03       |
|    explained_variance   | 0.716       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0721     |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.0408     |
|    value_loss           | 0.00244     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 36    |
|    time_elapsed    | 7490  |
|    total_timesteps | 73728 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 74000       |
| train/                  |             |
|    approx_kl            | 0.017987264 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.07       |
|    explained_variance   | 0.77        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0333     |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.036      |
|    value_loss           | 0.00263     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 37    |
|    time_elapsed    | 7696  |
|    total_timesteps | 75776 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 76000       |
| train/                  |             |
|    approx_kl            | 0.023898713 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.1        |
|    explained_variance   | 0.893       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0448     |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.0392     |
|    value_loss           | 0.00196     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 38    |
|    time_elapsed    | 7901  |
|    total_timesteps | 77824 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 78000       |
| train/                  |             |
|    approx_kl            | 0.023558574 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.07       |
|    explained_variance   | 0.799       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0505     |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.0354     |
|    value_loss           | 0.00342     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 39    |
|    time_elapsed    | 8106  |
|    total_timesteps | 79872 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.020722754 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.96       |
|    explained_variance   | 0.722       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0657     |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.0385     |
|    value_loss           | 0.00168     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 40    |
|    time_elapsed    | 8312  |
|    total_timesteps | 81920 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 82000       |
| train/                  |             |
|    approx_kl            | 0.018887868 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.1        |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0482     |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.0364     |
|    value_loss           | 0.00208     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 41    |
|    time_elapsed    | 8517  |
|    total_timesteps | 83968 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 84000       |
| train/                  |             |
|    approx_kl            | 0.020944132 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.05       |
|    explained_variance   | 0.845       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0527     |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.033      |
|    value_loss           | 0.00225     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 42    |
|    time_elapsed    | 8822  |
|    total_timesteps | 86016 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 87000       |
| train/                  |             |
|    approx_kl            | 0.018955037 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.9        |
|    explained_variance   | 0.856       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.032      |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0279     |
|    value_loss           | 0.000704    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 43    |
|    time_elapsed    | 9028  |
|    total_timesteps | 88064 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 89000       |
| train/                  |             |
|    approx_kl            | 0.018557351 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.98       |
|    explained_variance   | 0.827       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0475     |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.0274     |
|    value_loss           | 0.00047     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 44    |
|    time_elapsed    | 9233  |
|    total_timesteps | 90112 |
------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 91000      |
| train/                  |            |
|    approx_kl            | 0.02195245 |
|    clip_fraction        | 0.251      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.08      |
|    explained_variance   | 0.78       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0325    |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.0407    |
|    value_loss           | 0.00214    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 45    |
|    time_elapsed    | 9438  |
|    total_timesteps | 92160 |
------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+04     |
|    mean_reward          | 0         |
| time/                   |           |
|    total_timesteps      | 93000     |
| train/                  |           |
|    approx_kl            | 0.0200854 |
|    clip_fraction        | 0.22      |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.08     |
|    explained_variance   | 0.907     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0575   |
|    n_updates            | 450       |
|    policy_gradient_loss | -0.0355   |
|    value_loss           | 0.00224   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 46    |
|    time_elapsed    | 9644  |
|    total_timesteps | 94208 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 95000       |
| train/                  |             |
|    approx_kl            | 0.021236071 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.03       |
|    explained_variance   | 0.696       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0662     |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.0381     |
|    value_loss           | 0.00167     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 47    |
|    time_elapsed    | 9849  |
|    total_timesteps | 96256 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 97000       |
| train/                  |             |
|    approx_kl            | 0.019788055 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8          |
|    explained_variance   | 0.785       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0317     |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.0302     |
|    value_loss           | 0.000631    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 48    |
|    time_elapsed    | 10054 |
|    total_timesteps | 98304 |
------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 99000      |
| train/                  |            |
|    approx_kl            | 0.02169659 |
|    clip_fraction        | 0.242      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.94      |
|    explained_variance   | 0.842      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0201    |
|    n_updates            | 480        |
|    policy_gradient_loss | -0.0352    |
|    value_loss           | 0.00112    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 49     |
|    time_elapsed    | 10260  |
|    total_timesteps | 100352 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 101000      |
| train/                  |             |
|    approx_kl            | 0.021755759 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.75       |
|    explained_variance   | 0.681       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.047      |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.034      |
|    value_loss           | 0.00138     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 50     |
|    time_elapsed    | 10465  |
|    total_timesteps | 102400 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 103000     |
| train/                  |            |
|    approx_kl            | 0.01980004 |
|    clip_fraction        | 0.194      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.83      |
|    explained_variance   | 0.371      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0445    |
|    n_updates            | 500        |
|    policy_gradient_loss | -0.0291    |
|    value_loss           | 0.0016     |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 51     |
|    time_elapsed    | 10670  |
|    total_timesteps | 104448 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 105000      |
| train/                  |             |
|    approx_kl            | 0.018445848 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.82       |
|    explained_variance   | 0.833       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0608     |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.0329     |
|    value_loss           | 0.00149     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 52     |
|    time_elapsed    | 10876  |
|    total_timesteps | 106496 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 107000      |
| train/                  |             |
|    approx_kl            | 0.019258058 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.77       |
|    explained_variance   | 0.856       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0577     |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.0378     |
|    value_loss           | 0.00125     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 53     |
|    time_elapsed    | 11081  |
|    total_timesteps | 108544 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 109000      |
| train/                  |             |
|    approx_kl            | 0.020707343 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.87       |
|    explained_variance   | 0.815       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0463     |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.0391     |
|    value_loss           | 0.00116     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 54     |
|    time_elapsed    | 11286  |
|    total_timesteps | 110592 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 111000      |
| train/                  |             |
|    approx_kl            | 0.021869782 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.89       |
|    explained_variance   | 0.857       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0625     |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.031      |
|    value_loss           | 0.00106     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 55     |
|    time_elapsed    | 11492  |
|    total_timesteps | 112640 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 113000      |
| train/                  |             |
|    approx_kl            | 0.024935013 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.84       |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0143     |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.0415     |
|    value_loss           | 0.00212     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 56     |
|    time_elapsed    | 11697  |
|    total_timesteps | 114688 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 115000      |
| train/                  |             |
|    approx_kl            | 0.031131564 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.84       |
|    explained_variance   | 0.812       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0324     |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.0364     |
|    value_loss           | 0.00238     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 57     |
|    time_elapsed    | 11902  |
|    total_timesteps | 116736 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 117000      |
| train/                  |             |
|    approx_kl            | 0.022245636 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.83       |
|    explained_variance   | 0.622       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0372     |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.0381     |
|    value_loss           | 0.00147     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 58     |
|    time_elapsed    | 12108  |
|    total_timesteps | 118784 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 119000      |
| train/                  |             |
|    approx_kl            | 0.023765322 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.84       |
|    explained_variance   | 0.832       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0464     |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.0331     |
|    value_loss           | 0.001       |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 59     |
|    time_elapsed    | 12313  |
|    total_timesteps | 120832 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 121000      |
| train/                  |             |
|    approx_kl            | 0.020067967 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.81       |
|    explained_variance   | 0.713       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0436     |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.0249     |
|    value_loss           | 0.000832    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 60     |
|    time_elapsed    | 12518  |
|    total_timesteps | 122880 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 123000      |
| train/                  |             |
|    approx_kl            | 0.020750523 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.8        |
|    explained_variance   | 0.776       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0431     |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.0357     |
|    value_loss           | 0.00147     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 61     |
|    time_elapsed    | 12724  |
|    total_timesteps | 124928 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 125000      |
| train/                  |             |
|    approx_kl            | 0.020863228 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.88       |
|    explained_variance   | 0.869       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0406     |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.0392     |
|    value_loss           | 0.00175     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 62     |
|    time_elapsed    | 12929  |
|    total_timesteps | 126976 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 127000      |
| train/                  |             |
|    approx_kl            | 0.023792641 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.77       |
|    explained_variance   | 0.654       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0271     |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.0243     |
|    value_loss           | 0.00245     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 63     |
|    time_elapsed    | 13234  |
|    total_timesteps | 129024 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 130000      |
| train/                  |             |
|    approx_kl            | 0.021472063 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.73       |
|    explained_variance   | 0.632       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0578     |
|    n_updates            | 630         |
|    policy_gradient_loss | -0.0339     |
|    value_loss           | 0.00121     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 64     |
|    time_elapsed    | 13440  |
|    total_timesteps | 131072 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 132000      |
| train/                  |             |
|    approx_kl            | 0.020789457 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.6        |
|    explained_variance   | 0.768       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0682     |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.0343     |
|    value_loss           | 0.00147     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 65     |
|    time_elapsed    | 13645  |
|    total_timesteps | 133120 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 134000      |
| train/                  |             |
|    approx_kl            | 0.020655015 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.79       |
|    explained_variance   | 0.836       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0532     |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.0334     |
|    value_loss           | 0.00142     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 66     |
|    time_elapsed    | 13850  |
|    total_timesteps | 135168 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 136000      |
| train/                  |             |
|    approx_kl            | 0.019180339 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.58       |
|    explained_variance   | 0.673       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0506     |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.0353     |
|    value_loss           | 0.00117     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 67     |
|    time_elapsed    | 14056  |
|    total_timesteps | 137216 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 138000      |
| train/                  |             |
|    approx_kl            | 0.021825813 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.65       |
|    explained_variance   | 0.848       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0679     |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.0286     |
|    value_loss           | 0.00095     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 68     |
|    time_elapsed    | 14261  |
|    total_timesteps | 139264 |
-------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+04     |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 140000    |
| train/                  |           |
|    approx_kl            | 0.0199424 |
|    clip_fraction        | 0.218     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.86     |
|    explained_variance   | 0.756     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0246   |
|    n_updates            | 680       |
|    policy_gradient_loss | -0.0318   |
|    value_loss           | 0.000465  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 69     |
|    time_elapsed    | 14466  |
|    total_timesteps | 141312 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 142000      |
| train/                  |             |
|    approx_kl            | 0.024745896 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.61       |
|    explained_variance   | 0.817       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0645     |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.0361     |
|    value_loss           | 0.00186     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 70     |
|    time_elapsed    | 14672  |
|    total_timesteps | 143360 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 144000      |
| train/                  |             |
|    approx_kl            | 0.022125578 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.5        |
|    explained_variance   | 0.883       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0262     |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.0303     |
|    value_loss           | 0.000823    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 71     |
|    time_elapsed    | 14877  |
|    total_timesteps | 145408 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 146000      |
| train/                  |             |
|    approx_kl            | 0.025517697 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.62       |
|    explained_variance   | 0.709       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0449     |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.0354     |
|    value_loss           | 0.00146     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 72     |
|    time_elapsed    | 15083  |
|    total_timesteps | 147456 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 148000      |
| train/                  |             |
|    approx_kl            | 0.022437718 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.8        |
|    explained_variance   | 0.583       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.042      |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.0368     |
|    value_loss           | 0.000988    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 73     |
|    time_elapsed    | 15288  |
|    total_timesteps | 149504 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.027937233 |
|    clip_fraction        | 0.306       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.63       |
|    explained_variance   | 0.822       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0581     |
|    n_updates            | 730         |
|    policy_gradient_loss | -0.0402     |
|    value_loss           | 0.00176     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 151000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 74     |
|    time_elapsed    | 15493  |
|    total_timesteps | 151552 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.3        |
| time/                   |            |
|    total_timesteps      | 152000     |
| train/                  |            |
|    approx_kl            | 0.02168399 |
|    clip_fraction        | 0.243      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.47      |
|    explained_variance   | 0.542      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0464    |
|    n_updates            | 740        |
|    policy_gradient_loss | -0.0314    |
|    value_loss           | 0.00146    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 153000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 75     |
|    time_elapsed    | 15699  |
|    total_timesteps | 153600 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 154000      |
| train/                  |             |
|    approx_kl            | 0.024686709 |
|    clip_fraction        | 0.276       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.66       |
|    explained_variance   | 0.234       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0372     |
|    n_updates            | 750         |
|    policy_gradient_loss | -0.034      |
|    value_loss           | 0.000841    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 76     |
|    time_elapsed    | 15904  |
|    total_timesteps | 155648 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 156000      |
| train/                  |             |
|    approx_kl            | 0.020899488 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.47       |
|    explained_variance   | 0.824       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0581     |
|    n_updates            | 760         |
|    policy_gradient_loss | -0.0313     |
|    value_loss           | 0.00141     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 77     |
|    time_elapsed    | 16109  |
|    total_timesteps | 157696 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 158000      |
| train/                  |             |
|    approx_kl            | 0.021240417 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.45       |
|    explained_variance   | 0.854       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0521     |
|    n_updates            | 770         |
|    policy_gradient_loss | -0.0417     |
|    value_loss           | 0.0019      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 159000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 78     |
|    time_elapsed    | 16314  |
|    total_timesteps | 159744 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 160000      |
| train/                  |             |
|    approx_kl            | 0.020102013 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.59       |
|    explained_variance   | 0.849       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0451     |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.0325     |
|    value_loss           | 0.00121     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 161000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 79     |
|    time_elapsed    | 16520  |
|    total_timesteps | 161792 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 162000      |
| train/                  |             |
|    approx_kl            | 0.024671165 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.56       |
|    explained_variance   | 0.806       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00621    |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.0344     |
|    value_loss           | 0.000924    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 163000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 80     |
|    time_elapsed    | 16725  |
|    total_timesteps | 163840 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 164000      |
| train/                  |             |
|    approx_kl            | 0.019687865 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.58       |
|    explained_variance   | 0.75        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.03       |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.0335     |
|    value_loss           | 0.00134     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 165000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 81     |
|    time_elapsed    | 16930  |
|    total_timesteps | 165888 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 166000      |
| train/                  |             |
|    approx_kl            | 0.023850858 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.59       |
|    explained_variance   | 0.76        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0613     |
|    n_updates            | 810         |
|    policy_gradient_loss | -0.0373     |
|    value_loss           | 0.00202     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 167000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 82     |
|    time_elapsed    | 17136  |
|    total_timesteps | 167936 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.4        |
| time/                   |            |
|    total_timesteps      | 168000     |
| train/                  |            |
|    approx_kl            | 0.02344802 |
|    clip_fraction        | 0.245      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.32      |
|    explained_variance   | 0.81       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0397    |
|    n_updates            | 820        |
|    policy_gradient_loss | -0.0326    |
|    value_loss           | 0.00152    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 169000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 83     |
|    time_elapsed    | 17341  |
|    total_timesteps | 169984 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 170000      |
| train/                  |             |
|    approx_kl            | 0.021791246 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.37       |
|    explained_variance   | 0.896       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0353     |
|    n_updates            | 830         |
|    policy_gradient_loss | -0.0345     |
|    value_loss           | 0.000868    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 84     |
|    time_elapsed    | 17646  |
|    total_timesteps | 172032 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.3        |
| time/                   |            |
|    total_timesteps      | 173000     |
| train/                  |            |
|    approx_kl            | 0.02450553 |
|    clip_fraction        | 0.268      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.3       |
|    explained_variance   | 0.544      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0478    |
|    n_updates            | 840        |
|    policy_gradient_loss | -0.0351    |
|    value_loss           | 0.00201    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 85     |
|    time_elapsed    | 17852  |
|    total_timesteps | 174080 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 175000      |
| train/                  |             |
|    approx_kl            | 0.017154025 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.31       |
|    explained_variance   | 0.654       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0396     |
|    n_updates            | 850         |
|    policy_gradient_loss | -0.0284     |
|    value_loss           | 0.00168     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 86     |
|    time_elapsed    | 18057  |
|    total_timesteps | 176128 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 177000      |
| train/                  |             |
|    approx_kl            | 0.024558065 |
|    clip_fraction        | 0.298       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.22       |
|    explained_variance   | 0.854       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0398     |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.0313     |
|    value_loss           | 0.00116     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 87     |
|    time_elapsed    | 18262  |
|    total_timesteps | 178176 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 179000      |
| train/                  |             |
|    approx_kl            | 0.021065693 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.23       |
|    explained_variance   | 0.659       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0423     |
|    n_updates            | 870         |
|    policy_gradient_loss | -0.0319     |
|    value_loss           | 0.00105     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 88     |
|    time_elapsed    | 18468  |
|    total_timesteps | 180224 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 181000      |
| train/                  |             |
|    approx_kl            | 0.017027706 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.4        |
|    explained_variance   | 0.627       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0456     |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.0244     |
|    value_loss           | 0.00122     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 89     |
|    time_elapsed    | 18673  |
|    total_timesteps | 182272 |
-------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+04     |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 183000    |
| train/                  |           |
|    approx_kl            | 0.0203801 |
|    clip_fraction        | 0.263     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.41     |
|    explained_variance   | 0.721     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0361   |
|    n_updates            | 890       |
|    policy_gradient_loss | -0.0333   |
|    value_loss           | 0.0013    |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 90     |
|    time_elapsed    | 18878  |
|    total_timesteps | 184320 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.5        |
| time/                   |            |
|    total_timesteps      | 185000     |
| train/                  |            |
|    approx_kl            | 0.02011992 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.4       |
|    explained_variance   | 0.577      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0445    |
|    n_updates            | 900        |
|    policy_gradient_loss | -0.0321    |
|    value_loss           | 0.00165    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 91     |
|    time_elapsed    | 19084  |
|    total_timesteps | 186368 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 187000      |
| train/                  |             |
|    approx_kl            | 0.022655267 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.46       |
|    explained_variance   | 0.785       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0634     |
|    n_updates            | 910         |
|    policy_gradient_loss | -0.0362     |
|    value_loss           | 0.00308     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 92     |
|    time_elapsed    | 19289  |
|    total_timesteps | 188416 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 189000      |
| train/                  |             |
|    approx_kl            | 0.020858362 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.35       |
|    explained_variance   | 0.677       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0345     |
|    n_updates            | 920         |
|    policy_gradient_loss | -0.0288     |
|    value_loss           | 0.00249     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 93     |
|    time_elapsed    | 19494  |
|    total_timesteps | 190464 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 191000      |
| train/                  |             |
|    approx_kl            | 0.019213062 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.12       |
|    explained_variance   | 0.751       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0221     |
|    n_updates            | 930         |
|    policy_gradient_loss | -0.0254     |
|    value_loss           | 0.00168     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 94     |
|    time_elapsed    | 19700  |
|    total_timesteps | 192512 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.5        |
| time/                   |            |
|    total_timesteps      | 193000     |
| train/                  |            |
|    approx_kl            | 0.01889407 |
|    clip_fraction        | 0.233      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.34      |
|    explained_variance   | 0.848      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0724    |
|    n_updates            | 940        |
|    policy_gradient_loss | -0.0294    |
|    value_loss           | 0.00111    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 95     |
|    time_elapsed    | 19905  |
|    total_timesteps | 194560 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.5        |
| time/                   |            |
|    total_timesteps      | 195000     |
| train/                  |            |
|    approx_kl            | 0.01823182 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.35      |
|    explained_variance   | 0.76       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0262    |
|    n_updates            | 950        |
|    policy_gradient_loss | -0.0251    |
|    value_loss           | 0.00192    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 96     |
|    time_elapsed    | 20110  |
|    total_timesteps | 196608 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 197000      |
| train/                  |             |
|    approx_kl            | 0.016925637 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.22       |
|    explained_variance   | 0.78        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0445     |
|    n_updates            | 960         |
|    policy_gradient_loss | -0.022      |
|    value_loss           | 0.00189     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 97     |
|    time_elapsed    | 20316  |
|    total_timesteps | 198656 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 199000      |
| train/                  |             |
|    approx_kl            | 0.018644683 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.2        |
|    explained_variance   | 0.639       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0491     |
|    n_updates            | 970         |
|    policy_gradient_loss | -0.0266     |
|    value_loss           | 0.00151     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 98     |
|    time_elapsed    | 20521  |
|    total_timesteps | 200704 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 201000      |
| train/                  |             |
|    approx_kl            | 0.019019477 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.19       |
|    explained_variance   | 0.874       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0578     |
|    n_updates            | 980         |
|    policy_gradient_loss | -0.0282     |
|    value_loss           | 0.00141     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 99     |
|    time_elapsed    | 20726  |
|    total_timesteps | 202752 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 203000      |
| train/                  |             |
|    approx_kl            | 0.016721152 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.2        |
|    explained_variance   | 0.684       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00666    |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.0214     |
|    value_loss           | 0.00222     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 100    |
|    time_elapsed    | 20932  |
|    total_timesteps | 204800 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 205000      |
| train/                  |             |
|    approx_kl            | 0.017666664 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.26       |
|    explained_variance   | 0.731       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0498     |
|    n_updates            | 1000        |
|    policy_gradient_loss | -0.031      |
|    value_loss           | 0.00182     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 101    |
|    time_elapsed    | 21137  |
|    total_timesteps | 206848 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 207000      |
| train/                  |             |
|    approx_kl            | 0.022600226 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.16       |
|    explained_variance   | 0.549       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0494     |
|    n_updates            | 1010        |
|    policy_gradient_loss | -0.0305     |
|    value_loss           | 0.0025      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 102    |
|    time_elapsed    | 21342  |
|    total_timesteps | 208896 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 209000      |
| train/                  |             |
|    approx_kl            | 0.018296575 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.22       |
|    explained_variance   | 0.398       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0431     |
|    n_updates            | 1020        |
|    policy_gradient_loss | -0.0238     |
|    value_loss           | 0.00159     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 103    |
|    time_elapsed    | 21548  |
|    total_timesteps | 210944 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 211000      |
| train/                  |             |
|    approx_kl            | 0.021556156 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.37       |
|    explained_variance   | 0.611       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00985    |
|    n_updates            | 1030        |
|    policy_gradient_loss | -0.0299     |
|    value_loss           | 0.0025      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 104    |
|    time_elapsed    | 21753  |
|    total_timesteps | 212992 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 213000      |
| train/                  |             |
|    approx_kl            | 0.019374698 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.11       |
|    explained_variance   | 0.688       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0143     |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.0292     |
|    value_loss           | 0.00162     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 105    |
|    time_elapsed    | 22058  |
|    total_timesteps | 215040 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 216000      |
| train/                  |             |
|    approx_kl            | 0.022304447 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.36       |
|    explained_variance   | 0.468       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0501     |
|    n_updates            | 1050        |
|    policy_gradient_loss | -0.0256     |
|    value_loss           | 0.00333     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 106    |
|    time_elapsed    | 22264  |
|    total_timesteps | 217088 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.6        |
| time/                   |            |
|    total_timesteps      | 218000     |
| train/                  |            |
|    approx_kl            | 0.02390291 |
|    clip_fraction        | 0.257      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.15      |
|    explained_variance   | 0.81       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0566    |
|    n_updates            | 1060       |
|    policy_gradient_loss | -0.0287    |
|    value_loss           | 0.00144    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 107    |
|    time_elapsed    | 22469  |
|    total_timesteps | 219136 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 220000      |
| train/                  |             |
|    approx_kl            | 0.017984811 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.19       |
|    explained_variance   | 0.646       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0286     |
|    n_updates            | 1070        |
|    policy_gradient_loss | -0.0282     |
|    value_loss           | 0.0015      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 108    |
|    time_elapsed    | 22674  |
|    total_timesteps | 221184 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 222000      |
| train/                  |             |
|    approx_kl            | 0.023128744 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.31       |
|    explained_variance   | 0.857       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0599     |
|    n_updates            | 1080        |
|    policy_gradient_loss | -0.0319     |
|    value_loss           | 0.00117     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 109    |
|    time_elapsed    | 22880  |
|    total_timesteps | 223232 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 224000      |
| train/                  |             |
|    approx_kl            | 0.022782508 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.36       |
|    explained_variance   | 0.63        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0297     |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.0289     |
|    value_loss           | 0.00224     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 110    |
|    time_elapsed    | 23085  |
|    total_timesteps | 225280 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 226000      |
| train/                  |             |
|    approx_kl            | 0.020526186 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.34       |
|    explained_variance   | 0.552       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0217     |
|    n_updates            | 1100        |
|    policy_gradient_loss | -0.0246     |
|    value_loss           | 0.00295     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 111    |
|    time_elapsed    | 23290  |
|    total_timesteps | 227328 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 228000      |
| train/                  |             |
|    approx_kl            | 0.018712942 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.18       |
|    explained_variance   | 0.496       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0175     |
|    n_updates            | 1110        |
|    policy_gradient_loss | -0.0224     |
|    value_loss           | 0.00253     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 112    |
|    time_elapsed    | 23496  |
|    total_timesteps | 229376 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.7        |
| time/                   |            |
|    total_timesteps      | 230000     |
| train/                  |            |
|    approx_kl            | 0.02192315 |
|    clip_fraction        | 0.241      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.13      |
|    explained_variance   | 0.635      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0191     |
|    n_updates            | 1120       |
|    policy_gradient_loss | -0.0215    |
|    value_loss           | 0.00221    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 113    |
|    time_elapsed    | 23701  |
|    total_timesteps | 231424 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 232000      |
| train/                  |             |
|    approx_kl            | 0.020609405 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.08       |
|    explained_variance   | 0.283       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0597     |
|    n_updates            | 1130        |
|    policy_gradient_loss | -0.0254     |
|    value_loss           | 0.00233     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 114    |
|    time_elapsed    | 23906  |
|    total_timesteps | 233472 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 234000      |
| train/                  |             |
|    approx_kl            | 0.016248502 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.15       |
|    explained_variance   | 0.468       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0342     |
|    n_updates            | 1140        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.00268     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 115    |
|    time_elapsed    | 24112  |
|    total_timesteps | 235520 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 236000      |
| train/                  |             |
|    approx_kl            | 0.016130133 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.99       |
|    explained_variance   | 0.52        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0172     |
|    n_updates            | 1150        |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.00249     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 237000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 116    |
|    time_elapsed    | 24317  |
|    total_timesteps | 237568 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 238000      |
| train/                  |             |
|    approx_kl            | 0.022535449 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.94       |
|    explained_variance   | 0.587       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0405     |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.0275     |
|    value_loss           | 0.00239     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 239000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 117    |
|    time_elapsed    | 24522  |
|    total_timesteps | 239616 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 0.021704532 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.22       |
|    explained_variance   | 0.541       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0162     |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.0223     |
|    value_loss           | 0.00325     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 241000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 118    |
|    time_elapsed    | 24728  |
|    total_timesteps | 241664 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 242000      |
| train/                  |             |
|    approx_kl            | 0.023074068 |
|    clip_fraction        | 0.285       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.02       |
|    explained_variance   | 0.467       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0562     |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.0272     |
|    value_loss           | 0.00286     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 243000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 119    |
|    time_elapsed    | 24933  |
|    total_timesteps | 243712 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 244000      |
| train/                  |             |
|    approx_kl            | 0.023673713 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.1        |
|    explained_variance   | 0.734       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0603     |
|    n_updates            | 1190        |
|    policy_gradient_loss | -0.0336     |
|    value_loss           | 0.00243     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 120    |
|    time_elapsed    | 25138  |
|    total_timesteps | 245760 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 246000     |
| train/                  |            |
|    approx_kl            | 0.02735266 |
|    clip_fraction        | 0.302      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.29      |
|    explained_variance   | 0.87       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00435   |
|    n_updates            | 1200       |
|    policy_gradient_loss | -0.0336    |
|    value_loss           | 0.00145    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 247000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 121    |
|    time_elapsed    | 25344  |
|    total_timesteps | 247808 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 248000      |
| train/                  |             |
|    approx_kl            | 0.020792723 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.95       |
|    explained_variance   | 0.57        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.024      |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.0266     |
|    value_loss           | 0.00326     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 122    |
|    time_elapsed    | 25549  |
|    total_timesteps | 249856 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.019410053 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.97       |
|    explained_variance   | 0.717       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0186     |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.0225     |
|    value_loss           | 0.00257     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 123    |
|    time_elapsed    | 25754  |
|    total_timesteps | 251904 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 252000      |
| train/                  |             |
|    approx_kl            | 0.016632324 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7          |
|    explained_variance   | 0.669       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0206     |
|    n_updates            | 1230        |
|    policy_gradient_loss | -0.02       |
|    value_loss           | 0.00255     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 253000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 124    |
|    time_elapsed    | 25960  |
|    total_timesteps | 253952 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 254000      |
| train/                  |             |
|    approx_kl            | 0.018143877 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.04       |
|    explained_variance   | 0.606       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0419     |
|    n_updates            | 1240        |
|    policy_gradient_loss | -0.0229     |
|    value_loss           | 0.00269     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 255000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 125    |
|    time_elapsed    | 26265  |
|    total_timesteps | 256000 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 257000      |
| train/                  |             |
|    approx_kl            | 0.019883368 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.11       |
|    explained_variance   | 0.696       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0186      |
|    n_updates            | 1250        |
|    policy_gradient_loss | -0.026      |
|    value_loss           | 0.00224     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 126    |
|    time_elapsed    | 26470  |
|    total_timesteps | 258048 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 259000      |
| train/                  |             |
|    approx_kl            | 0.025307883 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.93       |
|    explained_variance   | 0.569       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0142     |
|    n_updates            | 1260        |
|    policy_gradient_loss | -0.0248     |
|    value_loss           | 0.00309     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 127    |
|    time_elapsed    | 26676  |
|    total_timesteps | 260096 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 261000      |
| train/                  |             |
|    approx_kl            | 0.016538702 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.9        |
|    explained_variance   | 0.379       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0187     |
|    n_updates            | 1270        |
|    policy_gradient_loss | -0.0201     |
|    value_loss           | 0.00308     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 128    |
|    time_elapsed    | 26881  |
|    total_timesteps | 262144 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 263000      |
| train/                  |             |
|    approx_kl            | 0.016702471 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.92       |
|    explained_variance   | 0.676       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0471     |
|    n_updates            | 1280        |
|    policy_gradient_loss | -0.0182     |
|    value_loss           | 0.00276     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 129    |
|    time_elapsed    | 27086  |
|    total_timesteps | 264192 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 265000     |
| train/                  |            |
|    approx_kl            | 0.01792942 |
|    clip_fraction        | 0.227      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.96      |
|    explained_variance   | 0.568      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0164    |
|    n_updates            | 1290       |
|    policy_gradient_loss | -0.018     |
|    value_loss           | 0.00348    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 130    |
|    time_elapsed    | 27292  |
|    total_timesteps | 266240 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 267000      |
| train/                  |             |
|    approx_kl            | 0.019437741 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.88       |
|    explained_variance   | 0.56        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0279     |
|    n_updates            | 1300        |
|    policy_gradient_loss | -0.0214     |
|    value_loss           | 0.00365     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 131    |
|    time_elapsed    | 27497  |
|    total_timesteps | 268288 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 269000      |
| train/                  |             |
|    approx_kl            | 0.018857237 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.95       |
|    explained_variance   | 0.434       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0242     |
|    n_updates            | 1310        |
|    policy_gradient_loss | -0.0181     |
|    value_loss           | 0.00395     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 132    |
|    time_elapsed    | 27703  |
|    total_timesteps | 270336 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 271000      |
| train/                  |             |
|    approx_kl            | 0.016736474 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.73       |
|    explained_variance   | 0.44        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0324     |
|    n_updates            | 1320        |
|    policy_gradient_loss | -0.0129     |
|    value_loss           | 0.00508     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 133    |
|    time_elapsed    | 27908  |
|    total_timesteps | 272384 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 273000     |
| train/                  |            |
|    approx_kl            | 0.01772938 |
|    clip_fraction        | 0.225      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.82      |
|    explained_variance   | 0.353      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00739   |
|    n_updates            | 1330       |
|    policy_gradient_loss | -0.015     |
|    value_loss           | 0.00358    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 134    |
|    time_elapsed    | 28113  |
|    total_timesteps | 274432 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 275000      |
| train/                  |             |
|    approx_kl            | 0.020896435 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.72       |
|    explained_variance   | 0.451       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0291     |
|    n_updates            | 1340        |
|    policy_gradient_loss | -0.0184     |
|    value_loss           | 0.00466     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 135    |
|    time_elapsed    | 28318  |
|    total_timesteps | 276480 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 277000      |
| train/                  |             |
|    approx_kl            | 0.021586258 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.68       |
|    explained_variance   | 0.327       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0133     |
|    n_updates            | 1350        |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 0.00498     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 278000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 136    |
|    time_elapsed    | 28524  |
|    total_timesteps | 278528 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 279000      |
| train/                  |             |
|    approx_kl            | 0.021156589 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.72       |
|    explained_variance   | 0.472       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0377     |
|    n_updates            | 1360        |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 0.00457     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 280000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 137    |
|    time_elapsed    | 28729  |
|    total_timesteps | 280576 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 281000      |
| train/                  |             |
|    approx_kl            | 0.029272182 |
|    clip_fraction        | 0.288       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.86       |
|    explained_variance   | 0.153       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0542     |
|    n_updates            | 1370        |
|    policy_gradient_loss | -0.0246     |
|    value_loss           | 0.0052      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 282000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 138    |
|    time_elapsed    | 28934  |
|    total_timesteps | 282624 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 283000      |
| train/                  |             |
|    approx_kl            | 0.017720604 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.66       |
|    explained_variance   | 0.314       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0331     |
|    n_updates            | 1380        |
|    policy_gradient_loss | -0.02       |
|    value_loss           | 0.00492     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 284000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 139    |
|    time_elapsed    | 29140  |
|    total_timesteps | 284672 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 285000      |
| train/                  |             |
|    approx_kl            | 0.022552118 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.7        |
|    explained_variance   | 0.266       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.022      |
|    n_updates            | 1390        |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.00548     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 286000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 140    |
|    time_elapsed    | 29345  |
|    total_timesteps | 286720 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 287000      |
| train/                  |             |
|    approx_kl            | 0.023122363 |
|    clip_fraction        | 0.285       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.8        |
|    explained_variance   | 0.37        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0334     |
|    n_updates            | 1400        |
|    policy_gradient_loss | -0.0197     |
|    value_loss           | 0.00505     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 288000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 141    |
|    time_elapsed    | 29550  |
|    total_timesteps | 288768 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 289000      |
| train/                  |             |
|    approx_kl            | 0.020262506 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.57       |
|    explained_variance   | 0.454       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0288     |
|    n_updates            | 1410        |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.00362     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 290000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 142    |
|    time_elapsed    | 29756  |
|    total_timesteps | 290816 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 291000     |
| train/                  |            |
|    approx_kl            | 0.02226949 |
|    clip_fraction        | 0.28       |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.41      |
|    explained_variance   | 0.401      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.028     |
|    n_updates            | 1420       |
|    policy_gradient_loss | -0.0165    |
|    value_loss           | 0.00563    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 292000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 143    |
|    time_elapsed    | 29961  |
|    total_timesteps | 292864 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 293000      |
| train/                  |             |
|    approx_kl            | 0.022701304 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.48       |
|    explained_variance   | 0.584       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0404     |
|    n_updates            | 1430        |
|    policy_gradient_loss | -0.0267     |
|    value_loss           | 0.00396     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 294000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 144    |
|    time_elapsed    | 30166  |
|    total_timesteps | 294912 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 295000     |
| train/                  |            |
|    approx_kl            | 0.01954111 |
|    clip_fraction        | 0.21       |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.17      |
|    explained_variance   | 0.333      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00692    |
|    n_updates            | 1440       |
|    policy_gradient_loss | -0.0118    |
|    value_loss           | 0.00516    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 296000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 145    |
|    time_elapsed    | 30372  |
|    total_timesteps | 296960 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 297000      |
| train/                  |             |
|    approx_kl            | 0.017284798 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.43       |
|    explained_variance   | 0.426       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.04       |
|    n_updates            | 1450        |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 0.00561     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 298000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 146    |
|    time_elapsed    | 30677  |
|    total_timesteps | 299008 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 300000      |
| train/                  |             |
|    approx_kl            | 0.018338772 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.09       |
|    explained_variance   | 0.35        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0236     |
|    n_updates            | 1460        |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.0073      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 301000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 147    |
|    time_elapsed    | 30882  |
|    total_timesteps | 301056 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 302000      |
| train/                  |             |
|    approx_kl            | 0.021829797 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.07       |
|    explained_variance   | 0.576       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0144     |
|    n_updates            | 1470        |
|    policy_gradient_loss | -0.0216     |
|    value_loss           | 0.00599     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 303000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 148    |
|    time_elapsed    | 31088  |
|    total_timesteps | 303104 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 304000      |
| train/                  |             |
|    approx_kl            | 0.020587882 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.93       |
|    explained_variance   | 0.466       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00106    |
|    n_updates            | 1480        |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 0.00574     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 305000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 149    |
|    time_elapsed    | 31293  |
|    total_timesteps | 305152 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 1          |
| time/                   |            |
|    total_timesteps      | 306000     |
| train/                  |            |
|    approx_kl            | 0.01975409 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.91      |
|    explained_variance   | 0.513      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0151    |
|    n_updates            | 1490       |
|    policy_gradient_loss | -0.0169    |
|    value_loss           | 0.00555    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 307000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 150    |
|    time_elapsed    | 31498  |
|    total_timesteps | 307200 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 308000      |
| train/                  |             |
|    approx_kl            | 0.023247795 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.54       |
|    explained_variance   | 0.597       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00744     |
|    n_updates            | 1500        |
|    policy_gradient_loss | -0.0219     |
|    value_loss           | 0.00485     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 309000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 151    |
|    time_elapsed    | 31704  |
|    total_timesteps | 309248 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 310000     |
| train/                  |            |
|    approx_kl            | 0.01970575 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.88      |
|    explained_variance   | 0.436      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0122    |
|    n_updates            | 1510       |
|    policy_gradient_loss | -0.0172    |
|    value_loss           | 0.00648    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 311000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 152    |
|    time_elapsed    | 31909  |
|    total_timesteps | 311296 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 312000      |
| train/                  |             |
|    approx_kl            | 0.022911899 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.7        |
|    explained_variance   | 0.409       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.026      |
|    n_updates            | 1520        |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 0.00719     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 313000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 153    |
|    time_elapsed    | 32115  |
|    total_timesteps | 313344 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 314000      |
| train/                  |             |
|    approx_kl            | 0.016686756 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.81       |
|    explained_variance   | 0.445       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00797    |
|    n_updates            | 1530        |
|    policy_gradient_loss | -0.00855    |
|    value_loss           | 0.00722     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 315000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 154    |
|    time_elapsed    | 32320  |
|    total_timesteps | 315392 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 1.1        |
| time/                   |            |
|    total_timesteps      | 316000     |
| train/                  |            |
|    approx_kl            | 0.02195989 |
|    clip_fraction        | 0.249      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.55      |
|    explained_variance   | 0.486      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00482    |
|    n_updates            | 1540       |
|    policy_gradient_loss | -0.0151    |
|    value_loss           | 0.00766    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 317000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 155    |
|    time_elapsed    | 32525  |
|    total_timesteps | 317440 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 318000      |
| train/                  |             |
|    approx_kl            | 0.020573832 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.61       |
|    explained_variance   | 0.418       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0131     |
|    n_updates            | 1550        |
|    policy_gradient_loss | -0.0143     |
|    value_loss           | 0.0071      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 319000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 156    |
|    time_elapsed    | 32730  |
|    total_timesteps | 319488 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 320000      |
| train/                  |             |
|    approx_kl            | 0.026063094 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.76       |
|    explained_variance   | 0.558       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0408     |
|    n_updates            | 1560        |
|    policy_gradient_loss | -0.0193     |
|    value_loss           | 0.00461     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 321000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 157    |
|    time_elapsed    | 32936  |
|    total_timesteps | 321536 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 322000      |
| train/                  |             |
|    approx_kl            | 0.017499719 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.43       |
|    explained_variance   | 0.442       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.017      |
|    n_updates            | 1570        |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 0.0072      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 323000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 158    |
|    time_elapsed    | 33141  |
|    total_timesteps | 323584 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 324000      |
| train/                  |             |
|    approx_kl            | 0.014672522 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.77       |
|    explained_variance   | 0.404       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0181     |
|    n_updates            | 1580        |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 0.00715     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 325000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 159    |
|    time_elapsed    | 33346  |
|    total_timesteps | 325632 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 326000      |
| train/                  |             |
|    approx_kl            | 0.013397578 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.58       |
|    explained_variance   | 0.399       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00347    |
|    n_updates            | 1590        |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 0.00707     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 327000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 160    |
|    time_elapsed    | 33552  |
|    total_timesteps | 327680 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 328000      |
| train/                  |             |
|    approx_kl            | 0.020004977 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.18       |
|    explained_variance   | 0.384       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00754    |
|    n_updates            | 1600        |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 0.00714     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 329000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 161    |
|    time_elapsed    | 33757  |
|    total_timesteps | 329728 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 330000      |
| train/                  |             |
|    approx_kl            | 0.013590315 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.91       |
|    explained_variance   | 0.414       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0185     |
|    n_updates            | 1610        |
|    policy_gradient_loss | -0.00687    |
|    value_loss           | 0.00703     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 331000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 162    |
|    time_elapsed    | 33962  |
|    total_timesteps | 331776 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 332000      |
| train/                  |             |
|    approx_kl            | 0.014759896 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.65       |
|    explained_variance   | 0.349       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00072     |
|    n_updates            | 1620        |
|    policy_gradient_loss | -0.0069     |
|    value_loss           | 0.00736     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 333000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 163    |
|    time_elapsed    | 34168  |
|    total_timesteps | 333824 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.1          |
| time/                   |              |
|    total_timesteps      | 334000       |
| train/                  |              |
|    approx_kl            | 0.0150735285 |
|    clip_fraction        | 0.222        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.42        |
|    explained_variance   | 0.367        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00993     |
|    n_updates            | 1630         |
|    policy_gradient_loss | -0.0101      |
|    value_loss           | 0.00765      |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 335000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 164    |
|    time_elapsed    | 34373  |
|    total_timesteps | 335872 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 1.1        |
| time/                   |            |
|    total_timesteps      | 336000     |
| train/                  |            |
|    approx_kl            | 0.01637014 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.72      |
|    explained_variance   | 0.398      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0259    |
|    n_updates            | 1640       |
|    policy_gradient_loss | -0.00951   |
|    value_loss           | 0.00748    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 337000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 165    |
|    time_elapsed    | 34578  |
|    total_timesteps | 337920 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 338000      |
| train/                  |             |
|    approx_kl            | 0.015740938 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.379       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00134     |
|    n_updates            | 1650        |
|    policy_gradient_loss | -0.00949    |
|    value_loss           | 0.00804     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 339000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 166    |
|    time_elapsed    | 34784  |
|    total_timesteps | 339968 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 340000      |
| train/                  |             |
|    approx_kl            | 0.020299835 |
|    clip_fraction        | 0.283       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.78       |
|    explained_variance   | 0.416       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00955     |
|    n_updates            | 1660        |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 0.00798     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 341000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 342000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 167    |
|    time_elapsed    | 35089  |
|    total_timesteps | 342016 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 1.1        |
| time/                   |            |
|    total_timesteps      | 343000     |
| train/                  |            |
|    approx_kl            | 0.06994144 |
|    clip_fraction        | 0.423      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.14      |
|    explained_variance   | 0.545      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0524    |
|    n_updates            | 1670       |
|    policy_gradient_loss | -0.0288    |
|    value_loss           | 0.00882    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 344000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 168    |
|    time_elapsed    | 35294  |
|    total_timesteps | 344064 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.1          |
| time/                   |              |
|    total_timesteps      | 345000       |
| train/                  |              |
|    approx_kl            | 0.0075398367 |
|    clip_fraction        | 0.122        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.58        |
|    explained_variance   | 0.39         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00947      |
|    n_updates            | 1680         |
|    policy_gradient_loss | -0.00574     |
|    value_loss           | 0.0075       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 346000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 169    |
|    time_elapsed    | 35500  |
|    total_timesteps | 346112 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 347000      |
| train/                  |             |
|    approx_kl            | 0.012093671 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.65       |
|    explained_variance   | 0.358       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0168     |
|    n_updates            | 1690        |
|    policy_gradient_loss | -0.00625    |
|    value_loss           | 0.00781     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 348000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 170    |
|    time_elapsed    | 35705  |
|    total_timesteps | 348160 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 349000      |
| train/                  |             |
|    approx_kl            | 0.025286106 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.328       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00271     |
|    n_updates            | 1700        |
|    policy_gradient_loss | -0.00713    |
|    value_loss           | 0.00789     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 350000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 171    |
|    time_elapsed    | 35910  |
|    total_timesteps | 350208 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 351000      |
| train/                  |             |
|    approx_kl            | 0.024877619 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.604       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0252     |
|    n_updates            | 1710        |
|    policy_gradient_loss | -0.0137     |
|    value_loss           | 0.00493     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 352000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 172    |
|    time_elapsed    | 36116  |
|    total_timesteps | 352256 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 353000      |
| train/                  |             |
|    approx_kl            | 0.032657426 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.16       |
|    explained_variance   | 0.653       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0213     |
|    n_updates            | 1720        |
|    policy_gradient_loss | -0.0139     |
|    value_loss           | 0.0061      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 354000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 173    |
|    time_elapsed    | 36321  |
|    total_timesteps | 354304 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 1.1        |
| time/                   |            |
|    total_timesteps      | 355000     |
| train/                  |            |
|    approx_kl            | 0.01884002 |
|    clip_fraction        | 0.248      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.48      |
|    explained_variance   | 0.627      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0223    |
|    n_updates            | 1730       |
|    policy_gradient_loss | -0.0125    |
|    value_loss           | 0.00637    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 356000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 174    |
|    time_elapsed    | 36527  |
|    total_timesteps | 356352 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 357000      |
| train/                  |             |
|    approx_kl            | 0.023152035 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.73       |
|    explained_variance   | 0.648       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0136     |
|    n_updates            | 1740        |
|    policy_gradient_loss | -0.0085     |
|    value_loss           | 0.00626     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 358000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 175    |
|    time_elapsed    | 36732  |
|    total_timesteps | 358400 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 359000      |
| train/                  |             |
|    approx_kl            | 0.028206106 |
|    clip_fraction        | 0.314       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.25       |
|    explained_variance   | 0.485       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00635    |
|    n_updates            | 1750        |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 0.0073      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 360000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 176    |
|    time_elapsed    | 36937  |
|    total_timesteps | 360448 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 1.1        |
| time/                   |            |
|    total_timesteps      | 361000     |
| train/                  |            |
|    approx_kl            | 0.02985646 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.55      |
|    explained_variance   | 0.722      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0247    |
|    n_updates            | 1760       |
|    policy_gradient_loss | -0.0152    |
|    value_loss           | 0.00679    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 362000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 177    |
|    time_elapsed    | 37142  |
|    total_timesteps | 362496 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 363000      |
| train/                  |             |
|    approx_kl            | 0.014397755 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.51       |
|    explained_variance   | 0.435       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0183     |
|    n_updates            | 1770        |
|    policy_gradient_loss | -0.00394    |
|    value_loss           | 0.00973     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 364000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 178    |
|    time_elapsed    | 37348  |
|    total_timesteps | 364544 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 1.1        |
| time/                   |            |
|    total_timesteps      | 365000     |
| train/                  |            |
|    approx_kl            | 0.03271174 |
|    clip_fraction        | 0.265      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.4       |
|    explained_variance   | 0.763      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0188    |
|    n_updates            | 1780       |
|    policy_gradient_loss | -0.0238    |
|    value_loss           | 0.00687    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 366000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 179    |
|    time_elapsed    | 37553  |
|    total_timesteps | 366592 |
-------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+04     |
|    mean_reward          | 1.1       |
| time/                   |           |
|    total_timesteps      | 367000    |
| train/                  |           |
|    approx_kl            | 0.0176171 |
|    clip_fraction        | 0.185     |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.87     |
|    explained_variance   | 0.752     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0157   |
|    n_updates            | 1790      |
|    policy_gradient_loss | -0.0176   |
|    value_loss           | 0.00444   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 368000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 180    |
|    time_elapsed    | 37758  |
|    total_timesteps | 368640 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 1.2        |
| time/                   |            |
|    total_timesteps      | 369000     |
| train/                  |            |
|    approx_kl            | 0.01836897 |
|    clip_fraction        | 0.202      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.72      |
|    explained_variance   | 0.525      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0186    |
|    n_updates            | 1800       |
|    policy_gradient_loss | -0.00842   |
|    value_loss           | 0.00688    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.2      |
| time/              |          |
|    total_timesteps | 370000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 181    |
|    time_elapsed    | 37964  |
|    total_timesteps | 370688 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.2         |
| time/                   |             |
|    total_timesteps      | 371000      |
| train/                  |             |
|    approx_kl            | 0.014884166 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.89       |
|    explained_variance   | 0.77        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00358     |
|    n_updates            | 1810        |
|    policy_gradient_loss | -0.00827    |
|    value_loss           | 0.00449     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.2      |
| time/              |          |
|    total_timesteps | 372000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 182    |
|    time_elapsed    | 38169  |
|    total_timesteps | 372736 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 1.2        |
| time/                   |            |
|    total_timesteps      | 373000     |
| train/                  |            |
|    approx_kl            | 0.01114797 |
|    clip_fraction        | 0.125      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.39      |
|    explained_variance   | 0.461      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0122    |
|    n_updates            | 1820       |
|    policy_gradient_loss | -0.00743   |
|    value_loss           | 0.00787    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.2      |
| time/              |          |
|    total_timesteps | 374000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 183    |
|    time_elapsed    | 38374  |
|    total_timesteps | 374784 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.2         |
| time/                   |             |
|    total_timesteps      | 375000      |
| train/                  |             |
|    approx_kl            | 0.013418522 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.39       |
|    explained_variance   | 0.637       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00275     |
|    n_updates            | 1830        |
|    policy_gradient_loss | -0.00416    |
|    value_loss           | 0.00573     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.2      |
| time/              |          |
|    total_timesteps | 376000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 184    |
|    time_elapsed    | 38580  |
|    total_timesteps | 376832 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.3         |
| time/                   |             |
|    total_timesteps      | 377000      |
| train/                  |             |
|    approx_kl            | 0.020428095 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | 0.46        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00243    |
|    n_updates            | 1840        |
|    policy_gradient_loss | -0.00957    |
|    value_loss           | 0.00841     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 378000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 185    |
|    time_elapsed    | 38785  |
|    total_timesteps | 378880 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 0.9          |
| time/                   |              |
|    total_timesteps      | 379000       |
| train/                  |              |
|    approx_kl            | 0.0109842345 |
|    clip_fraction        | 0.134        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.54        |
|    explained_variance   | 0.472        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0123      |
|    n_updates            | 1850         |
|    policy_gradient_loss | -0.00719     |
|    value_loss           | 0.00918      |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 380000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 186    |
|    time_elapsed    | 38990  |
|    total_timesteps | 380928 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.3         |
| time/                   |             |
|    total_timesteps      | 381000      |
| train/                  |             |
|    approx_kl            | 0.008929746 |
|    clip_fraction        | 0.0901      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.441       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000123    |
|    n_updates            | 1860        |
|    policy_gradient_loss | -0.00215    |
|    value_loss           | 0.0092      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 382000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 187    |
|    time_elapsed    | 39196  |
|    total_timesteps | 382976 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.3          |
| time/                   |              |
|    total_timesteps      | 383000       |
| train/                  |              |
|    approx_kl            | 0.0047333203 |
|    clip_fraction        | 0.0497       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.731       |
|    explained_variance   | 0.447        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00954     |
|    n_updates            | 1870         |
|    policy_gradient_loss | -0.00983     |
|    value_loss           | 0.00853      |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 384000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 385000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 188    |
|    time_elapsed    | 39501  |
|    total_timesteps | 385024 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.3         |
| time/                   |             |
|    total_timesteps      | 386000      |
| train/                  |             |
|    approx_kl            | 0.013032276 |
|    clip_fraction        | 0.0928      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.51        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0233      |
|    n_updates            | 1880        |
|    policy_gradient_loss | -0.00574    |
|    value_loss           | 0.0132      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 387000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 189    |
|    time_elapsed    | 39706  |
|    total_timesteps | 387072 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.3         |
| time/                   |             |
|    total_timesteps      | 388000      |
| train/                  |             |
|    approx_kl            | 0.026222903 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.506       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0129      |
|    n_updates            | 1890        |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.00979     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 389000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 190    |
|    time_elapsed    | 39912  |
|    total_timesteps | 389120 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.3         |
| time/                   |             |
|    total_timesteps      | 390000      |
| train/                  |             |
|    approx_kl            | 0.009491954 |
|    clip_fraction        | 0.0858      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.46        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0115     |
|    n_updates            | 1900        |
|    policy_gradient_loss | -0.00258    |
|    value_loss           | 0.0102      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 391000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 191    |
|    time_elapsed    | 40117  |
|    total_timesteps | 391168 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.3         |
| time/                   |             |
|    total_timesteps      | 392000      |
| train/                  |             |
|    approx_kl            | 0.007669042 |
|    clip_fraction        | 0.0871      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.368       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0116     |
|    n_updates            | 1910        |
|    policy_gradient_loss | -0.00376    |
|    value_loss           | 0.0107      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 393000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 192    |
|    time_elapsed    | 40322  |
|    total_timesteps | 393216 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 1.3        |
| time/                   |            |
|    total_timesteps      | 394000     |
| train/                  |            |
|    approx_kl            | 0.00826986 |
|    clip_fraction        | 0.0992     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.22      |
|    explained_variance   | 0.384      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0126    |
|    n_updates            | 1920       |
|    policy_gradient_loss | -0.00416   |
|    value_loss           | 0.0102     |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 395000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 193    |
|    time_elapsed    | 40528  |
|    total_timesteps | 395264 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.3         |
| time/                   |             |
|    total_timesteps      | 396000      |
| train/                  |             |
|    approx_kl            | 0.008088561 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.86       |
|    explained_variance   | 0.392       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0382      |
|    n_updates            | 1930        |
|    policy_gradient_loss | -0.00325    |
|    value_loss           | 0.00984     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 397000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 194    |
|    time_elapsed    | 40733  |
|    total_timesteps | 397312 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.3         |
| time/                   |             |
|    total_timesteps      | 398000      |
| train/                  |             |
|    approx_kl            | 0.009876203 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.4         |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0104      |
|    n_updates            | 1940        |
|    policy_gradient_loss | -0.00537    |
|    value_loss           | 0.0104      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 399000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 195    |
|    time_elapsed    | 40938  |
|    total_timesteps | 399360 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 1.3        |
| time/                   |            |
|    total_timesteps      | 400000     |
| train/                  |            |
|    approx_kl            | 0.01759577 |
|    clip_fraction        | 0.161      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.54      |
|    explained_variance   | 0.343      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00775   |
|    n_updates            | 1950       |
|    policy_gradient_loss | -0.00613   |
|    value_loss           | 0.00986    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 401000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 196    |
|    time_elapsed    | 41144  |
|    total_timesteps | 401408 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.3         |
| time/                   |             |
|    total_timesteps      | 402000      |
| train/                  |             |
|    approx_kl            | 0.007986572 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.61       |
|    explained_variance   | 0.373       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0168     |
|    n_updates            | 1960        |
|    policy_gradient_loss | -0.00242    |
|    value_loss           | 0.0101      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 403000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 197    |
|    time_elapsed    | 41349  |
|    total_timesteps | 403456 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.3         |
| time/                   |             |
|    total_timesteps      | 404000      |
| train/                  |             |
|    approx_kl            | 0.006129164 |
|    clip_fraction        | 0.0873      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.393       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00211     |
|    n_updates            | 1970        |
|    policy_gradient_loss | -0.00233    |
|    value_loss           | 0.0101      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 405000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 198    |
|    time_elapsed    | 41554  |
|    total_timesteps | 405504 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.3         |
| time/                   |             |
|    total_timesteps      | 406000      |
| train/                  |             |
|    approx_kl            | 0.009282449 |
|    clip_fraction        | 0.0873      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.352       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0207      |
|    n_updates            | 1980        |
|    policy_gradient_loss | -0.00279    |
|    value_loss           | 0.0107      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 407000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 199    |
|    time_elapsed    | 41759  |
|    total_timesteps | 407552 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.3          |
| time/                   |              |
|    total_timesteps      | 408000       |
| train/                  |              |
|    approx_kl            | 0.0075678714 |
|    clip_fraction        | 0.136        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.24        |
|    explained_variance   | 0.367        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0111      |
|    n_updates            | 1990         |
|    policy_gradient_loss | -0.00392     |
|    value_loss           | 0.0109       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 409000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 200    |
|    time_elapsed    | 41965  |
|    total_timesteps | 409600 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.3          |
| time/                   |              |
|    total_timesteps      | 410000       |
| train/                  |              |
|    approx_kl            | 0.0076489905 |
|    clip_fraction        | 0.116        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.19        |
|    explained_variance   | 0.403        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0147      |
|    n_updates            | 2000         |
|    policy_gradient_loss | -0.00542     |
|    value_loss           | 0.0105       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 411000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 201    |
|    time_elapsed    | 42170  |
|    total_timesteps | 411648 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.3          |
| time/                   |              |
|    total_timesteps      | 412000       |
| train/                  |              |
|    approx_kl            | 0.0072334497 |
|    clip_fraction        | 0.0857       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.13        |
|    explained_variance   | 0.352        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00085     |
|    n_updates            | 2010         |
|    policy_gradient_loss | -0.00214     |
|    value_loss           | 0.0107       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 413000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 202    |
|    time_elapsed    | 42376  |
|    total_timesteps | 413696 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.3         |
| time/                   |             |
|    total_timesteps      | 414000      |
| train/                  |             |
|    approx_kl            | 0.012704382 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.03       |
|    explained_variance   | 0.325       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0153     |
|    n_updates            | 2020        |
|    policy_gradient_loss | -0.00414    |
|    value_loss           | 0.01        |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 415000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 203    |
|    time_elapsed    | 42581  |
|    total_timesteps | 415744 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.3         |
| time/                   |             |
|    total_timesteps      | 416000      |
| train/                  |             |
|    approx_kl            | 0.009119186 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.358       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0182     |
|    n_updates            | 2030        |
|    policy_gradient_loss | -0.00467    |
|    value_loss           | 0.0107      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 417000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 204    |
|    time_elapsed    | 42786  |
|    total_timesteps | 417792 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.3         |
| time/                   |             |
|    total_timesteps      | 418000      |
| train/                  |             |
|    approx_kl            | 0.011881383 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.998      |
|    explained_variance   | 0.38        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0035     |
|    n_updates            | 2040        |
|    policy_gradient_loss | -0.00682    |
|    value_loss           | 0.0105      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 419000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 205    |
|    time_elapsed    | 42992  |
|    total_timesteps | 419840 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 1.2        |
| time/                   |            |
|    total_timesteps      | 420000     |
| train/                  |            |
|    approx_kl            | 0.00868118 |
|    clip_fraction        | 0.101      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.93      |
|    explained_variance   | 0.34       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.000282   |
|    n_updates            | 2050       |
|    policy_gradient_loss | -0.00224   |
|    value_loss           | 0.0108     |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.2      |
| time/              |          |
|    total_timesteps | 421000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 206    |
|    time_elapsed    | 43197  |
|    total_timesteps | 421888 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.3         |
| time/                   |             |
|    total_timesteps      | 422000      |
| train/                  |             |
|    approx_kl            | 0.005566008 |
|    clip_fraction        | 0.0552      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.923      |
|    explained_variance   | 0.364       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00747     |
|    n_updates            | 2060        |
|    policy_gradient_loss | -0.00242    |
|    value_loss           | 0.0106      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 423000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 207    |
|    time_elapsed    | 43402  |
|    total_timesteps | 423936 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 424000      |
| train/                  |             |
|    approx_kl            | 0.005455184 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.852      |
|    explained_variance   | 0.378       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00781    |
|    n_updates            | 2070        |
|    policy_gradient_loss | -0.0022     |
|    value_loss           | 0.0105      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 425000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 208    |
|    time_elapsed    | 43608  |
|    total_timesteps | 425984 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.3          |
| time/                   |              |
|    total_timesteps      | 426000       |
| train/                  |              |
|    approx_kl            | 0.0069056056 |
|    clip_fraction        | 0.0949       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.782       |
|    explained_variance   | 0.347        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000655     |
|    n_updates            | 2080         |
|    policy_gradient_loss | -0.0022      |
|    value_loss           | 0.0107       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 427000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 428000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 209    |
|    time_elapsed    | 43913  |
|    total_timesteps | 428032 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 1.2        |
| time/                   |            |
|    total_timesteps      | 429000     |
| train/                  |            |
|    approx_kl            | 0.00499091 |
|    clip_fraction        | 0.0716     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.734     |
|    explained_variance   | 0.345      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00739   |
|    n_updates            | 2090       |
|    policy_gradient_loss | -0.00122   |
|    value_loss           | 0.0151     |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.2      |
| time/              |          |
|    total_timesteps | 430000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 210    |
|    time_elapsed    | 44118  |
|    total_timesteps | 430080 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.3          |
| time/                   |              |
|    total_timesteps      | 431000       |
| train/                  |              |
|    approx_kl            | 0.0046175523 |
|    clip_fraction        | 0.0376       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.707       |
|    explained_variance   | 0.367        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00143     |
|    n_updates            | 2100         |
|    policy_gradient_loss | -0.00134     |
|    value_loss           | 0.0107       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 432000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 211    |
|    time_elapsed    | 44324  |
|    total_timesteps | 432128 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.3          |
| time/                   |              |
|    total_timesteps      | 433000       |
| train/                  |              |
|    approx_kl            | 0.0053364015 |
|    clip_fraction        | 0.0566       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.585       |
|    explained_variance   | 0.348        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0101      |
|    n_updates            | 2110         |
|    policy_gradient_loss | -0.00116     |
|    value_loss           | 0.0108       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 434000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 212    |
|    time_elapsed    | 44529  |
|    total_timesteps | 434176 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.3          |
| time/                   |              |
|    total_timesteps      | 435000       |
| train/                  |              |
|    approx_kl            | 0.0021449083 |
|    clip_fraction        | 0.0264       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.557       |
|    explained_variance   | 0.346        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0132       |
|    n_updates            | 2120         |
|    policy_gradient_loss | -0.000371    |
|    value_loss           | 0.0108       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 436000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 213    |
|    time_elapsed    | 44734  |
|    total_timesteps | 436224 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.3         |
| time/                   |             |
|    total_timesteps      | 437000      |
| train/                  |             |
|    approx_kl            | 0.004541185 |
|    clip_fraction        | 0.0613      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.478      |
|    explained_variance   | 0.338       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0144      |
|    n_updates            | 2130        |
|    policy_gradient_loss | -0.00164    |
|    value_loss           | 0.011       |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 438000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 214    |
|    time_elapsed    | 44940  |
|    total_timesteps | 438272 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.3         |
| time/                   |             |
|    total_timesteps      | 439000      |
| train/                  |             |
|    approx_kl            | 0.002443974 |
|    clip_fraction        | 0.0444      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.42       |
|    explained_variance   | 0.384       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000176    |
|    n_updates            | 2140        |
|    policy_gradient_loss | -0.00144    |
|    value_loss           | 0.0107      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 440000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 215    |
|    time_elapsed    | 45145  |
|    total_timesteps | 440320 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.3          |
| time/                   |              |
|    total_timesteps      | 441000       |
| train/                  |              |
|    approx_kl            | 0.0016722919 |
|    clip_fraction        | 0.0219       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.4         |
|    explained_variance   | 0.357        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00545      |
|    n_updates            | 2150         |
|    policy_gradient_loss | -0.00185     |
|    value_loss           | 0.0109       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 442000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 216    |
|    time_elapsed    | 45350  |
|    total_timesteps | 442368 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.3         |
| time/                   |             |
|    total_timesteps      | 443000      |
| train/                  |             |
|    approx_kl            | 0.002893327 |
|    clip_fraction        | 0.0359      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.375      |
|    explained_variance   | 0.346       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0166      |
|    n_updates            | 2160        |
|    policy_gradient_loss | -0.00151    |
|    value_loss           | 0.0109      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 444000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 217    |
|    time_elapsed    | 45556  |
|    total_timesteps | 444416 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.3          |
| time/                   |              |
|    total_timesteps      | 445000       |
| train/                  |              |
|    approx_kl            | 0.0070610372 |
|    clip_fraction        | 0.0641       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.291       |
|    explained_variance   | 0.337        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00262      |
|    n_updates            | 2170         |
|    policy_gradient_loss | -0.00235     |
|    value_loss           | 0.011        |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 446000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 218    |
|    time_elapsed    | 45761  |
|    total_timesteps | 446464 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 1.3         |
| time/                   |             |
|    total_timesteps      | 447000      |
| train/                  |             |
|    approx_kl            | 0.001723413 |
|    clip_fraction        | 0.0256      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.233      |
|    explained_variance   | 0.332       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00784     |
|    n_updates            | 2180        |
|    policy_gradient_loss | -0.000856   |
|    value_loss           | 0.0111      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 448000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 219    |
|    time_elapsed    | 45966  |
|    total_timesteps | 448512 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.3          |
| time/                   |              |
|    total_timesteps      | 449000       |
| train/                  |              |
|    approx_kl            | 0.0014790043 |
|    clip_fraction        | 0.0165       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.218       |
|    explained_variance   | 0.333        |
|    learning_rate        | 0.0003       |
|    loss                 | -3.43e-05    |
|    n_updates            | 2190         |
|    policy_gradient_loss | -0.00123     |
|    value_loss           | 0.011        |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 450000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 220    |
|    time_elapsed    | 46172  |
|    total_timesteps | 450560 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.3          |
| time/                   |              |
|    total_timesteps      | 451000       |
| train/                  |              |
|    approx_kl            | 0.0011279134 |
|    clip_fraction        | 0.0117       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.204       |
|    explained_variance   | 0.332        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0055      |
|    n_updates            | 2200         |
|    policy_gradient_loss | -0.000167    |
|    value_loss           | 0.0111       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 452000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 221    |
|    time_elapsed    | 46377  |
|    total_timesteps | 452608 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 1.3           |
| time/                   |               |
|    total_timesteps      | 453000        |
| train/                  |               |
|    approx_kl            | 0.00087345485 |
|    clip_fraction        | 0.0184        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.177        |
|    explained_variance   | 0.334         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00204       |
|    n_updates            | 2210          |
|    policy_gradient_loss | -0.000429     |
|    value_loss           | 0.0111        |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 454000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 222    |
|    time_elapsed    | 46582  |
|    total_timesteps | 454656 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 1.3           |
| time/                   |               |
|    total_timesteps      | 455000        |
| train/                  |               |
|    approx_kl            | 0.00090584334 |
|    clip_fraction        | 0.0187        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.171        |
|    explained_variance   | 0.343         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000359      |
|    n_updates            | 2220          |
|    policy_gradient_loss | -0.00102      |
|    value_loss           | 0.0112        |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 456000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 223    |
|    time_elapsed    | 46788  |
|    total_timesteps | 456704 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.3          |
| time/                   |              |
|    total_timesteps      | 457000       |
| train/                  |              |
|    approx_kl            | 0.0022555846 |
|    clip_fraction        | 0.0219       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.145       |
|    explained_variance   | 0.341        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00196      |
|    n_updates            | 2230         |
|    policy_gradient_loss | -0.000545    |
|    value_loss           | 0.011        |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 458000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 224    |
|    time_elapsed    | 46993  |
|    total_timesteps | 458752 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 1.3           |
| time/                   |               |
|    total_timesteps      | 459000        |
| train/                  |               |
|    approx_kl            | 0.00055260037 |
|    clip_fraction        | 0.0109        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.131        |
|    explained_variance   | 0.333         |
|    learning_rate        | 0.0003        |
|    loss                 | 9.46e-05      |
|    n_updates            | 2240          |
|    policy_gradient_loss | -0.000399     |
|    value_loss           | 0.0111        |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 460000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 225    |
|    time_elapsed    | 47198  |
|    total_timesteps | 460800 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 1.3           |
| time/                   |               |
|    total_timesteps      | 461000        |
| train/                  |               |
|    approx_kl            | 0.00083110813 |
|    clip_fraction        | 0.0122        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.112        |
|    explained_variance   | 0.339         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00176       |
|    n_updates            | 2250          |
|    policy_gradient_loss | -0.000426     |
|    value_loss           | 0.011         |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 462000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 226    |
|    time_elapsed    | 47404  |
|    total_timesteps | 462848 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.3          |
| time/                   |              |
|    total_timesteps      | 463000       |
| train/                  |              |
|    approx_kl            | 0.0010278382 |
|    clip_fraction        | 0.0104       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.1         |
|    explained_variance   | 0.332        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00926      |
|    n_updates            | 2260         |
|    policy_gradient_loss | -0.000899    |
|    value_loss           | 0.0111       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 464000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 227    |
|    time_elapsed    | 47609  |
|    total_timesteps | 464896 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.3          |
| time/                   |              |
|    total_timesteps      | 465000       |
| train/                  |              |
|    approx_kl            | 0.0016615498 |
|    clip_fraction        | 0.0118       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0819      |
|    explained_variance   | 0.346        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0117       |
|    n_updates            | 2270         |
|    policy_gradient_loss | -0.000382    |
|    value_loss           | 0.0111       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 466000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 228    |
|    time_elapsed    | 47814  |
|    total_timesteps | 466944 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.3          |
| time/                   |              |
|    total_timesteps      | 467000       |
| train/                  |              |
|    approx_kl            | 0.0004867561 |
|    clip_fraction        | 0.00801      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.069       |
|    explained_variance   | 0.333        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00242      |
|    n_updates            | 2280         |
|    policy_gradient_loss | -0.000403    |
|    value_loss           | 0.0111       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 468000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 229    |
|    time_elapsed    | 48020  |
|    total_timesteps | 468992 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 1.3           |
| time/                   |               |
|    total_timesteps      | 469000        |
| train/                  |               |
|    approx_kl            | 0.00032271678 |
|    clip_fraction        | 0.00713       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0575       |
|    explained_variance   | 0.332         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000565     |
|    n_updates            | 2290          |
|    policy_gradient_loss | -0.000266     |
|    value_loss           | 0.0111        |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 470000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 471000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 230    |
|    time_elapsed    | 48325  |
|    total_timesteps | 471040 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.3          |
| time/                   |              |
|    total_timesteps      | 472000       |
| train/                  |              |
|    approx_kl            | 0.0004997805 |
|    clip_fraction        | 0.00571      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0587      |
|    explained_variance   | 0.369        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000196    |
|    n_updates            | 2300         |
|    policy_gradient_loss | -0.00061     |
|    value_loss           | 0.0151       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 473000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 231    |
|    time_elapsed    | 48530  |
|    total_timesteps | 473088 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 1.3           |
| time/                   |               |
|    total_timesteps      | 474000        |
| train/                  |               |
|    approx_kl            | 0.00052812684 |
|    clip_fraction        | 0.00532       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0461       |
|    explained_variance   | 0.338         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0201        |
|    n_updates            | 2310          |
|    policy_gradient_loss | -0.000203     |
|    value_loss           | 0.011         |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 475000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 232    |
|    time_elapsed    | 48736  |
|    total_timesteps | 475136 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 1.3           |
| time/                   |               |
|    total_timesteps      | 476000        |
| train/                  |               |
|    approx_kl            | 0.00023726688 |
|    clip_fraction        | 0.00391       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0416       |
|    explained_variance   | 0.332         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000649      |
|    n_updates            | 2320          |
|    policy_gradient_loss | -0.000183     |
|    value_loss           | 0.0111        |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 477000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 233    |
|    time_elapsed    | 48941  |
|    total_timesteps | 477184 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.3          |
| time/                   |              |
|    total_timesteps      | 478000       |
| train/                  |              |
|    approx_kl            | 0.0008902262 |
|    clip_fraction        | 0.00581      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0425      |
|    explained_variance   | 0.332        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000762     |
|    n_updates            | 2330         |
|    policy_gradient_loss | -0.000416    |
|    value_loss           | 0.0111       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 479000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 234    |
|    time_elapsed    | 49146  |
|    total_timesteps | 479232 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.3          |
| time/                   |              |
|    total_timesteps      | 480000       |
| train/                  |              |
|    approx_kl            | 0.0019112881 |
|    clip_fraction        | 0.00459      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0545      |
|    explained_variance   | 0.34         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.003        |
|    n_updates            | 2340         |
|    policy_gradient_loss | -0.000612    |
|    value_loss           | 0.0111       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 481000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 235    |
|    time_elapsed    | 49352  |
|    total_timesteps | 481280 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.3          |
| time/                   |              |
|    total_timesteps      | 482000       |
| train/                  |              |
|    approx_kl            | 0.0002962973 |
|    clip_fraction        | 0.0041       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0433      |
|    explained_variance   | 0.331        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0015       |
|    n_updates            | 2350         |
|    policy_gradient_loss | -0.000194    |
|    value_loss           | 0.0112       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 483000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 236    |
|    time_elapsed    | 49557  |
|    total_timesteps | 483328 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 1.3           |
| time/                   |               |
|    total_timesteps      | 484000        |
| train/                  |               |
|    approx_kl            | 0.00011728922 |
|    clip_fraction        | 0.00161       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0443       |
|    explained_variance   | 0.333         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00941       |
|    n_updates            | 2360          |
|    policy_gradient_loss | 0.000107      |
|    value_loss           | 0.0111        |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 485000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 237    |
|    time_elapsed    | 49762  |
|    total_timesteps | 485376 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.3          |
| time/                   |              |
|    total_timesteps      | 486000       |
| train/                  |              |
|    approx_kl            | 0.0043803905 |
|    clip_fraction        | 0.00576      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0354      |
|    explained_variance   | 0.346        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00123     |
|    n_updates            | 2370         |
|    policy_gradient_loss | -7.06e-05    |
|    value_loss           | 0.0111       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 487000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 238    |
|    time_elapsed    | 49968  |
|    total_timesteps | 487424 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.3          |
| time/                   |              |
|    total_timesteps      | 488000       |
| train/                  |              |
|    approx_kl            | 0.0011141144 |
|    clip_fraction        | 0.00376      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0348      |
|    explained_variance   | 0.332        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0143       |
|    n_updates            | 2380         |
|    policy_gradient_loss | -0.000798    |
|    value_loss           | 0.0111       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 489000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 239    |
|    time_elapsed    | 50173  |
|    total_timesteps | 489472 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.3          |
| time/                   |              |
|    total_timesteps      | 490000       |
| train/                  |              |
|    approx_kl            | 0.0009239586 |
|    clip_fraction        | 0.00537      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0309      |
|    explained_variance   | 0.332        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00767      |
|    n_updates            | 2390         |
|    policy_gradient_loss | -0.00023     |
|    value_loss           | 0.0112       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 491000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 240    |
|    time_elapsed    | 50378  |
|    total_timesteps | 491520 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 1.3           |
| time/                   |               |
|    total_timesteps      | 492000        |
| train/                  |               |
|    approx_kl            | 0.00025107284 |
|    clip_fraction        | 0.00317       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0262       |
|    explained_variance   | 0.332         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000934      |
|    n_updates            | 2400          |
|    policy_gradient_loss | -9.5e-05      |
|    value_loss           | 0.0112        |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 493000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 241    |
|    time_elapsed    | 50584  |
|    total_timesteps | 493568 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.3          |
| time/                   |              |
|    total_timesteps      | 494000       |
| train/                  |              |
|    approx_kl            | 0.0013343919 |
|    clip_fraction        | 0.00347      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0401      |
|    explained_variance   | 0.337        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.011        |
|    n_updates            | 2410         |
|    policy_gradient_loss | -0.000512    |
|    value_loss           | 0.011        |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 495000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 242    |
|    time_elapsed    | 50789  |
|    total_timesteps | 495616 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.3          |
| time/                   |              |
|    total_timesteps      | 496000       |
| train/                  |              |
|    approx_kl            | 0.0007498263 |
|    clip_fraction        | 0.00522      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0453      |
|    explained_variance   | 0.342        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.75e-05     |
|    n_updates            | 2420         |
|    policy_gradient_loss | -0.00065     |
|    value_loss           | 0.0111       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 497000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 243    |
|    time_elapsed    | 50994  |
|    total_timesteps | 497664 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 1.3          |
| time/                   |              |
|    total_timesteps      | 498000       |
| train/                  |              |
|    approx_kl            | 0.0006237233 |
|    clip_fraction        | 0.00537      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0348      |
|    explained_variance   | 0.332        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000392     |
|    n_updates            | 2430         |
|    policy_gradient_loss | -0.000345    |
|    value_loss           | 0.0111       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 499000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 244    |
|    time_elapsed    | 51200  |
|    total_timesteps | 499712 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 1.3           |
| time/                   |               |
|    total_timesteps      | 500000        |
| train/                  |               |
|    approx_kl            | 0.00038647873 |
|    clip_fraction        | 0.00425       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.028        |
|    explained_variance   | 0.332         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000939      |
|    n_updates            | 2440          |
|    policy_gradient_loss | -0.000185     |
|    value_loss           | 0.0112        |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 501000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 245    |
|    time_elapsed    | 51405  |
|    total_timesteps | 501760 |
-------------------------------
