Logging to ./Logging/PPO_Forward_Baseline_10_200
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.75     |
| time/              |          |
|    fps             | 84       |
|    iterations      | 1        |
|    time_elapsed    | 24       |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 3000        |
| train/                  |             |
|    approx_kl            | 0.013358641 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.54       |
|    explained_variance   | 0.875       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0198     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0243     |
|    value_loss           | 0.0172      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.675    |
| time/              |          |
|    fps             | 82       |
|    iterations      | 2        |
|    time_elapsed    | 49       |
|    total_timesteps | 4096     |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.01719651 |
|    clip_fraction        | 0.189      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.53      |
|    explained_variance   | 0.851      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.000827  |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.022     |
|    value_loss           | 0.0128     |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.667    |
| time/              |          |
|    fps             | 82       |
|    iterations      | 3        |
|    time_elapsed    | 74       |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 7000        |
| train/                  |             |
|    approx_kl            | 0.017425299 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.52       |
|    explained_variance   | 0.823       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00824    |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0294     |
|    value_loss           | 0.00548     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.6      |
| time/              |          |
|    fps             | 81       |
|    iterations      | 4        |
|    time_elapsed    | 100      |
|    total_timesteps | 8192     |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 9000        |
| train/                  |             |
|    approx_kl            | 0.015835464 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.5        |
|    explained_variance   | 0.865       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0473     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0237     |
|    value_loss           | 0.00482     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.61     |
| time/              |          |
|    fps             | 81       |
|    iterations      | 5        |
|    time_elapsed    | 125      |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 11000       |
| train/                  |             |
|    approx_kl            | 0.020036422 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.5        |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0513     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0254     |
|    value_loss           | 0.00693     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.617    |
| time/              |          |
|    fps             | 81       |
|    iterations      | 6        |
|    time_elapsed    | 150      |
|    total_timesteps | 12288    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 13000       |
| train/                  |             |
|    approx_kl            | 0.013076188 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.48       |
|    explained_variance   | 0.782       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00436    |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0184     |
|    value_loss           | 0.00204     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.629    |
| time/              |          |
|    fps             | 81       |
|    iterations      | 7        |
|    time_elapsed    | 176      |
|    total_timesteps | 14336    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.015695248 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.48       |
|    explained_variance   | 0.853       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.059      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0263     |
|    value_loss           | 0.00172     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.656    |
| time/              |          |
|    fps             | 81       |
|    iterations      | 8        |
|    time_elapsed    | 201      |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 17000       |
| train/                  |             |
|    approx_kl            | 0.013743241 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.45       |
|    explained_variance   | 0.697       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0356     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0268     |
|    value_loss           | 0.00283     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.65     |
| time/              |          |
|    fps             | 81       |
|    iterations      | 9        |
|    time_elapsed    | 226      |
|    total_timesteps | 18432    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 19000       |
| train/                  |             |
|    approx_kl            | 0.017248232 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.42       |
|    explained_variance   | 0.828       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0334     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0217     |
|    value_loss           | 0.00174     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.64     |
| time/              |          |
|    fps             | 81       |
|    iterations      | 10       |
|    time_elapsed    | 252      |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 21000       |
| train/                  |             |
|    approx_kl            | 0.016327724 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.43       |
|    explained_variance   | 0.648       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0372     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0247     |
|    value_loss           | 0.00524     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.645    |
| time/              |          |
|    fps             | 81       |
|    iterations      | 11       |
|    time_elapsed    | 277      |
|    total_timesteps | 22528    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 23000       |
| train/                  |             |
|    approx_kl            | 0.018432226 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.43       |
|    explained_variance   | 0.778       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00642     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0294     |
|    value_loss           | 0.002       |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.65     |
| time/              |          |
|    fps             | 81       |
|    iterations      | 12       |
|    time_elapsed    | 302      |
|    total_timesteps | 24576    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.018616233 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.44       |
|    explained_variance   | 0.831       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0133     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0316     |
|    value_loss           | 0.00209     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.638    |
| time/              |          |
|    fps             | 81       |
|    iterations      | 13       |
|    time_elapsed    | 328      |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 27000       |
| train/                  |             |
|    approx_kl            | 0.018675044 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.37       |
|    explained_variance   | 0.692       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0157     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0208     |
|    value_loss           | 0.00101     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.643    |
| time/              |          |
|    fps             | 81       |
|    iterations      | 14       |
|    time_elapsed    | 353      |
|    total_timesteps | 28672    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 29000       |
| train/                  |             |
|    approx_kl            | 0.015473733 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.41       |
|    explained_variance   | 0.725       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0449     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.025      |
|    value_loss           | 0.00154     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.647    |
| time/              |          |
|    fps             | 81       |
|    iterations      | 15       |
|    time_elapsed    | 378      |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 31000       |
| train/                  |             |
|    approx_kl            | 0.015213008 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.36       |
|    explained_variance   | 0.808       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0176     |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0241     |
|    value_loss           | 0.00231     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.65     |
| time/              |          |
|    fps             | 81       |
|    iterations      | 16       |
|    time_elapsed    | 403      |
|    total_timesteps | 32768    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 33000       |
| train/                  |             |
|    approx_kl            | 0.014538655 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.37       |
|    explained_variance   | 0.854       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0604     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0252     |
|    value_loss           | 0.00127     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.656    |
| time/              |          |
|    fps             | 81       |
|    iterations      | 17       |
|    time_elapsed    | 429      |
|    total_timesteps | 34816    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.016071673 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.37       |
|    explained_variance   | 0.865       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0305     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0274     |
|    value_loss           | 0.00198     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.653    |
| time/              |          |
|    fps             | 81       |
|    iterations      | 18       |
|    time_elapsed    | 454      |
|    total_timesteps | 36864    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 37000      |
| train/                  |            |
|    approx_kl            | 0.02085312 |
|    clip_fraction        | 0.197      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.37      |
|    explained_variance   | 0.843      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0157    |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0258    |
|    value_loss           | 0.00133    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 989      |
|    ep_rew_mean     | 0.659    |
| time/              |          |
|    fps             | 81       |
|    iterations      | 19       |
|    time_elapsed    | 479      |
|    total_timesteps | 38912    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 39000       |
| train/                  |             |
|    approx_kl            | 0.014118567 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.31       |
|    explained_variance   | 0.732       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0578     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0253     |
|    value_loss           | 0.00159     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.672    |
| time/              |          |
|    fps             | 81       |
|    iterations      | 20       |
|    time_elapsed    | 505      |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 41000       |
| train/                  |             |
|    approx_kl            | 0.019110823 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.35       |
|    explained_variance   | 0.843       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00152     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0305     |
|    value_loss           | 0.000848    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.667    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 21       |
|    time_elapsed    | 540      |
|    total_timesteps | 43008    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 44000       |
| train/                  |             |
|    approx_kl            | 0.016017571 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.31       |
|    explained_variance   | 0.834       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000835    |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0228     |
|    value_loss           | 0.00156     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.671    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 22       |
|    time_elapsed    | 565      |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 46000       |
| train/                  |             |
|    approx_kl            | 0.016985752 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.29       |
|    explained_variance   | 0.791       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0489     |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0258     |
|    value_loss           | 0.00102     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.672    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 23       |
|    time_elapsed    | 591      |
|    total_timesteps | 47104    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 48000       |
| train/                  |             |
|    approx_kl            | 0.013512928 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.29       |
|    explained_variance   | 0.795       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0447     |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0214     |
|    value_loss           | 0.00249     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.671    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 24       |
|    time_elapsed    | 616      |
|    total_timesteps | 49152    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.019488214 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.29       |
|    explained_variance   | 0.889       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0306     |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0275     |
|    value_loss           | 0.000858    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.676    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 25       |
|    time_elapsed    | 641      |
|    total_timesteps | 51200    |
---------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+03     |
|    mean_reward          | 0         |
| time/                   |           |
|    total_timesteps      | 52000     |
| train/                  |           |
|    approx_kl            | 0.0183745 |
|    clip_fraction        | 0.174     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.3      |
|    explained_variance   | 0.906     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0416   |
|    n_updates            | 250       |
|    policy_gradient_loss | -0.0292   |
|    value_loss           | 0.000649  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.677    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 26       |
|    time_elapsed    | 667      |
|    total_timesteps | 53248    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 54000       |
| train/                  |             |
|    approx_kl            | 0.016895117 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.26       |
|    explained_variance   | 0.848       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0325     |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0283     |
|    value_loss           | 0.000917    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.678    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 27       |
|    time_elapsed    | 692      |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 56000       |
| train/                  |             |
|    approx_kl            | 0.016926479 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.29       |
|    explained_variance   | 0.864       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0374     |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0275     |
|    value_loss           | 0.00117     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.686    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 28       |
|    time_elapsed    | 717      |
|    total_timesteps | 57344    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 58000       |
| train/                  |             |
|    approx_kl            | 0.020328261 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.19       |
|    explained_variance   | 0.869       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0453     |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0269     |
|    value_loss           | 0.000694    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 998      |
|    ep_rew_mean     | 0.692    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 29       |
|    time_elapsed    | 743      |
|    total_timesteps | 59392    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.016780501 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.25       |
|    explained_variance   | 0.841       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0401     |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0264     |
|    value_loss           | 0.000976    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.692    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 30       |
|    time_elapsed    | 768      |
|    total_timesteps | 61440    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 62000       |
| train/                  |             |
|    approx_kl            | 0.018251015 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.23       |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00838     |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.0245     |
|    value_loss           | 0.000562    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.687    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 31       |
|    time_elapsed    | 793      |
|    total_timesteps | 63488    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 64000       |
| train/                  |             |
|    approx_kl            | 0.016825918 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.17       |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0235     |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.022      |
|    value_loss           | 0.000359    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 993      |
|    ep_rew_mean     | 0.688    |
| time/              |          |
|    fps             | 80       |
|    iterations      | 32       |
|    time_elapsed    | 819      |
|    total_timesteps | 65536    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 66000       |
| train/                  |             |
|    approx_kl            | 0.016001705 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.16       |
|    explained_variance   | 0.57        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.072      |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0277     |
|    value_loss           | 0.000843    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.703    |
| time/              |          |
|    fps             | 80       |
|    iterations      | 33       |
|    time_elapsed    | 844      |
|    total_timesteps | 67584    |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | 0.2          |
| time/                   |              |
|    total_timesteps      | 68000        |
| train/                  |              |
|    approx_kl            | 0.0143653285 |
|    clip_fraction        | 0.165        |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.16        |
|    explained_variance   | 0.699        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0491      |
|    n_updates            | 330          |
|    policy_gradient_loss | -0.0219      |
|    value_loss           | 0.00077      |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.7      |
| time/              |          |
|    fps             | 80       |
|    iterations      | 34       |
|    time_elapsed    | 869      |
|    total_timesteps | 69632    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.016718742 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.22       |
|    explained_variance   | 0.792       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0299     |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.0304     |
|    value_loss           | 0.00036     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.701    |
| time/              |          |
|    fps             | 80       |
|    iterations      | 35       |
|    time_elapsed    | 894      |
|    total_timesteps | 71680    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 72000      |
| train/                  |            |
|    approx_kl            | 0.01854195 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.22      |
|    explained_variance   | 0.93       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0521    |
|    n_updates            | 350        |
|    policy_gradient_loss | -0.0264    |
|    value_loss           | 0.000363   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.7      |
| time/              |          |
|    fps             | 80       |
|    iterations      | 36       |
|    time_elapsed    | 920      |
|    total_timesteps | 73728    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 74000      |
| train/                  |            |
|    approx_kl            | 0.02007864 |
|    clip_fraction        | 0.2        |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.22      |
|    explained_variance   | 0.875      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0307    |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.0261    |
|    value_loss           | 0.000287   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.7      |
| time/              |          |
|    fps             | 80       |
|    iterations      | 37       |
|    time_elapsed    | 945      |
|    total_timesteps | 75776    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 76000       |
| train/                  |             |
|    approx_kl            | 0.017307788 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.19       |
|    explained_variance   | 0.894       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0272     |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.0221     |
|    value_loss           | 0.000303    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.7      |
| time/              |          |
|    fps             | 80       |
|    iterations      | 38       |
|    time_elapsed    | 970      |
|    total_timesteps | 77824    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 78000      |
| train/                  |            |
|    approx_kl            | 0.01822846 |
|    clip_fraction        | 0.186      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.1       |
|    explained_variance   | 0.884      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0483    |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.025     |
|    value_loss           | 0.000586   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.704    |
| time/              |          |
|    fps             | 80       |
|    iterations      | 39       |
|    time_elapsed    | 996      |
|    total_timesteps | 79872    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.014311893 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.1        |
|    explained_variance   | 0.893       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0112     |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.0246     |
|    value_loss           | 0.000431    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.704    |
| time/              |          |
|    fps             | 80       |
|    iterations      | 40       |
|    time_elapsed    | 1021     |
|    total_timesteps | 81920    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 82000       |
| train/                  |             |
|    approx_kl            | 0.015304486 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.06       |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00427     |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.0221     |
|    value_loss           | 0.000979    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.704    |
| time/              |          |
|    fps             | 80       |
|    iterations      | 41       |
|    time_elapsed    | 1046     |
|    total_timesteps | 83968    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 84000       |
| train/                  |             |
|    approx_kl            | 0.015148886 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.11       |
|    explained_variance   | 0.935       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00193    |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.0241     |
|    value_loss           | 0.000182    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 996      |
|    ep_rew_mean     | 0.708    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 42       |
|    time_elapsed    | 1082     |
|    total_timesteps | 86016    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 87000       |
| train/                  |             |
|    approx_kl            | 0.015880011 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.06       |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0219     |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0256     |
|    value_loss           | 0.000706    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.715    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 43       |
|    time_elapsed    | 1107     |
|    total_timesteps | 88064    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 89000       |
| train/                  |             |
|    approx_kl            | 0.017203037 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.07       |
|    explained_variance   | 0.858       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0504     |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.0249     |
|    value_loss           | 0.000223    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.716    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 44       |
|    time_elapsed    | 1132     |
|    total_timesteps | 90112    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 91000       |
| train/                  |             |
|    approx_kl            | 0.018432142 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.11       |
|    explained_variance   | 0.883       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0434     |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.0242     |
|    value_loss           | 0.000398    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.713    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 45       |
|    time_elapsed    | 1158     |
|    total_timesteps | 92160    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 93000       |
| train/                  |             |
|    approx_kl            | 0.018770002 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.09       |
|    explained_variance   | 0.502       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00263     |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.00061     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.72     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 46       |
|    time_elapsed    | 1183     |
|    total_timesteps | 94208    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 95000       |
| train/                  |             |
|    approx_kl            | 0.017491072 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.07       |
|    explained_variance   | 0.851       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0391     |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.0258     |
|    value_loss           | 0.000595    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.725    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 47       |
|    time_elapsed    | 1208     |
|    total_timesteps | 96256    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 97000       |
| train/                  |             |
|    approx_kl            | 0.020291623 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.04       |
|    explained_variance   | 0.789       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0504     |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.0273     |
|    value_loss           | 0.000527    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.727    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 48       |
|    time_elapsed    | 1233     |
|    total_timesteps | 98304    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 99000       |
| train/                  |             |
|    approx_kl            | 0.020125337 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.02       |
|    explained_variance   | 0.855       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0312     |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.025      |
|    value_loss           | 0.000561    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.727    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 49       |
|    time_elapsed    | 1259     |
|    total_timesteps | 100352   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 101000      |
| train/                  |             |
|    approx_kl            | 0.014337927 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.01       |
|    explained_variance   | 0.897       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0303     |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.0237     |
|    value_loss           | 0.000268    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 990      |
|    ep_rew_mean     | 0.733    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 50       |
|    time_elapsed    | 1284     |
|    total_timesteps | 102400   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 103000      |
| train/                  |             |
|    approx_kl            | 0.020430129 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.98       |
|    explained_variance   | 0.887       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0116      |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.0278     |
|    value_loss           | 0.000797    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 990      |
|    ep_rew_mean     | 0.738    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 51       |
|    time_elapsed    | 1309     |
|    total_timesteps | 104448   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 105000      |
| train/                  |             |
|    approx_kl            | 0.014424212 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.97       |
|    explained_variance   | 0.931       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0126     |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.0207     |
|    value_loss           | 0.000319    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 988      |
|    ep_rew_mean     | 0.745    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 52       |
|    time_elapsed    | 1335     |
|    total_timesteps | 106496   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.3        |
| time/                   |            |
|    total_timesteps      | 107000     |
| train/                  |            |
|    approx_kl            | 0.01611539 |
|    clip_fraction        | 0.177      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.94      |
|    explained_variance   | 0.888      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0116    |
|    n_updates            | 520        |
|    policy_gradient_loss | -0.0227    |
|    value_loss           | 0.000499   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 985      |
|    ep_rew_mean     | 0.756    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 53       |
|    time_elapsed    | 1360     |
|    total_timesteps | 108544   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 109000      |
| train/                  |             |
|    approx_kl            | 0.015773227 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.88       |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00105     |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.0215     |
|    value_loss           | 0.000722    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 984      |
|    ep_rew_mean     | 0.767    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 54       |
|    time_elapsed    | 1385     |
|    total_timesteps | 110592   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 111000      |
| train/                  |             |
|    approx_kl            | 0.017386714 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.98       |
|    explained_variance   | 0.77        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0285      |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.000665    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 990      |
|    ep_rew_mean     | 0.777    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 55       |
|    time_elapsed    | 1410     |
|    total_timesteps | 112640   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.4        |
| time/                   |            |
|    total_timesteps      | 113000     |
| train/                  |            |
|    approx_kl            | 0.01693602 |
|    clip_fraction        | 0.178      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.97      |
|    explained_variance   | 0.925      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0408    |
|    n_updates            | 550        |
|    policy_gradient_loss | -0.0213    |
|    value_loss           | 0.000232   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 991      |
|    ep_rew_mean     | 0.778    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 56       |
|    time_elapsed    | 1436     |
|    total_timesteps | 114688   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 115000      |
| train/                  |             |
|    approx_kl            | 0.016519986 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.92       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0392     |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.0279     |
|    value_loss           | 0.000154    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 990      |
|    ep_rew_mean     | 0.78     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 57       |
|    time_elapsed    | 1461     |
|    total_timesteps | 116736   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 117000      |
| train/                  |             |
|    approx_kl            | 0.018968131 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.91       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0204     |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.0275     |
|    value_loss           | 9e-05       |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 986      |
|    ep_rew_mean     | 0.795    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 58       |
|    time_elapsed    | 1486     |
|    total_timesteps | 118784   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 119000      |
| train/                  |             |
|    approx_kl            | 0.020256095 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.97       |
|    explained_variance   | 0.836       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0421     |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.0185     |
|    value_loss           | 0.00107     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 974      |
|    ep_rew_mean     | 0.812    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 59       |
|    time_elapsed    | 1512     |
|    total_timesteps | 120832   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 121000      |
| train/                  |             |
|    approx_kl            | 0.017858319 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.8        |
|    explained_variance   | 0.856       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0207      |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.0224     |
|    value_loss           | 0.00165     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 954      |
|    ep_rew_mean     | 0.836    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 60       |
|    time_elapsed    | 1537     |
|    total_timesteps | 122880   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 123000      |
| train/                  |             |
|    approx_kl            | 0.017934442 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.82       |
|    explained_variance   | 0.86        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0298     |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.0193     |
|    value_loss           | 0.00139     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 955      |
|    ep_rew_mean     | 0.845    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 61       |
|    time_elapsed    | 1562     |
|    total_timesteps | 124928   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.4        |
| time/                   |            |
|    total_timesteps      | 125000     |
| train/                  |            |
|    approx_kl            | 0.01663535 |
|    clip_fraction        | 0.191      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.8       |
|    explained_variance   | 0.753      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0413    |
|    n_updates            | 610        |
|    policy_gradient_loss | -0.0109    |
|    value_loss           | 0.00224    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 954      |
|    ep_rew_mean     | 0.864    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 62       |
|    time_elapsed    | 1588     |
|    total_timesteps | 126976   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 127000      |
| train/                  |             |
|    approx_kl            | 0.012989288 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.04       |
|    explained_variance   | 0.723       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0173      |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 0.0013      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 960      |
|    ep_rew_mean     | 0.881    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 63       |
|    time_elapsed    | 1623     |
|    total_timesteps | 129024   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 130000      |
| train/                  |             |
|    approx_kl            | 0.016685603 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.86       |
|    explained_variance   | 0.697       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0216     |
|    n_updates            | 630         |
|    policy_gradient_loss | -0.0194     |
|    value_loss           | 0.00187     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 945      |
|    ep_rew_mean     | 0.888    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 64       |
|    time_elapsed    | 1648     |
|    total_timesteps | 131072   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 132000      |
| train/                  |             |
|    approx_kl            | 0.016336994 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.8        |
|    explained_variance   | 0.777       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0186     |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.00168     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 937      |
|    ep_rew_mean     | 0.909    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 65       |
|    time_elapsed    | 1674     |
|    total_timesteps | 133120   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 134000      |
| train/                  |             |
|    approx_kl            | 0.014444184 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.74       |
|    explained_variance   | 0.655       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0204     |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 0.00305     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 926      |
|    ep_rew_mean     | 0.929    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 66       |
|    time_elapsed    | 1699     |
|    total_timesteps | 135168   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 136000      |
| train/                  |             |
|    approx_kl            | 0.014189034 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.81       |
|    explained_variance   | 0.613       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0397     |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.0187     |
|    value_loss           | 0.00192     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 859      |
|    ep_rew_mean     | 0.958    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 67       |
|    time_elapsed    | 1724     |
|    total_timesteps | 137216   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 138000      |
| train/                  |             |
|    approx_kl            | 0.017176945 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.67       |
|    explained_variance   | 0.789       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00221    |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.00218     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 840      |
|    ep_rew_mean     | 0.979    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 68       |
|    time_elapsed    | 1750     |
|    total_timesteps | 139264   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 140000      |
| train/                  |             |
|    approx_kl            | 0.012734488 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.64       |
|    explained_variance   | 0.8         |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0209     |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.00126     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 757      |
|    ep_rew_mean     | 1        |
| time/              |          |
|    fps             | 79       |
|    iterations      | 69       |
|    time_elapsed    | 1775     |
|    total_timesteps | 141312   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 142000      |
| train/                  |             |
|    approx_kl            | 0.015012968 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.51       |
|    explained_variance   | 0.833       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00855    |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.0198     |
|    value_loss           | 0.00237     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 682      |
|    ep_rew_mean     | 1.03     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 70       |
|    time_elapsed    | 1800     |
|    total_timesteps | 143360   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 144000      |
| train/                  |             |
|    approx_kl            | 0.015905099 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.42       |
|    explained_variance   | 0.798       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0194     |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 0.00287     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 632      |
|    ep_rew_mean     | 1.06     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 71       |
|    time_elapsed    | 1825     |
|    total_timesteps | 145408   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 146000      |
| train/                  |             |
|    approx_kl            | 0.017026437 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.48       |
|    explained_variance   | 0.75        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000784   |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.00274     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 574      |
|    ep_rew_mean     | 1.08     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 72       |
|    time_elapsed    | 1851     |
|    total_timesteps | 147456   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 148000      |
| train/                  |             |
|    approx_kl            | 0.015268116 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.43       |
|    explained_variance   | 0.763       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0279     |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 0.00245     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 425      |
|    ep_rew_mean     | 1.1      |
| time/              |          |
|    fps             | 79       |
|    iterations      | 73       |
|    time_elapsed    | 1876     |
|    total_timesteps | 149504   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.018024199 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.13       |
|    explained_variance   | 0.843       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0245     |
|    n_updates            | 730         |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 0.00293     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 151000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 381      |
|    ep_rew_mean     | 1.1      |
| time/              |          |
|    fps             | 79       |
|    iterations      | 74       |
|    time_elapsed    | 1901     |
|    total_timesteps | 151552   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 152000      |
| train/                  |             |
|    approx_kl            | 0.016273668 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.4        |
|    explained_variance   | 0.677       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0157     |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.00265     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 153000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 327      |
|    ep_rew_mean     | 1.1      |
| time/              |          |
|    fps             | 79       |
|    iterations      | 75       |
|    time_elapsed    | 1927     |
|    total_timesteps | 153600   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 154000      |
| train/                  |             |
|    approx_kl            | 0.014151203 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.27       |
|    explained_variance   | 0.741       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.014      |
|    n_updates            | 750         |
|    policy_gradient_loss | -0.00984    |
|    value_loss           | 0.00294     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 233      |
|    ep_rew_mean     | 1.06     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 76       |
|    time_elapsed    | 1952     |
|    total_timesteps | 155648   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 156000      |
| train/                  |             |
|    approx_kl            | 0.016522579 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.05       |
|    explained_variance   | 0.77        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00579    |
|    n_updates            | 760         |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 0.00346     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 174      |
|    ep_rew_mean     | 1.02     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 77       |
|    time_elapsed    | 1978     |
|    total_timesteps | 157696   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 158000      |
| train/                  |             |
|    approx_kl            | 0.015179563 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.75       |
|    explained_variance   | 0.838       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0231     |
|    n_updates            | 770         |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.00307     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 159000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 123      |
|    ep_rew_mean     | 0.983    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 78       |
|    time_elapsed    | 2003     |
|    total_timesteps | 159744   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 160000      |
| train/                  |             |
|    approx_kl            | 0.014863169 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.69       |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.04       |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.00218     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 161000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.2     |
|    ep_rew_mean     | 0.961    |
| time/              |          |
|    fps             | 79       |
|    iterations      | 79       |
|    time_elapsed    | 2028     |
|    total_timesteps | 161792   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 9          |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 162000     |
| train/                  |            |
|    approx_kl            | 0.02097074 |
|    clip_fraction        | 0.218      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.75      |
|    explained_variance   | 0.837      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0292    |
|    n_updates            | 790        |
|    policy_gradient_loss | -0.0174    |
|    value_loss           | 0.00299    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 163000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67.3     |
|    ep_rew_mean     | 0.941    |
| time/              |          |
|    fps             | 80       |
|    iterations      | 80       |
|    time_elapsed    | 2034     |
|    total_timesteps | 163840   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 164000      |
| train/                  |             |
|    approx_kl            | 0.019262005 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | 0.866       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0219     |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.0194     |
|    value_loss           | 0.00406     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 165000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.4     |
|    ep_rew_mean     | 0.937    |
| time/              |          |
|    fps             | 81       |
|    iterations      | 81       |
|    time_elapsed    | 2039     |
|    total_timesteps | 165888   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 166000      |
| train/                  |             |
|    approx_kl            | 0.019675624 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.03       |
|    explained_variance   | 0.892       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0047     |
|    n_updates            | 810         |
|    policy_gradient_loss | -0.0182     |
|    value_loss           | 0.00396     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 167000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.5     |
|    ep_rew_mean     | 0.931    |
| time/              |          |
|    fps             | 82       |
|    iterations      | 82       |
|    time_elapsed    | 2045     |
|    total_timesteps | 167936   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 9          |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 168000     |
| train/                  |            |
|    approx_kl            | 0.01706015 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.86      |
|    explained_variance   | 0.886      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0014    |
|    n_updates            | 820        |
|    policy_gradient_loss | -0.0127    |
|    value_loss           | 0.00456    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 169000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 0.923    |
| time/              |          |
|    fps             | 82       |
|    iterations      | 83       |
|    time_elapsed    | 2051     |
|    total_timesteps | 169984   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 170000      |
| train/                  |             |
|    approx_kl            | 0.017560834 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.58       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0224     |
|    n_updates            | 830         |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.00378     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.7     |
|    ep_rew_mean     | 0.92     |
| time/              |          |
|    fps             | 83       |
|    iterations      | 84       |
|    time_elapsed    | 2056     |
|    total_timesteps | 172032   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 173000      |
| train/                  |             |
|    approx_kl            | 0.016093016 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0603     |
|    n_updates            | 840         |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.00165     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 31.8     |
|    ep_rew_mean     | 0.92     |
| time/              |          |
|    fps             | 84       |
|    iterations      | 85       |
|    time_elapsed    | 2062     |
|    total_timesteps | 174080   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 175000      |
| train/                  |             |
|    approx_kl            | 0.020777352 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.09       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000197   |
|    n_updates            | 850         |
|    policy_gradient_loss | -0.0134     |
|    value_loss           | 0.00334     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.5     |
|    ep_rew_mean     | 0.921    |
| time/              |          |
|    fps             | 85       |
|    iterations      | 86       |
|    time_elapsed    | 2068     |
|    total_timesteps | 176128   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 9          |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 177000     |
| train/                  |            |
|    approx_kl            | 0.02276495 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.01      |
|    explained_variance   | 0.938      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00719    |
|    n_updates            | 860        |
|    policy_gradient_loss | -0.0142    |
|    value_loss           | 0.00319    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.5     |
|    ep_rew_mean     | 0.916    |
| time/              |          |
|    fps             | 85       |
|    iterations      | 87       |
|    time_elapsed    | 2073     |
|    total_timesteps | 178176   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 9          |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 179000     |
| train/                  |            |
|    approx_kl            | 0.03000179 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.87      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0533    |
|    n_updates            | 870        |
|    policy_gradient_loss | -0.0204    |
|    value_loss           | 0.000242   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.8     |
|    ep_rew_mean     | 0.913    |
| time/              |          |
|    fps             | 86       |
|    iterations      | 88       |
|    time_elapsed    | 2079     |
|    total_timesteps | 180224   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 181000      |
| train/                  |             |
|    approx_kl            | 0.017605018 |
|    clip_fraction        | 0.32        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.83       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00335    |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.0218     |
|    value_loss           | 0.00186     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.2     |
|    ep_rew_mean     | 0.914    |
| time/              |          |
|    fps             | 87       |
|    iterations      | 89       |
|    time_elapsed    | 2085     |
|    total_timesteps | 182272   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 183000      |
| train/                  |             |
|    approx_kl            | 0.017862663 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.51       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.023      |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 0.0031      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | 0.908    |
| time/              |          |
|    fps             | 88       |
|    iterations      | 90       |
|    time_elapsed    | 2090     |
|    total_timesteps | 184320   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 185000      |
| train/                  |             |
|    approx_kl            | 0.027152132 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.22       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0063      |
|    n_updates            | 900         |
|    policy_gradient_loss | -0.0279     |
|    value_loss           | 0.000918    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.1     |
|    ep_rew_mean     | 0.905    |
| time/              |          |
|    fps             | 88       |
|    iterations      | 91       |
|    time_elapsed    | 2096     |
|    total_timesteps | 186368   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 187000      |
| train/                  |             |
|    approx_kl            | 0.019521983 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.94       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.058      |
|    n_updates            | 910         |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.000697    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.3     |
|    ep_rew_mean     | 0.906    |
| time/              |          |
|    fps             | 89       |
|    iterations      | 92       |
|    time_elapsed    | 2102     |
|    total_timesteps | 188416   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 189000      |
| train/                  |             |
|    approx_kl            | 0.025342036 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.49       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0522     |
|    n_updates            | 920         |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.000709    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.7     |
|    ep_rew_mean     | 0.906    |
| time/              |          |
|    fps             | 90       |
|    iterations      | 93       |
|    time_elapsed    | 2107     |
|    total_timesteps | 190464   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 191000      |
| train/                  |             |
|    approx_kl            | 0.019622587 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.45       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00191     |
|    n_updates            | 930         |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 0.000664    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.1     |
|    ep_rew_mean     | 0.906    |
| time/              |          |
|    fps             | 91       |
|    iterations      | 94       |
|    time_elapsed    | 2113     |
|    total_timesteps | 192512   |
---------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 9         |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 193000    |
| train/                  |           |
|    approx_kl            | 0.0358951 |
|    clip_fraction        | 0.262     |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.16     |
|    explained_variance   | 0.971     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0169    |
|    n_updates            | 940       |
|    policy_gradient_loss | -0.0205   |
|    value_loss           | 0.00151   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 11.3     |
|    ep_rew_mean     | 0.907    |
| time/              |          |
|    fps             | 91       |
|    iterations      | 95       |
|    time_elapsed    | 2119     |
|    total_timesteps | 194560   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 195000      |
| train/                  |             |
|    approx_kl            | 0.029438429 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.02       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0703     |
|    n_updates            | 950         |
|    policy_gradient_loss | -0.00701    |
|    value_loss           | 0.00144     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 11.5     |
|    ep_rew_mean     | 0.908    |
| time/              |          |
|    fps             | 92       |
|    iterations      | 96       |
|    time_elapsed    | 2125     |
|    total_timesteps | 196608   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 197000      |
| train/                  |             |
|    approx_kl            | 0.021388825 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.938      |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0513     |
|    n_updates            | 960         |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 0.00189     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.6     |
|    ep_rew_mean     | 0.9      |
| time/              |          |
|    fps             | 93       |
|    iterations      | 97       |
|    time_elapsed    | 2130     |
|    total_timesteps | 198656   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 199000      |
| train/                  |             |
|    approx_kl            | 0.059777528 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.69       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0861     |
|    n_updates            | 970         |
|    policy_gradient_loss | -0.0235     |
|    value_loss           | 2.55e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.1     |
|    ep_rew_mean     | 0.9      |
| time/              |          |
|    fps             | 93       |
|    iterations      | 98       |
|    time_elapsed    | 2136     |
|    total_timesteps | 200704   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 201000      |
| train/                  |             |
|    approx_kl            | 0.019311232 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.06       |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0379     |
|    n_updates            | 980         |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 0.000116    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 10       |
|    ep_rew_mean     | 0.908    |
| time/              |          |
|    fps             | 94       |
|    iterations      | 99       |
|    time_elapsed    | 2142     |
|    total_timesteps | 202752   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 203000      |
| train/                  |             |
|    approx_kl            | 0.010893184 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.17       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00273    |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.0058     |
|    value_loss           | 0.00208     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.7     |
|    ep_rew_mean     | 0.91     |
| time/              |          |
|    fps             | 95       |
|    iterations      | 100      |
|    time_elapsed    | 2147     |
|    total_timesteps | 204800   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 205000      |
| train/                  |             |
|    approx_kl            | 0.014139477 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.03       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0131     |
|    n_updates            | 1000        |
|    policy_gradient_loss | -0.0185     |
|    value_loss           | 0.000361    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 10       |
|    ep_rew_mean     | 0.908    |
| time/              |          |
|    fps             | 96       |
|    iterations      | 101      |
|    time_elapsed    | 2153     |
|    total_timesteps | 206848   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 9          |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 207000     |
| train/                  |            |
|    approx_kl            | 0.00821603 |
|    clip_fraction        | 0.0516     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.534     |
|    explained_variance   | 0.956      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0435    |
|    n_updates            | 1010       |
|    policy_gradient_loss | -0.0124    |
|    value_loss           | 0.00306    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.57     |
|    ep_rew_mean     | 0.902    |
| time/              |          |
|    fps             | 96       |
|    iterations      | 102      |
|    time_elapsed    | 2159     |
|    total_timesteps | 208896   |
---------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 9         |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 209000    |
| train/                  |           |
|    approx_kl            | 0.0177467 |
|    clip_fraction        | 0.146     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.477    |
|    explained_variance   | 0.996     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0492   |
|    n_updates            | 1020      |
|    policy_gradient_loss | -0.0287   |
|    value_loss           | 9.5e-05   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.1     |
|    ep_rew_mean     | 0.906    |
| time/              |          |
|    fps             | 97       |
|    iterations      | 103      |
|    time_elapsed    | 2165     |
|    total_timesteps | 210944   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 211000      |
| train/                  |             |
|    approx_kl            | 0.036319494 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.52       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0697     |
|    n_updates            | 1030        |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 0.000926    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 10.1     |
|    ep_rew_mean     | 0.908    |
| time/              |          |
|    fps             | 98       |
|    iterations      | 104      |
|    time_elapsed    | 2170     |
|    total_timesteps | 212992   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 213000      |
| train/                  |             |
|    approx_kl            | 0.011768553 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.75       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0158     |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 0.00216     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.56     |
|    ep_rew_mean     | 0.9      |
| time/              |          |
|    fps             | 98       |
|    iterations      | 105      |
|    time_elapsed    | 2176     |
|    total_timesteps | 215040   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 216000      |
| train/                  |             |
|    approx_kl            | 0.060607404 |
|    clip_fraction        | 0.0531      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.365      |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.039      |
|    n_updates            | 1050        |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.00165     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.56     |
|    ep_rew_mean     | 0.908    |
| time/              |          |
|    fps             | 99       |
|    iterations      | 106      |
|    time_elapsed    | 2182     |
|    total_timesteps | 217088   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 218000      |
| train/                  |             |
|    approx_kl            | 0.019486738 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.33       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0283     |
|    n_updates            | 1060        |
|    policy_gradient_loss | -0.00773    |
|    value_loss           | 0.00168     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.2     |
|    ep_rew_mean     | 0.905    |
| time/              |          |
|    fps             | 100      |
|    iterations      | 107      |
|    time_elapsed    | 2188     |
|    total_timesteps | 219136   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 9          |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 220000     |
| train/                  |            |
|    approx_kl            | 0.02726374 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.61      |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.18       |
|    n_updates            | 1070       |
|    policy_gradient_loss | -0.00633   |
|    value_loss           | 0.000386   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 10.3     |
|    ep_rew_mean     | 0.905    |
| time/              |          |
|    fps             | 100      |
|    iterations      | 108      |
|    time_elapsed    | 2194     |
|    total_timesteps | 221184   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 222000      |
| train/                  |             |
|    approx_kl            | 0.008173085 |
|    clip_fraction        | 0.0613      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.847      |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0142     |
|    n_updates            | 1080        |
|    policy_gradient_loss | -0.0137     |
|    value_loss           | 0.000843    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.29     |
|    ep_rew_mean     | 0.9      |
| time/              |          |
|    fps             | 101      |
|    iterations      | 109      |
|    time_elapsed    | 2200     |
|    total_timesteps | 223232   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 224000      |
| train/                  |             |
|    approx_kl            | 0.010056027 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.63       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0279      |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.00704    |
|    value_loss           | 7.41e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.25     |
|    ep_rew_mean     | 0.9      |
| time/              |          |
|    fps             | 102      |
|    iterations      | 110      |
|    time_elapsed    | 2205     |
|    total_timesteps | 225280   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 226000      |
| train/                  |             |
|    approx_kl            | 0.031382006 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.53       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000459   |
|    n_updates            | 1100        |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 4.12e-06    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.24     |
|    ep_rew_mean     | 0.9      |
| time/              |          |
|    fps             | 102      |
|    iterations      | 111      |
|    time_elapsed    | 2211     |
|    total_timesteps | 227328   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 228000      |
| train/                  |             |
|    approx_kl            | 0.011889646 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0457     |
|    n_updates            | 1110        |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 6.03e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.25     |
|    ep_rew_mean     | 0.9      |
| time/              |          |
|    fps             | 103      |
|    iterations      | 112      |
|    time_elapsed    | 2217     |
|    total_timesteps | 229376   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 230000      |
| train/                  |             |
|    approx_kl            | 0.018910605 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.7        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00155     |
|    n_updates            | 1120        |
|    policy_gradient_loss | -0.00949    |
|    value_loss           | 3.73e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.28     |
|    ep_rew_mean     | 0.902    |
| time/              |          |
|    fps             | 104      |
|    iterations      | 113      |
|    time_elapsed    | 2223     |
|    total_timesteps | 231424   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 232000      |
| train/                  |             |
|    approx_kl            | 0.020414801 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.55       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0114     |
|    n_updates            | 1130        |
|    policy_gradient_loss | -0.00985    |
|    value_loss           | 4.95e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.18     |
|    ep_rew_mean     | 0.9      |
| time/              |          |
|    fps             | 104      |
|    iterations      | 114      |
|    time_elapsed    | 2228     |
|    total_timesteps | 233472   |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 9            |
|    mean_reward          | 0.9          |
| time/                   |              |
|    total_timesteps      | 234000       |
| train/                  |              |
|    approx_kl            | 0.0068280054 |
|    clip_fraction        | 0.0851       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.3         |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.024       |
|    n_updates            | 1140         |
|    policy_gradient_loss | -0.00946     |
|    value_loss           | 0.00186      |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.36     |
|    ep_rew_mean     | 0.908    |
| time/              |          |
|    fps             | 105      |
|    iterations      | 115      |
|    time_elapsed    | 2234     |
|    total_timesteps | 235520   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 236000      |
| train/                  |             |
|    approx_kl            | 0.007776635 |
|    clip_fraction        | 0.0932      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0015      |
|    n_updates            | 1150        |
|    policy_gradient_loss | -0.00665    |
|    value_loss           | 0.00173     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 237000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.21     |
|    ep_rew_mean     | 0.9      |
| time/              |          |
|    fps             | 106      |
|    iterations      | 116      |
|    time_elapsed    | 2240     |
|    total_timesteps | 237568   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 238000      |
| train/                  |             |
|    approx_kl            | 0.007722313 |
|    clip_fraction        | 0.0653      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0181      |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.00339    |
|    value_loss           | 0.000478    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 239000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.16     |
|    ep_rew_mean     | 0.9      |
| time/              |          |
|    fps             | 106      |
|    iterations      | 117      |
|    time_elapsed    | 2246     |
|    total_timesteps | 239616   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 0.016019788 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.41       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0406     |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.00786    |
|    value_loss           | 2.83e-06    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 241000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.94     |
|    ep_rew_mean     | 0.9      |
| time/              |          |
|    fps             | 107      |
|    iterations      | 118      |
|    time_elapsed    | 2251     |
|    total_timesteps | 241664   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 242000      |
| train/                  |             |
|    approx_kl            | 0.024176285 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.69       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.049      |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.0235     |
|    value_loss           | 0.000212    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 243000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.5      |
|    ep_rew_mean     | 0.9      |
| time/              |          |
|    fps             | 107      |
|    iterations      | 119      |
|    time_elapsed    | 2257     |
|    total_timesteps | 243712   |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 9            |
|    mean_reward          | 0.9          |
| time/                   |              |
|    total_timesteps      | 244000       |
| train/                  |              |
|    approx_kl            | 0.0107325995 |
|    clip_fraction        | 0.0909       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.54        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0201       |
|    n_updates            | 1190         |
|    policy_gradient_loss | -0.00639     |
|    value_loss           | 7.53e-05     |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.55     |
|    ep_rew_mean     | 0.9      |
| time/              |          |
|    fps             | 108      |
|    iterations      | 120      |
|    time_elapsed    | 2263     |
|    total_timesteps | 245760   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 9          |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 246000     |
| train/                  |            |
|    approx_kl            | 0.08239497 |
|    clip_fraction        | 0.156      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.745     |
|    explained_variance   | 1          |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0455    |
|    n_updates            | 1200       |
|    policy_gradient_loss | -0.032     |
|    value_loss           | 3.71e-06   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 247000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.15     |
|    ep_rew_mean     | 0.9      |
| time/              |          |
|    fps             | 109      |
|    iterations      | 121      |
|    time_elapsed    | 2269     |
|    total_timesteps | 247808   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 248000      |
| train/                  |             |
|    approx_kl            | 0.022135485 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.79       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0227      |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 1.33e-06    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.74     |
|    ep_rew_mean     | 0.905    |
| time/              |          |
|    fps             | 109      |
|    iterations      | 122      |
|    time_elapsed    | 2274     |
|    total_timesteps | 249856   |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 9            |
|    mean_reward          | 0.9          |
| time/                   |              |
|    total_timesteps      | 250000       |
| train/                  |              |
|    approx_kl            | 0.0040667057 |
|    clip_fraction        | 0.0514       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.63        |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0326      |
|    n_updates            | 1220         |
|    policy_gradient_loss | -0.0115      |
|    value_loss           | 0.000242     |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.4     |
|    ep_rew_mean     | 0.903    |
| time/              |          |
|    fps             | 110      |
|    iterations      | 123      |
|    time_elapsed    | 2280     |
|    total_timesteps | 251904   |
---------------------------------
