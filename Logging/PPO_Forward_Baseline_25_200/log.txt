Logging to ./Logging/PPO_Forward_Baseline_25_200
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
-----------------------------
| time/              |      |
|    fps             | 10   |
|    iterations      | 1    |
|    time_elapsed    | 204  |
|    total_timesteps | 2048 |
-----------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 3000       |
| train/                  |            |
|    approx_kl            | 0.01744909 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.37      |
|    explained_variance   | 0.55       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.057     |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.0305    |
|    value_loss           | 0.000866   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
-----------------------------
| time/              |      |
|    fps             | 10   |
|    iterations      | 2    |
|    time_elapsed    | 409  |
|    total_timesteps | 4096 |
-----------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.020642359 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.36       |
|    explained_variance   | 0.858       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0595     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0394     |
|    value_loss           | 0.0134      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
-----------------------------
| time/              |      |
|    fps             | 9    |
|    iterations      | 3    |
|    time_elapsed    | 614  |
|    total_timesteps | 6144 |
-----------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 7000        |
| train/                  |             |
|    approx_kl            | 0.021547604 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.35       |
|    explained_variance   | 0.857       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0368     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.034      |
|    value_loss           | 0.00177     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
-----------------------------
| time/              |      |
|    fps             | 9    |
|    iterations      | 4    |
|    time_elapsed    | 820  |
|    total_timesteps | 8192 |
-----------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 9000       |
| train/                  |            |
|    approx_kl            | 0.02656268 |
|    clip_fraction        | 0.281      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.33      |
|    explained_variance   | 0.776      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.073     |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0428    |
|    value_loss           | 0.00985    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 5     |
|    time_elapsed    | 1025  |
|    total_timesteps | 10240 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 11000       |
| train/                  |             |
|    approx_kl            | 0.022002477 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.33       |
|    explained_variance   | 0.839       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0375     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0431     |
|    value_loss           | 0.0135      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 6     |
|    time_elapsed    | 1230  |
|    total_timesteps | 12288 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 13000       |
| train/                  |             |
|    approx_kl            | 0.021182682 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.33       |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0523     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0445     |
|    value_loss           | 0.00321     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 7     |
|    time_elapsed    | 1436  |
|    total_timesteps | 14336 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.021845218 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.32       |
|    explained_variance   | 0.749       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0588     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0446     |
|    value_loss           | 0.00832     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 8     |
|    time_elapsed    | 1641  |
|    total_timesteps | 16384 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 17000       |
| train/                  |             |
|    approx_kl            | 0.024573611 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.29       |
|    explained_variance   | 0.566       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0761     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0374     |
|    value_loss           | 0.00429     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 9     |
|    time_elapsed    | 1846  |
|    total_timesteps | 18432 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 19000       |
| train/                  |             |
|    approx_kl            | 0.021165632 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.3        |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0552     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0442     |
|    value_loss           | 0.0035      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 10    |
|    time_elapsed    | 2052  |
|    total_timesteps | 20480 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 21000       |
| train/                  |             |
|    approx_kl            | 0.021613998 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.29       |
|    explained_variance   | 0.845       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0806     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0434     |
|    value_loss           | 0.00559     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 11    |
|    time_elapsed    | 2257  |
|    total_timesteps | 22528 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 23000       |
| train/                  |             |
|    approx_kl            | 0.030294262 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.23       |
|    explained_variance   | 0.893       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0703     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0404     |
|    value_loss           | 0.0025      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 12    |
|    time_elapsed    | 2462  |
|    total_timesteps | 24576 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.023583548 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.24       |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0349     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0401     |
|    value_loss           | 0.00345     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 13    |
|    time_elapsed    | 2668  |
|    total_timesteps | 26624 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 27000       |
| train/                  |             |
|    approx_kl            | 0.020566052 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.23       |
|    explained_variance   | 0.837       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0505     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0395     |
|    value_loss           | 0.00218     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 14    |
|    time_elapsed    | 2873  |
|    total_timesteps | 28672 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 29000       |
| train/                  |             |
|    approx_kl            | 0.018750383 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.23       |
|    explained_variance   | 0.764       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0578     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0372     |
|    value_loss           | 0.00237     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 15    |
|    time_elapsed    | 3078  |
|    total_timesteps | 30720 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 31000       |
| train/                  |             |
|    approx_kl            | 0.018411422 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.21       |
|    explained_variance   | 0.811       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0138     |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0351     |
|    value_loss           | 0.0019      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 16    |
|    time_elapsed    | 3284  |
|    total_timesteps | 32768 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 33000       |
| train/                  |             |
|    approx_kl            | 0.024137944 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.18       |
|    explained_variance   | 0.904       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0602     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0311     |
|    value_loss           | 0.00296     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 17    |
|    time_elapsed    | 3489  |
|    total_timesteps | 34816 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.018977586 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.16       |
|    explained_variance   | 0.73        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0412     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0356     |
|    value_loss           | 0.00209     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 18    |
|    time_elapsed    | 3694  |
|    total_timesteps | 36864 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 37000       |
| train/                  |             |
|    approx_kl            | 0.019307405 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.21       |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0445     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0334     |
|    value_loss           | 0.00244     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 19    |
|    time_elapsed    | 3900  |
|    total_timesteps | 38912 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 39000       |
| train/                  |             |
|    approx_kl            | 0.022900436 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.19       |
|    explained_variance   | 0.767       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0458     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0398     |
|    value_loss           | 0.00254     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 20    |
|    time_elapsed    | 4105  |
|    total_timesteps | 40960 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 41000       |
| train/                  |             |
|    approx_kl            | 0.021534517 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.17       |
|    explained_variance   | 0.802       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0182     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0346     |
|    value_loss           | 0.00471     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 21    |
|    time_elapsed    | 4410  |
|    total_timesteps | 43008 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 44000       |
| train/                  |             |
|    approx_kl            | 0.018134385 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.19       |
|    explained_variance   | 0.558       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0433     |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.031      |
|    value_loss           | 0.00129     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 22    |
|    time_elapsed    | 4615  |
|    total_timesteps | 45056 |
------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 46000      |
| train/                  |            |
|    approx_kl            | 0.02640444 |
|    clip_fraction        | 0.256      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.1       |
|    explained_variance   | 0.777      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0575    |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0407    |
|    value_loss           | 0.00271    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 23    |
|    time_elapsed    | 4821  |
|    total_timesteps | 47104 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 48000       |
| train/                  |             |
|    approx_kl            | 0.015151273 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.92       |
|    explained_variance   | 0.78        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0333     |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0249     |
|    value_loss           | 0.00262     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 24    |
|    time_elapsed    | 5026  |
|    total_timesteps | 49152 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.021913305 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.07       |
|    explained_variance   | 0.845       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0124     |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0374     |
|    value_loss           | 0.00257     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 25    |
|    time_elapsed    | 5231  |
|    total_timesteps | 51200 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 52000       |
| train/                  |             |
|    approx_kl            | 0.022132304 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.17       |
|    explained_variance   | 0.87        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0411     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0392     |
|    value_loss           | 0.00443     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 26    |
|    time_elapsed    | 5437  |
|    total_timesteps | 53248 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 54000       |
| train/                  |             |
|    approx_kl            | 0.020934701 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.13       |
|    explained_variance   | 0.754       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.071      |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0334     |
|    value_loss           | 0.00185     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 27    |
|    time_elapsed    | 5642  |
|    total_timesteps | 55296 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 56000       |
| train/                  |             |
|    approx_kl            | 0.018936608 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.16       |
|    explained_variance   | 0.696       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0643     |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.041      |
|    value_loss           | 0.00264     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 28    |
|    time_elapsed    | 5847  |
|    total_timesteps | 57344 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 58000       |
| train/                  |             |
|    approx_kl            | 0.025379907 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.11       |
|    explained_variance   | 0.832       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.06       |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0422     |
|    value_loss           | 0.00552     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 29    |
|    time_elapsed    | 6053  |
|    total_timesteps | 59392 |
------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.02218157 |
|    clip_fraction        | 0.254      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.97      |
|    explained_variance   | 0.846      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.000485   |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0311    |
|    value_loss           | 0.00167    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 30    |
|    time_elapsed    | 6258  |
|    total_timesteps | 61440 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 62000       |
| train/                  |             |
|    approx_kl            | 0.019299438 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.08       |
|    explained_variance   | 0.564       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0458     |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.0296     |
|    value_loss           | 0.00211     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 31    |
|    time_elapsed    | 6463  |
|    total_timesteps | 63488 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 64000       |
| train/                  |             |
|    approx_kl            | 0.024381662 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.92       |
|    explained_variance   | 0.696       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0341     |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.0353     |
|    value_loss           | 0.00272     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 32    |
|    time_elapsed    | 6669  |
|    total_timesteps | 65536 |
------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 66000      |
| train/                  |            |
|    approx_kl            | 0.02517731 |
|    clip_fraction        | 0.249      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.1       |
|    explained_variance   | 0.809      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0148    |
|    n_updates            | 320        |
|    policy_gradient_loss | -0.0405    |
|    value_loss           | 0.00154    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 33    |
|    time_elapsed    | 6874  |
|    total_timesteps | 67584 |
------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 68000      |
| train/                  |            |
|    approx_kl            | 0.01953016 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.07      |
|    explained_variance   | 0.847      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0227    |
|    n_updates            | 330        |
|    policy_gradient_loss | -0.0325    |
|    value_loss           | 0.00278    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 34    |
|    time_elapsed    | 7079  |
|    total_timesteps | 69632 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.024520732 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8          |
|    explained_variance   | 0.758       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0545     |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.0372     |
|    value_loss           | 0.00143     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 35    |
|    time_elapsed    | 7285  |
|    total_timesteps | 71680 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 72000       |
| train/                  |             |
|    approx_kl            | 0.022126062 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.02       |
|    explained_variance   | 0.631       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.038      |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.0352     |
|    value_loss           | 0.000544    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 36    |
|    time_elapsed    | 7490  |
|    total_timesteps | 73728 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 74000       |
| train/                  |             |
|    approx_kl            | 0.019348906 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.09       |
|    explained_variance   | 0.851       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0167     |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.0375     |
|    value_loss           | 0.000716    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 37    |
|    time_elapsed    | 7695  |
|    total_timesteps | 75776 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 76000       |
| train/                  |             |
|    approx_kl            | 0.022774031 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.88       |
|    explained_variance   | 0.624       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0193      |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.0355     |
|    value_loss           | 0.00154     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 38    |
|    time_elapsed    | 7901  |
|    total_timesteps | 77824 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 78000       |
| train/                  |             |
|    approx_kl            | 0.020399842 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.98       |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0516     |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.0334     |
|    value_loss           | 0.00125     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 39    |
|    time_elapsed    | 8106  |
|    total_timesteps | 79872 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.022009704 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.03       |
|    explained_variance   | 0.885       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0687     |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.0444     |
|    value_loss           | 0.00144     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 40    |
|    time_elapsed    | 8311  |
|    total_timesteps | 81920 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 82000       |
| train/                  |             |
|    approx_kl            | 0.020295866 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8          |
|    explained_variance   | 0.807       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00438    |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.0343     |
|    value_loss           | 0.000963    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 41    |
|    time_elapsed    | 8516  |
|    total_timesteps | 83968 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 84000       |
| train/                  |             |
|    approx_kl            | 0.024381705 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.85       |
|    explained_variance   | 0.871       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0771     |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.0401     |
|    value_loss           | 0.00132     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 42    |
|    time_elapsed    | 8822  |
|    total_timesteps | 86016 |
------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 87000      |
| train/                  |            |
|    approx_kl            | 0.02217374 |
|    clip_fraction        | 0.241      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.91      |
|    explained_variance   | 0.878      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0499    |
|    n_updates            | 420        |
|    policy_gradient_loss | -0.0333    |
|    value_loss           | 0.000588   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 43    |
|    time_elapsed    | 9027  |
|    total_timesteps | 88064 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 89000       |
| train/                  |             |
|    approx_kl            | 0.019574186 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.02       |
|    explained_variance   | 0.719       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0373     |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.0372     |
|    value_loss           | 0.00184     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 44    |
|    time_elapsed    | 9232  |
|    total_timesteps | 90112 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 91000       |
| train/                  |             |
|    approx_kl            | 0.023886282 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.01       |
|    explained_variance   | 0.811       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0717     |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.0435     |
|    value_loss           | 0.00129     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 45    |
|    time_elapsed    | 9438  |
|    total_timesteps | 92160 |
------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 93000      |
| train/                  |            |
|    approx_kl            | 0.02224467 |
|    clip_fraction        | 0.246      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.01      |
|    explained_variance   | 0.685      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0541    |
|    n_updates            | 450        |
|    policy_gradient_loss | -0.0396    |
|    value_loss           | 0.00186    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 46    |
|    time_elapsed    | 9643  |
|    total_timesteps | 94208 |
------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+04     |
|    mean_reward          | 0         |
| time/                   |           |
|    total_timesteps      | 95000     |
| train/                  |           |
|    approx_kl            | 0.0235475 |
|    clip_fraction        | 0.264     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.95     |
|    explained_variance   | 0.478     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0598   |
|    n_updates            | 460       |
|    policy_gradient_loss | -0.0367   |
|    value_loss           | 0.00207   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 47    |
|    time_elapsed    | 9849  |
|    total_timesteps | 96256 |
------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 97000      |
| train/                  |            |
|    approx_kl            | 0.02415235 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.92      |
|    explained_variance   | 0.858      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0506    |
|    n_updates            | 470        |
|    policy_gradient_loss | -0.0474    |
|    value_loss           | 0.00323    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 9     |
|    iterations      | 48    |
|    time_elapsed    | 10054 |
|    total_timesteps | 98304 |
------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 99000       |
| train/                  |             |
|    approx_kl            | 0.022360459 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.87       |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0539     |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.0429     |
|    value_loss           | 0.00186     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 49     |
|    time_elapsed    | 10259  |
|    total_timesteps | 100352 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 101000     |
| train/                  |            |
|    approx_kl            | 0.02195543 |
|    clip_fraction        | 0.24       |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.89      |
|    explained_variance   | 0.78       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.046     |
|    n_updates            | 490        |
|    policy_gradient_loss | -0.0333    |
|    value_loss           | 0.000964   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 50     |
|    time_elapsed    | 10464  |
|    total_timesteps | 102400 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 103000      |
| train/                  |             |
|    approx_kl            | 0.027163848 |
|    clip_fraction        | 0.287       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.79       |
|    explained_variance   | 0.821       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0136      |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.0357     |
|    value_loss           | 0.00166     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 51     |
|    time_elapsed    | 10670  |
|    total_timesteps | 104448 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 105000      |
| train/                  |             |
|    approx_kl            | 0.021570751 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.93       |
|    explained_variance   | 0.61        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0417     |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.0385     |
|    value_loss           | 0.00127     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 52     |
|    time_elapsed    | 10875  |
|    total_timesteps | 106496 |
-------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+04     |
|    mean_reward          | 0         |
| time/                   |           |
|    total_timesteps      | 107000    |
| train/                  |           |
|    approx_kl            | 0.0231137 |
|    clip_fraction        | 0.256     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.86     |
|    explained_variance   | 0.727     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.00839  |
|    n_updates            | 520       |
|    policy_gradient_loss | -0.0368   |
|    value_loss           | 0.00085   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 53     |
|    time_elapsed    | 11080  |
|    total_timesteps | 108544 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 109000      |
| train/                  |             |
|    approx_kl            | 0.025984585 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.85       |
|    explained_variance   | 0.714       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0167     |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.0303     |
|    value_loss           | 0.000499    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 54     |
|    time_elapsed    | 11286  |
|    total_timesteps | 110592 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 111000      |
| train/                  |             |
|    approx_kl            | 0.022257533 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.74       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0277     |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.0342     |
|    value_loss           | 0.00105     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 55     |
|    time_elapsed    | 11491  |
|    total_timesteps | 112640 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 113000      |
| train/                  |             |
|    approx_kl            | 0.022514502 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.82       |
|    explained_variance   | 0.847       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0697     |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.0376     |
|    value_loss           | 0.00133     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 56     |
|    time_elapsed    | 11696  |
|    total_timesteps | 114688 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 115000      |
| train/                  |             |
|    approx_kl            | 0.022149127 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.88       |
|    explained_variance   | 0.823       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0741     |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.0381     |
|    value_loss           | 0.00274     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 57     |
|    time_elapsed    | 11902  |
|    total_timesteps | 116736 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 117000      |
| train/                  |             |
|    approx_kl            | 0.022355475 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.81       |
|    explained_variance   | 0.765       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0346     |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.036      |
|    value_loss           | 0.0011      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 58     |
|    time_elapsed    | 12107  |
|    total_timesteps | 118784 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 119000     |
| train/                  |            |
|    approx_kl            | 0.02257966 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.63      |
|    explained_variance   | 0.626      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0388    |
|    n_updates            | 580        |
|    policy_gradient_loss | -0.0304    |
|    value_loss           | 0.00154    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 59     |
|    time_elapsed    | 12312  |
|    total_timesteps | 120832 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 121000     |
| train/                  |            |
|    approx_kl            | 0.02674165 |
|    clip_fraction        | 0.28       |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.84      |
|    explained_variance   | 0.804      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0706    |
|    n_updates            | 590        |
|    policy_gradient_loss | -0.0397    |
|    value_loss           | 0.000665   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 60     |
|    time_elapsed    | 12518  |
|    total_timesteps | 122880 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 123000      |
| train/                  |             |
|    approx_kl            | 0.026156915 |
|    clip_fraction        | 0.283       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.81       |
|    explained_variance   | 0.861       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0705     |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.0351     |
|    value_loss           | 0.000798    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 61     |
|    time_elapsed    | 12723  |
|    total_timesteps | 124928 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 125000     |
| train/                  |            |
|    approx_kl            | 0.02130628 |
|    clip_fraction        | 0.238      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.65      |
|    explained_variance   | 0.832      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0452    |
|    n_updates            | 610        |
|    policy_gradient_loss | -0.0394    |
|    value_loss           | 0.00155    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 62     |
|    time_elapsed    | 12928  |
|    total_timesteps | 126976 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 127000      |
| train/                  |             |
|    approx_kl            | 0.023777328 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.86       |
|    explained_variance   | 0.801       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.066      |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.0414     |
|    value_loss           | 0.00149     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 63     |
|    time_elapsed    | 13234  |
|    total_timesteps | 129024 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 130000      |
| train/                  |             |
|    approx_kl            | 0.023608882 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.85       |
|    explained_variance   | 0.763       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0634     |
|    n_updates            | 630         |
|    policy_gradient_loss | -0.0321     |
|    value_loss           | 0.00192     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 64     |
|    time_elapsed    | 13439  |
|    total_timesteps | 131072 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 132000      |
| train/                  |             |
|    approx_kl            | 0.022296257 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.85       |
|    explained_variance   | 0.618       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0364     |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.0361     |
|    value_loss           | 0.0007      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 65     |
|    time_elapsed    | 13644  |
|    total_timesteps | 133120 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 134000      |
| train/                  |             |
|    approx_kl            | 0.026098564 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.8        |
|    explained_variance   | 0.887       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0517     |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.037      |
|    value_loss           | 0.000891    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 66     |
|    time_elapsed    | 13850  |
|    total_timesteps | 135168 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 136000     |
| train/                  |            |
|    approx_kl            | 0.01862576 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.83      |
|    explained_variance   | 0.823      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0452    |
|    n_updates            | 660        |
|    policy_gradient_loss | -0.0323    |
|    value_loss           | 0.00105    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 67     |
|    time_elapsed    | 14055  |
|    total_timesteps | 137216 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 138000      |
| train/                  |             |
|    approx_kl            | 0.023558024 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.78       |
|    explained_variance   | 0.78        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0387     |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.0323     |
|    value_loss           | 0.00464     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 68     |
|    time_elapsed    | 14260  |
|    total_timesteps | 139264 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 140000      |
| train/                  |             |
|    approx_kl            | 0.021590492 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.66       |
|    explained_variance   | 0.807       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0419     |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.0355     |
|    value_loss           | 0.000752    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 69     |
|    time_elapsed    | 14465  |
|    total_timesteps | 141312 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 142000      |
| train/                  |             |
|    approx_kl            | 0.028195582 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.61       |
|    explained_variance   | 0.866       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0296     |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.0332     |
|    value_loss           | 0.00058     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 70     |
|    time_elapsed    | 14671  |
|    total_timesteps | 143360 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 144000      |
| train/                  |             |
|    approx_kl            | 0.024245499 |
|    clip_fraction        | 0.282       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.56       |
|    explained_variance   | 0.646       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0538     |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.0368     |
|    value_loss           | 0.0015      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 71     |
|    time_elapsed    | 14876  |
|    total_timesteps | 145408 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 146000      |
| train/                  |             |
|    approx_kl            | 0.022401866 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.69       |
|    explained_variance   | 0.71        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0399     |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.0337     |
|    value_loss           | 0.00117     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 72     |
|    time_elapsed    | 15081  |
|    total_timesteps | 147456 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 148000      |
| train/                  |             |
|    approx_kl            | 0.022418117 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.65       |
|    explained_variance   | 0.68        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.079      |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.0395     |
|    value_loss           | 0.000744    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 73     |
|    time_elapsed    | 15287  |
|    total_timesteps | 149504 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.022483215 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.75       |
|    explained_variance   | 0.509       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0637     |
|    n_updates            | 730         |
|    policy_gradient_loss | -0.0397     |
|    value_loss           | 0.000983    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 151000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 74     |
|    time_elapsed    | 15492  |
|    total_timesteps | 151552 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 152000      |
| train/                  |             |
|    approx_kl            | 0.023177002 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.61       |
|    explained_variance   | 0.538       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0242     |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.0313     |
|    value_loss           | 0.000565    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 153000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 75     |
|    time_elapsed    | 15697  |
|    total_timesteps | 153600 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 154000      |
| train/                  |             |
|    approx_kl            | 0.020227633 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.64       |
|    explained_variance   | 0.439       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0577     |
|    n_updates            | 750         |
|    policy_gradient_loss | -0.0321     |
|    value_loss           | 0.00121     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 76     |
|    time_elapsed    | 15903  |
|    total_timesteps | 155648 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 156000      |
| train/                  |             |
|    approx_kl            | 0.024518397 |
|    clip_fraction        | 0.282       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.58       |
|    explained_variance   | 0.777       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0621     |
|    n_updates            | 760         |
|    policy_gradient_loss | -0.0394     |
|    value_loss           | 0.00132     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 77     |
|    time_elapsed    | 16108  |
|    total_timesteps | 157696 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.3        |
| time/                   |            |
|    total_timesteps      | 158000     |
| train/                  |            |
|    approx_kl            | 0.02256905 |
|    clip_fraction        | 0.241      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.56      |
|    explained_variance   | 0.477      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0608    |
|    n_updates            | 770        |
|    policy_gradient_loss | -0.033     |
|    value_loss           | 0.000952   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 159000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 78     |
|    time_elapsed    | 16313  |
|    total_timesteps | 159744 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 160000      |
| train/                  |             |
|    approx_kl            | 0.023906197 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.48       |
|    explained_variance   | 0.893       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0633     |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.038      |
|    value_loss           | 0.000447    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 161000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 79     |
|    time_elapsed    | 16518  |
|    total_timesteps | 161792 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 162000     |
| train/                  |            |
|    approx_kl            | 0.02362942 |
|    clip_fraction        | 0.258      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.45      |
|    explained_variance   | 0.855      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0162    |
|    n_updates            | 790        |
|    policy_gradient_loss | -0.0322    |
|    value_loss           | 0.00201    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 163000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 80     |
|    time_elapsed    | 16724  |
|    total_timesteps | 163840 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 164000      |
| train/                  |             |
|    approx_kl            | 0.024343275 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.33       |
|    explained_variance   | 0.825       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.059      |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.034      |
|    value_loss           | 0.000706    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 165000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 81     |
|    time_elapsed    | 16929  |
|    total_timesteps | 165888 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 166000      |
| train/                  |             |
|    approx_kl            | 0.017686758 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.34       |
|    explained_variance   | 0.665       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0292     |
|    n_updates            | 810         |
|    policy_gradient_loss | -0.0258     |
|    value_loss           | 0.0012      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 167000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 82     |
|    time_elapsed    | 17134  |
|    total_timesteps | 167936 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.3        |
| time/                   |            |
|    total_timesteps      | 168000     |
| train/                  |            |
|    approx_kl            | 0.02485807 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.32      |
|    explained_variance   | 0.664      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.078     |
|    n_updates            | 820        |
|    policy_gradient_loss | -0.0287    |
|    value_loss           | 0.000755   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 169000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 83     |
|    time_elapsed    | 17340  |
|    total_timesteps | 169984 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 170000      |
| train/                  |             |
|    approx_kl            | 0.019411858 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.31       |
|    explained_variance   | 0.567       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0383     |
|    n_updates            | 830         |
|    policy_gradient_loss | -0.0309     |
|    value_loss           | 0.000534    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 84     |
|    time_elapsed    | 17645  |
|    total_timesteps | 172032 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 173000      |
| train/                  |             |
|    approx_kl            | 0.023255087 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.45       |
|    explained_variance   | 0.657       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0173     |
|    n_updates            | 840         |
|    policy_gradient_loss | -0.034      |
|    value_loss           | 0.000989    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 85     |
|    time_elapsed    | 17850  |
|    total_timesteps | 174080 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 175000     |
| train/                  |            |
|    approx_kl            | 0.02047402 |
|    clip_fraction        | 0.264      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.17      |
|    explained_variance   | 0.822      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0493    |
|    n_updates            | 850        |
|    policy_gradient_loss | -0.0302    |
|    value_loss           | 0.000878   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 86     |
|    time_elapsed    | 18056  |
|    total_timesteps | 176128 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 177000      |
| train/                  |             |
|    approx_kl            | 0.020735163 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.26       |
|    explained_variance   | 0.686       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0149     |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.0262     |
|    value_loss           | 0.00157     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 87     |
|    time_elapsed    | 18261  |
|    total_timesteps | 178176 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 179000      |
| train/                  |             |
|    approx_kl            | 0.027367296 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.52       |
|    explained_variance   | 0.675       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0512     |
|    n_updates            | 870         |
|    policy_gradient_loss | -0.0328     |
|    value_loss           | 0.00157     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 88     |
|    time_elapsed    | 18466  |
|    total_timesteps | 180224 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 181000      |
| train/                  |             |
|    approx_kl            | 0.023433164 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.44       |
|    explained_variance   | 0.882       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0289     |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.0355     |
|    value_loss           | 0.000783    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 89     |
|    time_elapsed    | 18672  |
|    total_timesteps | 182272 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 183000     |
| train/                  |            |
|    approx_kl            | 0.02072959 |
|    clip_fraction        | 0.27       |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.29      |
|    explained_variance   | 0.757      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0403    |
|    n_updates            | 890        |
|    policy_gradient_loss | -0.0315    |
|    value_loss           | 0.00184    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 90     |
|    time_elapsed    | 18877  |
|    total_timesteps | 184320 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 185000      |
| train/                  |             |
|    approx_kl            | 0.022725169 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.2        |
|    explained_variance   | 0.6         |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0525     |
|    n_updates            | 900         |
|    policy_gradient_loss | -0.0352     |
|    value_loss           | 0.000796    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 91     |
|    time_elapsed    | 19082  |
|    total_timesteps | 186368 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 187000      |
| train/                  |             |
|    approx_kl            | 0.020850098 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.22       |
|    explained_variance   | 0.761       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0247     |
|    n_updates            | 910         |
|    policy_gradient_loss | -0.0297     |
|    value_loss           | 0.00183     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 92     |
|    time_elapsed    | 19287  |
|    total_timesteps | 188416 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 189000      |
| train/                  |             |
|    approx_kl            | 0.022413453 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.33       |
|    explained_variance   | 0.844       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0274     |
|    n_updates            | 920         |
|    policy_gradient_loss | -0.0235     |
|    value_loss           | 0.00231     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 93     |
|    time_elapsed    | 19493  |
|    total_timesteps | 190464 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 191000      |
| train/                  |             |
|    approx_kl            | 0.029373404 |
|    clip_fraction        | 0.282       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.28       |
|    explained_variance   | 0.636       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0208     |
|    n_updates            | 930         |
|    policy_gradient_loss | -0.0284     |
|    value_loss           | 0.00121     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 94     |
|    time_elapsed    | 19698  |
|    total_timesteps | 192512 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 193000      |
| train/                  |             |
|    approx_kl            | 0.020557173 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.31       |
|    explained_variance   | 0.792       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0398     |
|    n_updates            | 940         |
|    policy_gradient_loss | -0.0272     |
|    value_loss           | 0.00149     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 95     |
|    time_elapsed    | 19903  |
|    total_timesteps | 194560 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 195000      |
| train/                  |             |
|    approx_kl            | 0.024064481 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.39       |
|    explained_variance   | 0.636       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.062      |
|    n_updates            | 950         |
|    policy_gradient_loss | -0.0312     |
|    value_loss           | 0.00139     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 96     |
|    time_elapsed    | 20109  |
|    total_timesteps | 196608 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 197000      |
| train/                  |             |
|    approx_kl            | 0.020347904 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.14       |
|    explained_variance   | 0.642       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0483     |
|    n_updates            | 960         |
|    policy_gradient_loss | -0.0268     |
|    value_loss           | 0.000756    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 97     |
|    time_elapsed    | 20314  |
|    total_timesteps | 198656 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 199000     |
| train/                  |            |
|    approx_kl            | 0.02353512 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.24      |
|    explained_variance   | 0.524      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0273    |
|    n_updates            | 970        |
|    policy_gradient_loss | -0.0313    |
|    value_loss           | 0.000844   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 98     |
|    time_elapsed    | 20519  |
|    total_timesteps | 200704 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 201000      |
| train/                  |             |
|    approx_kl            | 0.022315314 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.25       |
|    explained_variance   | 0.495       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0427     |
|    n_updates            | 980         |
|    policy_gradient_loss | -0.0353     |
|    value_loss           | 0.00132     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 99     |
|    time_elapsed    | 20725  |
|    total_timesteps | 202752 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 203000      |
| train/                  |             |
|    approx_kl            | 0.022966947 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.12       |
|    explained_variance   | 0.688       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0428     |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.0322     |
|    value_loss           | 0.000935    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 100    |
|    time_elapsed    | 20930  |
|    total_timesteps | 204800 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 205000     |
| train/                  |            |
|    approx_kl            | 0.02121573 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.99      |
|    explained_variance   | 0.601      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0339    |
|    n_updates            | 1000       |
|    policy_gradient_loss | -0.028     |
|    value_loss           | 0.00114    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 101    |
|    time_elapsed    | 21135  |
|    total_timesteps | 206848 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 207000      |
| train/                  |             |
|    approx_kl            | 0.020243349 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.01       |
|    explained_variance   | 0.669       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0257     |
|    n_updates            | 1010        |
|    policy_gradient_loss | -0.0254     |
|    value_loss           | 0.00154     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 102    |
|    time_elapsed    | 21341  |
|    total_timesteps | 208896 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 209000      |
| train/                  |             |
|    approx_kl            | 0.018749561 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.99       |
|    explained_variance   | 0.464       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0527     |
|    n_updates            | 1020        |
|    policy_gradient_loss | -0.0227     |
|    value_loss           | 0.00116     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 103    |
|    time_elapsed    | 21546  |
|    total_timesteps | 210944 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 211000      |
| train/                  |             |
|    approx_kl            | 0.021225294 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.98       |
|    explained_variance   | 0.801       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0279     |
|    n_updates            | 1030        |
|    policy_gradient_loss | -0.03       |
|    value_loss           | 0.00107     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 104    |
|    time_elapsed    | 21751  |
|    total_timesteps | 212992 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 213000      |
| train/                  |             |
|    approx_kl            | 0.023355175 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.95       |
|    explained_variance   | 0.576       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0352     |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.0309     |
|    value_loss           | 0.0012      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 105    |
|    time_elapsed    | 22057  |
|    total_timesteps | 215040 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 216000     |
| train/                  |            |
|    approx_kl            | 0.02372455 |
|    clip_fraction        | 0.274      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.04      |
|    explained_variance   | 0.694      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0421    |
|    n_updates            | 1050       |
|    policy_gradient_loss | -0.0239    |
|    value_loss           | 0.0015     |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 106    |
|    time_elapsed    | 22262  |
|    total_timesteps | 217088 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 218000      |
| train/                  |             |
|    approx_kl            | 0.018638609 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.77       |
|    explained_variance   | 0.513       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0507     |
|    n_updates            | 1060        |
|    policy_gradient_loss | -0.0211     |
|    value_loss           | 0.000905    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 107    |
|    time_elapsed    | 22467  |
|    total_timesteps | 219136 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 220000      |
| train/                  |             |
|    approx_kl            | 0.021470975 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.77       |
|    explained_variance   | 0.675       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0509     |
|    n_updates            | 1070        |
|    policy_gradient_loss | -0.0256     |
|    value_loss           | 0.000928    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 108    |
|    time_elapsed    | 22672  |
|    total_timesteps | 221184 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 222000      |
| train/                  |             |
|    approx_kl            | 0.022697188 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.56       |
|    explained_variance   | 0.709       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0321     |
|    n_updates            | 1080        |
|    policy_gradient_loss | -0.0305     |
|    value_loss           | 0.00103     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 109    |
|    time_elapsed    | 22878  |
|    total_timesteps | 223232 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 224000      |
| train/                  |             |
|    approx_kl            | 0.020598471 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.91       |
|    explained_variance   | 0.428       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0412     |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.0274     |
|    value_loss           | 0.00105     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 110    |
|    time_elapsed    | 23083  |
|    total_timesteps | 225280 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 226000      |
| train/                  |             |
|    approx_kl            | 0.015678003 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.68       |
|    explained_variance   | 0.514       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0278     |
|    n_updates            | 1100        |
|    policy_gradient_loss | -0.021      |
|    value_loss           | 0.00132     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 111    |
|    time_elapsed    | 23288  |
|    total_timesteps | 227328 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 228000      |
| train/                  |             |
|    approx_kl            | 0.019505538 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.71       |
|    explained_variance   | 0.554       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0179     |
|    n_updates            | 1110        |
|    policy_gradient_loss | -0.0227     |
|    value_loss           | 0.00142     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 112    |
|    time_elapsed    | 23494  |
|    total_timesteps | 229376 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 230000      |
| train/                  |             |
|    approx_kl            | 0.019420173 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.52       |
|    explained_variance   | 0.603       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00181    |
|    n_updates            | 1120        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.00206     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 113    |
|    time_elapsed    | 23699  |
|    total_timesteps | 231424 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 232000      |
| train/                  |             |
|    approx_kl            | 0.021914069 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.69       |
|    explained_variance   | 0.608       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0582     |
|    n_updates            | 1130        |
|    policy_gradient_loss | -0.0259     |
|    value_loss           | 0.00171     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 114    |
|    time_elapsed    | 23904  |
|    total_timesteps | 233472 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 234000      |
| train/                  |             |
|    approx_kl            | 0.019332387 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.69       |
|    explained_variance   | 0.355       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0413     |
|    n_updates            | 1140        |
|    policy_gradient_loss | -0.0255     |
|    value_loss           | 0.00134     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 115    |
|    time_elapsed    | 24110  |
|    total_timesteps | 235520 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.6        |
| time/                   |            |
|    total_timesteps      | 236000     |
| train/                  |            |
|    approx_kl            | 0.01859372 |
|    clip_fraction        | 0.2        |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.95      |
|    explained_variance   | 0.323      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0251    |
|    n_updates            | 1150       |
|    policy_gradient_loss | -0.0146    |
|    value_loss           | 0.00186    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 237000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 116    |
|    time_elapsed    | 24315  |
|    total_timesteps | 237568 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 238000      |
| train/                  |             |
|    approx_kl            | 0.015948705 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.72       |
|    explained_variance   | 0.844       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0381     |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.0192     |
|    value_loss           | 0.00113     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 239000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 117    |
|    time_elapsed    | 24520  |
|    total_timesteps | 239616 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 0.025495049 |
|    clip_fraction        | 0.27        |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.59       |
|    explained_variance   | 0.756       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0693     |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.0285     |
|    value_loss           | 0.0016      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 241000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 118    |
|    time_elapsed    | 24726  |
|    total_timesteps | 241664 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 242000      |
| train/                  |             |
|    approx_kl            | 0.021387318 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.76       |
|    explained_variance   | 0.565       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0235     |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.0258     |
|    value_loss           | 0.00149     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 243000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 119    |
|    time_elapsed    | 24931  |
|    total_timesteps | 243712 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 244000      |
| train/                  |             |
|    approx_kl            | 0.016486527 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.91       |
|    explained_variance   | 0.551       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0171     |
|    n_updates            | 1190        |
|    policy_gradient_loss | -0.0205     |
|    value_loss           | 0.00133     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 120    |
|    time_elapsed    | 25136  |
|    total_timesteps | 245760 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 246000     |
| train/                  |            |
|    approx_kl            | 0.01670647 |
|    clip_fraction        | 0.188      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.83      |
|    explained_variance   | 0.667      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0391    |
|    n_updates            | 1200       |
|    policy_gradient_loss | -0.0165    |
|    value_loss           | 0.00146    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 247000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 121    |
|    time_elapsed    | 25342  |
|    total_timesteps | 247808 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 248000      |
| train/                  |             |
|    approx_kl            | 0.018680315 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.85       |
|    explained_variance   | 0.462       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0313     |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.0192     |
|    value_loss           | 0.00174     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 122    |
|    time_elapsed    | 25547  |
|    total_timesteps | 249856 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.018905444 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.9        |
|    explained_variance   | 0.584       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0374     |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.0206     |
|    value_loss           | 0.00164     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 123    |
|    time_elapsed    | 25752  |
|    total_timesteps | 251904 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 252000      |
| train/                  |             |
|    approx_kl            | 0.017880898 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.86       |
|    explained_variance   | 0.286       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0245     |
|    n_updates            | 1230        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.00221     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 253000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 124    |
|    time_elapsed    | 25958  |
|    total_timesteps | 253952 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 254000      |
| train/                  |             |
|    approx_kl            | 0.021467045 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.69       |
|    explained_variance   | 0.575       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0507     |
|    n_updates            | 1240        |
|    policy_gradient_loss | -0.0226     |
|    value_loss           | 0.00184     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 255000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 125    |
|    time_elapsed    | 26263  |
|    total_timesteps | 256000 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 257000      |
| train/                  |             |
|    approx_kl            | 0.015395109 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.83       |
|    explained_variance   | 0.548       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0101      |
|    n_updates            | 1250        |
|    policy_gradient_loss | -0.0202     |
|    value_loss           | 0.00134     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 126    |
|    time_elapsed    | 26468  |
|    total_timesteps | 258048 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 259000      |
| train/                  |             |
|    approx_kl            | 0.016057052 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.6        |
|    explained_variance   | 0.62        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0306     |
|    n_updates            | 1260        |
|    policy_gradient_loss | -0.0185     |
|    value_loss           | 0.00196     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 127    |
|    time_elapsed    | 26674  |
|    total_timesteps | 260096 |
-------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+04     |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 261000    |
| train/                  |           |
|    approx_kl            | 0.0207376 |
|    clip_fraction        | 0.243     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.65     |
|    explained_variance   | 0.509     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0292   |
|    n_updates            | 1270      |
|    policy_gradient_loss | -0.0209   |
|    value_loss           | 0.00226   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 128    |
|    time_elapsed    | 26879  |
|    total_timesteps | 262144 |
-------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+04     |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 263000    |
| train/                  |           |
|    approx_kl            | 0.0172041 |
|    clip_fraction        | 0.233     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.48     |
|    explained_variance   | 0.416     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0143   |
|    n_updates            | 1280      |
|    policy_gradient_loss | -0.0168   |
|    value_loss           | 0.00298   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 129    |
|    time_elapsed    | 27084  |
|    total_timesteps | 264192 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 265000      |
| train/                  |             |
|    approx_kl            | 0.020562187 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.49       |
|    explained_variance   | 0.472       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0354     |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.0182     |
|    value_loss           | 0.00343     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 130    |
|    time_elapsed    | 27290  |
|    total_timesteps | 266240 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 267000      |
| train/                  |             |
|    approx_kl            | 0.020302165 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.77       |
|    explained_variance   | 0.43        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.029      |
|    n_updates            | 1300        |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.00314     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 131    |
|    time_elapsed    | 27495  |
|    total_timesteps | 268288 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.7        |
| time/                   |            |
|    total_timesteps      | 269000     |
| train/                  |            |
|    approx_kl            | 0.02061665 |
|    clip_fraction        | 0.258      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.75      |
|    explained_variance   | 0.456      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0141     |
|    n_updates            | 1310       |
|    policy_gradient_loss | -0.0169    |
|    value_loss           | 0.00396    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 132    |
|    time_elapsed    | 27700  |
|    total_timesteps | 270336 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 271000      |
| train/                  |             |
|    approx_kl            | 0.019007057 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.7        |
|    explained_variance   | 0.496       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0053      |
|    n_updates            | 1320        |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 0.00366     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 133    |
|    time_elapsed    | 27906  |
|    total_timesteps | 272384 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 273000      |
| train/                  |             |
|    approx_kl            | 0.015107739 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.69       |
|    explained_variance   | 0.424       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0156     |
|    n_updates            | 1330        |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 0.00358     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 134    |
|    time_elapsed    | 28111  |
|    total_timesteps | 274432 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.7        |
| time/                   |            |
|    total_timesteps      | 275000     |
| train/                  |            |
|    approx_kl            | 0.02618123 |
|    clip_fraction        | 0.274      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.55      |
|    explained_variance   | 0.569      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0038     |
|    n_updates            | 1340       |
|    policy_gradient_loss | -0.0158    |
|    value_loss           | 0.00364    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 135    |
|    time_elapsed    | 28316  |
|    total_timesteps | 276480 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 277000      |
| train/                  |             |
|    approx_kl            | 0.018123966 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.45       |
|    explained_variance   | 0.55        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0235     |
|    n_updates            | 1350        |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.00319     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 278000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 136    |
|    time_elapsed    | 28521  |
|    total_timesteps | 278528 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 279000      |
| train/                  |             |
|    approx_kl            | 0.018506283 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.25       |
|    explained_variance   | 0.482       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0122     |
|    n_updates            | 1360        |
|    policy_gradient_loss | -0.0137     |
|    value_loss           | 0.0034      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 280000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 137    |
|    time_elapsed    | 28727  |
|    total_timesteps | 280576 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 281000      |
| train/                  |             |
|    approx_kl            | 0.015962819 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.25       |
|    explained_variance   | 0.464       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0122     |
|    n_updates            | 1370        |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 0.00339     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 282000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 138    |
|    time_elapsed    | 28932  |
|    total_timesteps | 282624 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.7        |
| time/                   |            |
|    total_timesteps      | 283000     |
| train/                  |            |
|    approx_kl            | 0.02050253 |
|    clip_fraction        | 0.255      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.29      |
|    explained_variance   | 0.469      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0187    |
|    n_updates            | 1380       |
|    policy_gradient_loss | -0.013     |
|    value_loss           | 0.00349    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 284000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 139    |
|    time_elapsed    | 29137  |
|    total_timesteps | 284672 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 285000      |
| train/                  |             |
|    approx_kl            | 0.018671483 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.25       |
|    explained_variance   | 0.428       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00347     |
|    n_updates            | 1390        |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.0035      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 286000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 140    |
|    time_elapsed    | 29343  |
|    total_timesteps | 286720 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 287000      |
| train/                  |             |
|    approx_kl            | 0.016025256 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.25       |
|    explained_variance   | 0.338       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0198     |
|    n_updates            | 1400        |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 0.00424     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 288000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 141    |
|    time_elapsed    | 29548  |
|    total_timesteps | 288768 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 289000      |
| train/                  |             |
|    approx_kl            | 0.014750896 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.15       |
|    explained_variance   | 0.609       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00253    |
|    n_updates            | 1410        |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 0.00272     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 290000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 142    |
|    time_elapsed    | 29753  |
|    total_timesteps | 290816 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 291000      |
| train/                  |             |
|    approx_kl            | 0.015394671 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.29       |
|    explained_variance   | 0.406       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00467     |
|    n_updates            | 1420        |
|    policy_gradient_loss | -0.0098     |
|    value_loss           | 0.00431     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 292000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 143    |
|    time_elapsed    | 29959  |
|    total_timesteps | 292864 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 293000      |
| train/                  |             |
|    approx_kl            | 0.015148252 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.29       |
|    explained_variance   | 0.442       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0444      |
|    n_updates            | 1430        |
|    policy_gradient_loss | -0.0131     |
|    value_loss           | 0.00407     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 294000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 144    |
|    time_elapsed    | 30164  |
|    total_timesteps | 294912 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 295000      |
| train/                  |             |
|    approx_kl            | 0.019399442 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.15       |
|    explained_variance   | 0.655       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0202     |
|    n_updates            | 1440        |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 0.00375     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 296000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 145    |
|    time_elapsed    | 30369  |
|    total_timesteps | 296960 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 297000      |
| train/                  |             |
|    approx_kl            | 0.018459268 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.21       |
|    explained_variance   | 0.441       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0204     |
|    n_updates            | 1450        |
|    policy_gradient_loss | -0.00937    |
|    value_loss           | 0.00439     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 298000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 146    |
|    time_elapsed    | 30675  |
|    total_timesteps | 299008 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 300000      |
| train/                  |             |
|    approx_kl            | 0.018772338 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.17       |
|    explained_variance   | 0.452       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00883    |
|    n_updates            | 1460        |
|    policy_gradient_loss | -0.00912    |
|    value_loss           | 0.00642     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 301000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 147    |
|    time_elapsed    | 30880  |
|    total_timesteps | 301056 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 302000      |
| train/                  |             |
|    approx_kl            | 0.018053059 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.13       |
|    explained_variance   | 0.427       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0223     |
|    n_updates            | 1470        |
|    policy_gradient_loss | -0.00916    |
|    value_loss           | 0.00468     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 303000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 148    |
|    time_elapsed    | 31085  |
|    total_timesteps | 303104 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 304000      |
| train/                  |             |
|    approx_kl            | 0.014240346 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.09       |
|    explained_variance   | 0.47        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00553    |
|    n_updates            | 1480        |
|    policy_gradient_loss | -0.00853    |
|    value_loss           | 0.00435     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 305000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 149    |
|    time_elapsed    | 31291  |
|    total_timesteps | 305152 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 306000      |
| train/                  |             |
|    approx_kl            | 0.014438691 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6          |
|    explained_variance   | 0.4         |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00244     |
|    n_updates            | 1490        |
|    policy_gradient_loss | -0.00821    |
|    value_loss           | 0.00453     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 307000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 150    |
|    time_elapsed    | 31496  |
|    total_timesteps | 307200 |
-------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+04     |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 308000    |
| train/                  |           |
|    approx_kl            | 0.0174307 |
|    clip_fraction        | 0.205     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.94     |
|    explained_variance   | 0.388     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0133   |
|    n_updates            | 1500      |
|    policy_gradient_loss | -0.00783  |
|    value_loss           | 0.00463   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 309000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 151    |
|    time_elapsed    | 31701  |
|    total_timesteps | 309248 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 310000      |
| train/                  |             |
|    approx_kl            | 0.015841387 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.81       |
|    explained_variance   | 0.513       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0142     |
|    n_updates            | 1510        |
|    policy_gradient_loss | -0.0142     |
|    value_loss           | 0.00336     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 311000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 152    |
|    time_elapsed    | 31907  |
|    total_timesteps | 311296 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 312000      |
| train/                  |             |
|    approx_kl            | 0.019732142 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.77       |
|    explained_variance   | 0.359       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00636     |
|    n_updates            | 1520        |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 0.00486     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 313000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 153    |
|    time_elapsed    | 32112  |
|    total_timesteps | 313344 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 314000      |
| train/                  |             |
|    approx_kl            | 0.012291275 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.64       |
|    explained_variance   | 0.411       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0153      |
|    n_updates            | 1530        |
|    policy_gradient_loss | -0.00602    |
|    value_loss           | 0.00475     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 315000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 154    |
|    time_elapsed    | 32317  |
|    total_timesteps | 315392 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 316000      |
| train/                  |             |
|    approx_kl            | 0.012599665 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.65       |
|    explained_variance   | 0.321       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.02        |
|    n_updates            | 1540        |
|    policy_gradient_loss | -0.00362    |
|    value_loss           | 0.00489     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 317000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 155    |
|    time_elapsed    | 32523  |
|    total_timesteps | 317440 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 318000     |
| train/                  |            |
|    approx_kl            | 0.01649126 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.54      |
|    explained_variance   | 0.443      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00618   |
|    n_updates            | 1550       |
|    policy_gradient_loss | -0.00776   |
|    value_loss           | 0.00468    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 319000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 156    |
|    time_elapsed    | 32728  |
|    total_timesteps | 319488 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 320000      |
| train/                  |             |
|    approx_kl            | 0.012087816 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.45       |
|    explained_variance   | 0.401       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0133      |
|    n_updates            | 1560        |
|    policy_gradient_loss | -0.00665    |
|    value_loss           | 0.0049      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 321000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 157    |
|    time_elapsed    | 32933  |
|    total_timesteps | 321536 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 322000      |
| train/                  |             |
|    approx_kl            | 0.012707167 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.47       |
|    explained_variance   | 0.385       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00849    |
|    n_updates            | 1570        |
|    policy_gradient_loss | -0.00779    |
|    value_loss           | 0.00503     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 323000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 158    |
|    time_elapsed    | 33139  |
|    total_timesteps | 323584 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 324000      |
| train/                  |             |
|    approx_kl            | 0.017723186 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.36       |
|    explained_variance   | 0.319       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0329      |
|    n_updates            | 1580        |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 0.00539     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 325000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 159    |
|    time_elapsed    | 33344  |
|    total_timesteps | 325632 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 326000      |
| train/                  |             |
|    approx_kl            | 0.016765615 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.33       |
|    explained_variance   | 0.386       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.016      |
|    n_updates            | 1590        |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 0.00533     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 327000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 160    |
|    time_elapsed    | 33549  |
|    total_timesteps | 327680 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 328000      |
| train/                  |             |
|    approx_kl            | 0.010370977 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.23       |
|    explained_variance   | 0.423       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0221     |
|    n_updates            | 1600        |
|    policy_gradient_loss | -0.00508    |
|    value_loss           | 0.0048      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 329000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 161    |
|    time_elapsed    | 33754  |
|    total_timesteps | 329728 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 330000      |
| train/                  |             |
|    approx_kl            | 0.012047295 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.2        |
|    explained_variance   | 0.411       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0266     |
|    n_updates            | 1610        |
|    policy_gradient_loss | -0.00809    |
|    value_loss           | 0.00434     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 331000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 162    |
|    time_elapsed    | 33960  |
|    total_timesteps | 331776 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 0.9          |
| time/                   |              |
|    total_timesteps      | 332000       |
| train/                  |              |
|    approx_kl            | 0.0132571915 |
|    clip_fraction        | 0.179        |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.08        |
|    explained_variance   | 0.403        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.012        |
|    n_updates            | 1620         |
|    policy_gradient_loss | -0.00533     |
|    value_loss           | 0.00522      |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 333000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 163    |
|    time_elapsed    | 34165  |
|    total_timesteps | 333824 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 334000      |
| train/                  |             |
|    approx_kl            | 0.019505046 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.02       |
|    explained_variance   | 0.353       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0245     |
|    n_updates            | 1630        |
|    policy_gradient_loss | -0.00746    |
|    value_loss           | 0.00537     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 335000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 164    |
|    time_elapsed    | 34370  |
|    total_timesteps | 335872 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 336000      |
| train/                  |             |
|    approx_kl            | 0.014833377 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.98       |
|    explained_variance   | 0.332       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00677     |
|    n_updates            | 1640        |
|    policy_gradient_loss | -0.00589    |
|    value_loss           | 0.00554     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 337000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 165    |
|    time_elapsed    | 34576  |
|    total_timesteps | 337920 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 338000      |
| train/                  |             |
|    approx_kl            | 0.016371723 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.99       |
|    explained_variance   | 0.375       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0031      |
|    n_updates            | 1650        |
|    policy_gradient_loss | -0.00729    |
|    value_loss           | 0.00545     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 339000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 166    |
|    time_elapsed    | 34781  |
|    total_timesteps | 339968 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 340000      |
| train/                  |             |
|    approx_kl            | 0.013254166 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.87       |
|    explained_variance   | 0.413       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00141     |
|    n_updates            | 1660        |
|    policy_gradient_loss | -0.00361    |
|    value_loss           | 0.00529     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 341000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 342000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 167    |
|    time_elapsed    | 35086  |
|    total_timesteps | 342016 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 343000     |
| train/                  |            |
|    approx_kl            | 0.01631702 |
|    clip_fraction        | 0.249      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.78      |
|    explained_variance   | 0.375      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0222     |
|    n_updates            | 1670       |
|    policy_gradient_loss | -0.00813   |
|    value_loss           | 0.00768    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 344000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 168    |
|    time_elapsed    | 35292  |
|    total_timesteps | 344064 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 345000      |
| train/                  |             |
|    approx_kl            | 0.012875936 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.7        |
|    explained_variance   | 0.323       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00199     |
|    n_updates            | 1680        |
|    policy_gradient_loss | -0.00589    |
|    value_loss           | 0.00545     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 346000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 169    |
|    time_elapsed    | 35497  |
|    total_timesteps | 346112 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 347000      |
| train/                  |             |
|    approx_kl            | 0.017657544 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.332       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0321      |
|    n_updates            | 1690        |
|    policy_gradient_loss | -0.00495    |
|    value_loss           | 0.0054      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 348000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 170    |
|    time_elapsed    | 35702  |
|    total_timesteps | 348160 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 349000     |
| train/                  |            |
|    approx_kl            | 0.01663977 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.4       |
|    explained_variance   | 0.411      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00634   |
|    n_updates            | 1700       |
|    policy_gradient_loss | -0.00599   |
|    value_loss           | 0.00528    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 350000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 171    |
|    time_elapsed    | 35908  |
|    total_timesteps | 350208 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 351000      |
| train/                  |             |
|    approx_kl            | 0.012290539 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.32       |
|    explained_variance   | 0.333       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00521    |
|    n_updates            | 1710        |
|    policy_gradient_loss | -0.00429    |
|    value_loss           | 0.00542     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 352000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 172    |
|    time_elapsed    | 36113  |
|    total_timesteps | 352256 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 353000      |
| train/                  |             |
|    approx_kl            | 0.015924474 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.11       |
|    explained_variance   | 0.351       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0244     |
|    n_updates            | 1720        |
|    policy_gradient_loss | -0.00593    |
|    value_loss           | 0.00538     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 354000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 173    |
|    time_elapsed    | 36318  |
|    total_timesteps | 354304 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 355000      |
| train/                  |             |
|    approx_kl            | 0.015509369 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.03       |
|    explained_variance   | 0.336       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0228      |
|    n_updates            | 1730        |
|    policy_gradient_loss | -0.0061     |
|    value_loss           | 0.00558     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 356000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 174    |
|    time_elapsed    | 36524  |
|    total_timesteps | 356352 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 357000     |
| train/                  |            |
|    approx_kl            | 0.02013427 |
|    clip_fraction        | 0.229      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.81      |
|    explained_variance   | 0.352      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0118    |
|    n_updates            | 1740       |
|    policy_gradient_loss | -0.00584   |
|    value_loss           | 0.0056     |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 358000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 175    |
|    time_elapsed    | 36729  |
|    total_timesteps | 358400 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 359000     |
| train/                  |            |
|    approx_kl            | 0.01802024 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.64      |
|    explained_variance   | 0.373      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00783   |
|    n_updates            | 1750       |
|    policy_gradient_loss | -0.00542   |
|    value_loss           | 0.00552    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 360000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 176    |
|    time_elapsed    | 36934  |
|    total_timesteps | 360448 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 361000      |
| train/                  |             |
|    approx_kl            | 0.014706782 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.54       |
|    explained_variance   | 0.343       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0129     |
|    n_updates            | 1760        |
|    policy_gradient_loss | -0.00582    |
|    value_loss           | 0.00565     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 362000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 177    |
|    time_elapsed    | 37140  |
|    total_timesteps | 362496 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 363000      |
| train/                  |             |
|    approx_kl            | 0.016577473 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.34       |
|    explained_variance   | 0.316       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0358     |
|    n_updates            | 1770        |
|    policy_gradient_loss | -0.00702    |
|    value_loss           | 0.00575     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 364000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 178    |
|    time_elapsed    | 37345  |
|    total_timesteps | 364544 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 365000      |
| train/                  |             |
|    approx_kl            | 0.010709839 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.18       |
|    explained_variance   | 0.329       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00866     |
|    n_updates            | 1780        |
|    policy_gradient_loss | -0.00741    |
|    value_loss           | 0.00566     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 366000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 179    |
|    time_elapsed    | 37550  |
|    total_timesteps | 366592 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 367000      |
| train/                  |             |
|    approx_kl            | 0.014591568 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.02       |
|    explained_variance   | 0.343       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00286    |
|    n_updates            | 1790        |
|    policy_gradient_loss | -0.00262    |
|    value_loss           | 0.00562     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 368000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 180    |
|    time_elapsed    | 37756  |
|    total_timesteps | 368640 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 369000      |
| train/                  |             |
|    approx_kl            | 0.013210883 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.87       |
|    explained_variance   | 0.329       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00639    |
|    n_updates            | 1800        |
|    policy_gradient_loss | -0.00267    |
|    value_loss           | 0.00578     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 370000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 181    |
|    time_elapsed    | 37961  |
|    total_timesteps | 370688 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 371000      |
| train/                  |             |
|    approx_kl            | 0.017790211 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.8        |
|    explained_variance   | 0.348       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0127     |
|    n_updates            | 1810        |
|    policy_gradient_loss | -0.00496    |
|    value_loss           | 0.00576     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 372000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 182    |
|    time_elapsed    | 38166  |
|    total_timesteps | 372736 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 373000      |
| train/                  |             |
|    approx_kl            | 0.007269521 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.66       |
|    explained_variance   | 0.358       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00956    |
|    n_updates            | 1820        |
|    policy_gradient_loss | -0.00247    |
|    value_loss           | 0.00572     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 374000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 183    |
|    time_elapsed    | 38372  |
|    total_timesteps | 374784 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 375000      |
| train/                  |             |
|    approx_kl            | 0.009739239 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.62       |
|    explained_variance   | 0.302       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0245     |
|    n_updates            | 1830        |
|    policy_gradient_loss | -0.00365    |
|    value_loss           | 0.00578     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 376000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 184    |
|    time_elapsed    | 38577  |
|    total_timesteps | 376832 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 377000     |
| train/                  |            |
|    approx_kl            | 0.01672493 |
|    clip_fraction        | 0.192      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.45      |
|    explained_variance   | 0.321      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0192    |
|    n_updates            | 1840       |
|    policy_gradient_loss | -0.00651   |
|    value_loss           | 0.00578    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 378000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 185    |
|    time_elapsed    | 38782  |
|    total_timesteps | 378880 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 379000      |
| train/                  |             |
|    approx_kl            | 0.010522796 |
|    clip_fraction        | 0.0835      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.4        |
|    explained_variance   | 0.306       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000499   |
|    n_updates            | 1850        |
|    policy_gradient_loss | -0.00255    |
|    value_loss           | 0.00585     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 380000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 186    |
|    time_elapsed    | 38988  |
|    total_timesteps | 380928 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 381000      |
| train/                  |             |
|    approx_kl            | 0.020423828 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.32       |
|    explained_variance   | 0.339       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0274     |
|    n_updates            | 1860        |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 0.00583     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 382000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 187    |
|    time_elapsed    | 39193  |
|    total_timesteps | 382976 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 383000      |
| train/                  |             |
|    approx_kl            | 0.014203954 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.25       |
|    explained_variance   | 0.316       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00778    |
|    n_updates            | 1870        |
|    policy_gradient_loss | -0.00339    |
|    value_loss           | 0.00578     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 384000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 385000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 188    |
|    time_elapsed    | 39498  |
|    total_timesteps | 385024 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 386000      |
| train/                  |             |
|    approx_kl            | 0.021969782 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.16       |
|    explained_variance   | 0.328       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0101      |
|    n_updates            | 1880        |
|    policy_gradient_loss | -0.00691    |
|    value_loss           | 0.00848     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 387000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 189    |
|    time_elapsed    | 39704  |
|    total_timesteps | 387072 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 388000     |
| train/                  |            |
|    approx_kl            | 0.01767381 |
|    clip_fraction        | 0.169      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.01      |
|    explained_variance   | 0.306      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00822   |
|    n_updates            | 1890       |
|    policy_gradient_loss | -0.00454   |
|    value_loss           | 0.00591    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 389000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 190    |
|    time_elapsed    | 39909  |
|    total_timesteps | 389120 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 390000      |
| train/                  |             |
|    approx_kl            | 0.016780935 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.89       |
|    explained_variance   | 0.366       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00589    |
|    n_updates            | 1900        |
|    policy_gradient_loss | -0.00362    |
|    value_loss           | 0.00576     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 391000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 191    |
|    time_elapsed    | 40114  |
|    total_timesteps | 391168 |
-------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+04      |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 392000     |
| train/                  |            |
|    approx_kl            | 0.01774533 |
|    clip_fraction        | 0.161      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.73      |
|    explained_variance   | 0.322      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0344     |
|    n_updates            | 1910       |
|    policy_gradient_loss | -0.00337   |
|    value_loss           | 0.00585    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 393000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 192    |
|    time_elapsed    | 40320  |
|    total_timesteps | 393216 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 394000      |
| train/                  |             |
|    approx_kl            | 0.006967846 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.65       |
|    explained_variance   | 0.303       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0197      |
|    n_updates            | 1920        |
|    policy_gradient_loss | -0.0043     |
|    value_loss           | 0.00581     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 395000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 193    |
|    time_elapsed    | 40525  |
|    total_timesteps | 395264 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 396000      |
| train/                  |             |
|    approx_kl            | 0.011938073 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.62       |
|    explained_variance   | 0.304       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00937    |
|    n_updates            | 1930        |
|    policy_gradient_loss | -0.00878    |
|    value_loss           | 0.00584     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 397000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 194    |
|    time_elapsed    | 40730  |
|    total_timesteps | 397312 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 398000      |
| train/                  |             |
|    approx_kl            | 0.017506346 |
|    clip_fraction        | 0.0603      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.55       |
|    explained_variance   | 0.311       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0285      |
|    n_updates            | 1940        |
|    policy_gradient_loss | -0.00165    |
|    value_loss           | 0.00586     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 399000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 195    |
|    time_elapsed    | 40935  |
|    total_timesteps | 399360 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 400000      |
| train/                  |             |
|    approx_kl            | 0.012336062 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.5        |
|    explained_variance   | 0.349       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0189     |
|    n_updates            | 1950        |
|    policy_gradient_loss | -0.00315    |
|    value_loss           | 0.0058      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 401000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 196    |
|    time_elapsed    | 41141  |
|    total_timesteps | 401408 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 402000      |
| train/                  |             |
|    approx_kl            | 0.005523025 |
|    clip_fraction        | 0.0602      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.48       |
|    explained_variance   | 0.309       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00236    |
|    n_updates            | 1960        |
|    policy_gradient_loss | -0.00293    |
|    value_loss           | 0.00577     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 403000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 197    |
|    time_elapsed    | 41346  |
|    total_timesteps | 403456 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 404000      |
| train/                  |             |
|    approx_kl            | 0.008506099 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.311       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0323     |
|    n_updates            | 1970        |
|    policy_gradient_loss | -0.0041     |
|    value_loss           | 0.00579     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 405000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 198    |
|    time_elapsed    | 41551  |
|    total_timesteps | 405504 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 406000      |
| train/                  |             |
|    approx_kl            | 0.007460681 |
|    clip_fraction        | 0.0545      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.311       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0256      |
|    n_updates            | 1980        |
|    policy_gradient_loss | -0.000495   |
|    value_loss           | 0.00591     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 407000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 199    |
|    time_elapsed    | 41757  |
|    total_timesteps | 407552 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 408000      |
| train/                  |             |
|    approx_kl            | 0.011404341 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.311       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0251     |
|    n_updates            | 1990        |
|    policy_gradient_loss | -0.00599    |
|    value_loss           | 0.00589     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 409000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 200    |
|    time_elapsed    | 41962  |
|    total_timesteps | 409600 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 410000      |
| train/                  |             |
|    approx_kl            | 0.005681058 |
|    clip_fraction        | 0.0997      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.293       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00751     |
|    n_updates            | 2000        |
|    policy_gradient_loss | -0.00442    |
|    value_loss           | 0.00596     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 411000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 201    |
|    time_elapsed    | 42167  |
|    total_timesteps | 411648 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 412000      |
| train/                  |             |
|    approx_kl            | 0.014046371 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.298       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0268     |
|    n_updates            | 2010        |
|    policy_gradient_loss | -0.00653    |
|    value_loss           | 0.00597     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 413000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 202    |
|    time_elapsed    | 42373  |
|    total_timesteps | 413696 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 414000      |
| train/                  |             |
|    approx_kl            | 0.017962184 |
|    clip_fraction        | 0.0974      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.293       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0336     |
|    n_updates            | 2020        |
|    policy_gradient_loss | -0.000922   |
|    value_loss           | 0.00596     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 415000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 203    |
|    time_elapsed    | 42578  |
|    total_timesteps | 415744 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 0.9          |
| time/                   |              |
|    total_timesteps      | 416000       |
| train/                  |              |
|    approx_kl            | 0.0070302207 |
|    clip_fraction        | 0.0301       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.935       |
|    explained_variance   | 0.295        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00385      |
|    n_updates            | 2030         |
|    policy_gradient_loss | 0.00113      |
|    value_loss           | 0.00596      |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 417000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 204    |
|    time_elapsed    | 42783  |
|    total_timesteps | 417792 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 0.9          |
| time/                   |              |
|    total_timesteps      | 418000       |
| train/                  |              |
|    approx_kl            | 0.0036273832 |
|    clip_fraction        | 0.0775       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.855       |
|    explained_variance   | 0.319        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00505     |
|    n_updates            | 2040         |
|    policy_gradient_loss | -7.03e-05    |
|    value_loss           | 0.00588      |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 419000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 205    |
|    time_elapsed    | 42989  |
|    total_timesteps | 419840 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 0.9          |
| time/                   |              |
|    total_timesteps      | 420000       |
| train/                  |              |
|    approx_kl            | 0.0048064957 |
|    clip_fraction        | 0.0539       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.781       |
|    explained_variance   | 0.303        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0203       |
|    n_updates            | 2050         |
|    policy_gradient_loss | -0.000302    |
|    value_loss           | 0.0059       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 421000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 206    |
|    time_elapsed    | 43194  |
|    total_timesteps | 421888 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 422000      |
| train/                  |             |
|    approx_kl            | 0.007330614 |
|    clip_fraction        | 0.0813      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.705      |
|    explained_variance   | 0.295       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0079     |
|    n_updates            | 2060        |
|    policy_gradient_loss | -7.27e-05   |
|    value_loss           | 0.00589     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 423000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 207    |
|    time_elapsed    | 43399  |
|    total_timesteps | 423936 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 424000      |
| train/                  |             |
|    approx_kl            | 0.006662857 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.59       |
|    explained_variance   | 0.297       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00731    |
|    n_updates            | 2070        |
|    policy_gradient_loss | -0.00556    |
|    value_loss           | 0.00593     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 425000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 208    |
|    time_elapsed    | 43605  |
|    total_timesteps | 425984 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 426000      |
| train/                  |             |
|    approx_kl            | 0.001954935 |
|    clip_fraction        | 0.0611      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.518      |
|    explained_variance   | 0.296       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00609    |
|    n_updates            | 2080        |
|    policy_gradient_loss | -0.000279   |
|    value_loss           | 0.00591     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 427000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 428000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 209    |
|    time_elapsed    | 43910  |
|    total_timesteps | 428032 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 0.9          |
| time/                   |              |
|    total_timesteps      | 429000       |
| train/                  |              |
|    approx_kl            | 0.0031386304 |
|    clip_fraction        | 0.0634       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.461       |
|    explained_variance   | 0.302        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0033       |
|    n_updates            | 2090         |
|    policy_gradient_loss | -0.00148     |
|    value_loss           | 0.00846      |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 430000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 210    |
|    time_elapsed    | 44115  |
|    total_timesteps | 430080 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 431000      |
| train/                  |             |
|    approx_kl            | 0.003974481 |
|    clip_fraction        | 0.0555      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.418      |
|    explained_variance   | 0.341       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00501     |
|    n_updates            | 2100        |
|    policy_gradient_loss | -0.00111    |
|    value_loss           | 0.00579     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 432000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 211    |
|    time_elapsed    | 44321  |
|    total_timesteps | 432128 |
-------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+04     |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 433000    |
| train/                  |           |
|    approx_kl            | 0.0026105 |
|    clip_fraction        | 0.0223    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.386    |
|    explained_variance   | 0.295     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0152    |
|    n_updates            | 2110      |
|    policy_gradient_loss | -0.000194 |
|    value_loss           | 0.00593   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 434000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 212    |
|    time_elapsed    | 44526  |
|    total_timesteps | 434176 |
-------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+04       |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 435000      |
| train/                  |             |
|    approx_kl            | 0.002257069 |
|    clip_fraction        | 0.0462      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.324      |
|    explained_variance   | 0.293       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00238    |
|    n_updates            | 2120        |
|    policy_gradient_loss | -0.00156    |
|    value_loss           | 0.00596     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 436000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 213    |
|    time_elapsed    | 44731  |
|    total_timesteps | 436224 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 0.9          |
| time/                   |              |
|    total_timesteps      | 437000       |
| train/                  |              |
|    approx_kl            | 0.0020972842 |
|    clip_fraction        | 0.0403       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.287       |
|    explained_variance   | 0.302        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00235      |
|    n_updates            | 2130         |
|    policy_gradient_loss | -0.00143     |
|    value_loss           | 0.00591      |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 438000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 214    |
|    time_elapsed    | 44937  |
|    total_timesteps | 438272 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 0.9          |
| time/                   |              |
|    total_timesteps      | 439000       |
| train/                  |              |
|    approx_kl            | 0.0014591832 |
|    clip_fraction        | 0.0147       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.252       |
|    explained_variance   | 0.294        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000311     |
|    n_updates            | 2140         |
|    policy_gradient_loss | -0.000444    |
|    value_loss           | 0.00593      |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 440000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 215    |
|    time_elapsed    | 45142  |
|    total_timesteps | 440320 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 0.9          |
| time/                   |              |
|    total_timesteps      | 441000       |
| train/                  |              |
|    approx_kl            | 0.0016301546 |
|    clip_fraction        | 0.0124       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.225       |
|    explained_variance   | 0.293        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0078       |
|    n_updates            | 2150         |
|    policy_gradient_loss | -0.000738    |
|    value_loss           | 0.00599      |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 442000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 216    |
|    time_elapsed    | 45347  |
|    total_timesteps | 442368 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 0.9          |
| time/                   |              |
|    total_timesteps      | 443000       |
| train/                  |              |
|    approx_kl            | 0.0015680469 |
|    clip_fraction        | 0.0135       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.205       |
|    explained_variance   | 0.294        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000457    |
|    n_updates            | 2160         |
|    policy_gradient_loss | 8.46e-05     |
|    value_loss           | 0.00594      |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 444000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 217    |
|    time_elapsed    | 45553  |
|    total_timesteps | 444416 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 0.9          |
| time/                   |              |
|    total_timesteps      | 445000       |
| train/                  |              |
|    approx_kl            | 0.0014409267 |
|    clip_fraction        | 0.0359       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.172       |
|    explained_variance   | 0.295        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.017        |
|    n_updates            | 2170         |
|    policy_gradient_loss | -0.00131     |
|    value_loss           | 0.00593      |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 446000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 218    |
|    time_elapsed    | 45758  |
|    total_timesteps | 446464 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 0.9           |
| time/                   |               |
|    total_timesteps      | 447000        |
| train/                  |               |
|    approx_kl            | 0.00039552856 |
|    clip_fraction        | 0.00371       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.164        |
|    explained_variance   | 0.293         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00166       |
|    n_updates            | 2180          |
|    policy_gradient_loss | -0.000329     |
|    value_loss           | 0.00596       |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 448000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 219    |
|    time_elapsed    | 45963  |
|    total_timesteps | 448512 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 0.9          |
| time/                   |              |
|    total_timesteps      | 449000       |
| train/                  |              |
|    approx_kl            | 0.0005543496 |
|    clip_fraction        | 0.0123       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.165       |
|    explained_variance   | 0.295        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00977      |
|    n_updates            | 2190         |
|    policy_gradient_loss | -0.000712    |
|    value_loss           | 0.00596      |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 450000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 220    |
|    time_elapsed    | 46168  |
|    total_timesteps | 450560 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 0.9           |
| time/                   |               |
|    total_timesteps      | 451000        |
| train/                  |               |
|    approx_kl            | 0.00084801775 |
|    clip_fraction        | 0.0204        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.128        |
|    explained_variance   | 0.293         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0215        |
|    n_updates            | 2200          |
|    policy_gradient_loss | -0.000692     |
|    value_loss           | 0.00593       |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 452000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 221    |
|    time_elapsed    | 46374  |
|    total_timesteps | 452608 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 0.9           |
| time/                   |               |
|    total_timesteps      | 453000        |
| train/                  |               |
|    approx_kl            | 0.00085081847 |
|    clip_fraction        | 0.0135        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.109        |
|    explained_variance   | 0.293         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00255       |
|    n_updates            | 2210          |
|    policy_gradient_loss | -0.000521     |
|    value_loss           | 0.00595       |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 454000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 222    |
|    time_elapsed    | 46579  |
|    total_timesteps | 454656 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 0.9           |
| time/                   |               |
|    total_timesteps      | 455000        |
| train/                  |               |
|    approx_kl            | 0.00046466844 |
|    clip_fraction        | 0.00674       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0909       |
|    explained_variance   | 0.293         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00496       |
|    n_updates            | 2220          |
|    policy_gradient_loss | -0.000138     |
|    value_loss           | 0.00596       |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 456000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 223    |
|    time_elapsed    | 46785  |
|    total_timesteps | 456704 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 0.9           |
| time/                   |               |
|    total_timesteps      | 457000        |
| train/                  |               |
|    approx_kl            | 0.00042127894 |
|    clip_fraction        | 0.0113        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0713       |
|    explained_variance   | 0.293         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000246      |
|    n_updates            | 2230          |
|    policy_gradient_loss | -0.000463     |
|    value_loss           | 0.00596       |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 458000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 224    |
|    time_elapsed    | 46990  |
|    total_timesteps | 458752 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 0.9          |
| time/                   |              |
|    total_timesteps      | 459000       |
| train/                  |              |
|    approx_kl            | 0.0005219951 |
|    clip_fraction        | 0.00991      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0607      |
|    explained_variance   | 0.307        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000505     |
|    n_updates            | 2240         |
|    policy_gradient_loss | -0.000352    |
|    value_loss           | 0.0059       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 460000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 225    |
|    time_elapsed    | 47195  |
|    total_timesteps | 460800 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 0.9           |
| time/                   |               |
|    total_timesteps      | 461000        |
| train/                  |               |
|    approx_kl            | 0.00031055335 |
|    clip_fraction        | 0.00259       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0545       |
|    explained_variance   | 0.294         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00021      |
|    n_updates            | 2250          |
|    policy_gradient_loss | 8.45e-05      |
|    value_loss           | 0.00593       |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 462000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 226    |
|    time_elapsed    | 47401  |
|    total_timesteps | 462848 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 0.9           |
| time/                   |               |
|    total_timesteps      | 463000        |
| train/                  |               |
|    approx_kl            | 0.00037441793 |
|    clip_fraction        | 0.00352       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0567       |
|    explained_variance   | 0.293         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000193      |
|    n_updates            | 2260          |
|    policy_gradient_loss | -3.65e-06     |
|    value_loss           | 0.00593       |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 464000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 227    |
|    time_elapsed    | 47606  |
|    total_timesteps | 464896 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 0.9           |
| time/                   |               |
|    total_timesteps      | 465000        |
| train/                  |               |
|    approx_kl            | 0.00023866285 |
|    clip_fraction        | 0.00376       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0665       |
|    explained_variance   | 0.293         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000809      |
|    n_updates            | 2270          |
|    policy_gradient_loss | -0.000936     |
|    value_loss           | 0.00594       |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 466000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 228    |
|    time_elapsed    | 47811  |
|    total_timesteps | 466944 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 0.9           |
| time/                   |               |
|    total_timesteps      | 467000        |
| train/                  |               |
|    approx_kl            | 0.00022696186 |
|    clip_fraction        | 0.00835       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0548       |
|    explained_variance   | 0.294         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000434     |
|    n_updates            | 2280          |
|    policy_gradient_loss | -0.00031      |
|    value_loss           | 0.00595       |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 468000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 229    |
|    time_elapsed    | 48016  |
|    total_timesteps | 468992 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 0.9           |
| time/                   |               |
|    total_timesteps      | 469000        |
| train/                  |               |
|    approx_kl            | 0.00032325668 |
|    clip_fraction        | 0.00356       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0621       |
|    explained_variance   | 0.295         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00607       |
|    n_updates            | 2290          |
|    policy_gradient_loss | -7.59e-05     |
|    value_loss           | 0.00595       |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 470000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 471000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 230    |
|    time_elapsed    | 48322  |
|    total_timesteps | 471040 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 0.9           |
| time/                   |               |
|    total_timesteps      | 472000        |
| train/                  |               |
|    approx_kl            | 0.00029503752 |
|    clip_fraction        | 0.0062        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0529       |
|    explained_variance   | 0.317         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00043       |
|    n_updates            | 2300          |
|    policy_gradient_loss | -0.00017      |
|    value_loss           | 0.00789       |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 473000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 231    |
|    time_elapsed    | 48527  |
|    total_timesteps | 473088 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 0.9           |
| time/                   |               |
|    total_timesteps      | 474000        |
| train/                  |               |
|    approx_kl            | 0.00035810177 |
|    clip_fraction        | 0.00581       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0424       |
|    explained_variance   | 0.294         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00942       |
|    n_updates            | 2310          |
|    policy_gradient_loss | -0.000263     |
|    value_loss           | 0.00595       |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 475000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 232    |
|    time_elapsed    | 48732  |
|    total_timesteps | 475136 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 0.9           |
| time/                   |               |
|    total_timesteps      | 476000        |
| train/                  |               |
|    approx_kl            | 0.00019964439 |
|    clip_fraction        | 0.00508       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0349       |
|    explained_variance   | 0.294         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000414     |
|    n_updates            | 2320          |
|    policy_gradient_loss | -0.000299     |
|    value_loss           | 0.00596       |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 477000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 233    |
|    time_elapsed    | 48938  |
|    total_timesteps | 477184 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 0.9           |
| time/                   |               |
|    total_timesteps      | 478000        |
| train/                  |               |
|    approx_kl            | 0.00010264013 |
|    clip_fraction        | 0.00137       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0308       |
|    explained_variance   | 0.295         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00872       |
|    n_updates            | 2330          |
|    policy_gradient_loss | -6.62e-05     |
|    value_loss           | 0.00599       |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 479000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 234    |
|    time_elapsed    | 49143  |
|    total_timesteps | 479232 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 0.9           |
| time/                   |               |
|    total_timesteps      | 480000        |
| train/                  |               |
|    approx_kl            | 6.5488304e-05 |
|    clip_fraction        | 0.0019        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0243       |
|    explained_variance   | 0.293         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00188       |
|    n_updates            | 2340          |
|    policy_gradient_loss | -0.000129     |
|    value_loss           | 0.00597       |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 481000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 235    |
|    time_elapsed    | 49348  |
|    total_timesteps | 481280 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 0.9          |
| time/                   |              |
|    total_timesteps      | 482000       |
| train/                  |              |
|    approx_kl            | 7.056212e-05 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0205      |
|    explained_variance   | 0.293        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000123     |
|    n_updates            | 2350         |
|    policy_gradient_loss | -7.77e-05    |
|    value_loss           | 0.00595      |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 483000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 236    |
|    time_elapsed    | 49554  |
|    total_timesteps | 483328 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 0.9           |
| time/                   |               |
|    total_timesteps      | 484000        |
| train/                  |               |
|    approx_kl            | 0.00010609283 |
|    clip_fraction        | 0.00112       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0188       |
|    explained_variance   | 0.295         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00745       |
|    n_updates            | 2360          |
|    policy_gradient_loss | -7.58e-05     |
|    value_loss           | 0.00593       |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 485000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 237    |
|    time_elapsed    | 49759  |
|    total_timesteps | 485376 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 0.9           |
| time/                   |               |
|    total_timesteps      | 486000        |
| train/                  |               |
|    approx_kl            | 0.00026366612 |
|    clip_fraction        | 0.00254       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0143       |
|    explained_variance   | 0.292         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.01          |
|    n_updates            | 2370          |
|    policy_gradient_loss | -0.000129     |
|    value_loss           | 0.00589       |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 487000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 238    |
|    time_elapsed    | 49964  |
|    total_timesteps | 487424 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 0.9           |
| time/                   |               |
|    total_timesteps      | 488000        |
| train/                  |               |
|    approx_kl            | 0.00029284504 |
|    clip_fraction        | 0.00151       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0125       |
|    explained_variance   | 0.293         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000928      |
|    n_updates            | 2380          |
|    policy_gradient_loss | -3.55e-05     |
|    value_loss           | 0.00594       |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 489000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 239    |
|    time_elapsed    | 50169  |
|    total_timesteps | 489472 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 0.9           |
| time/                   |               |
|    total_timesteps      | 490000        |
| train/                  |               |
|    approx_kl            | 1.8614926e-05 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0107       |
|    explained_variance   | 0.293         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00727       |
|    n_updates            | 2390          |
|    policy_gradient_loss | -1.79e-05     |
|    value_loss           | 0.00596       |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 491000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 240    |
|    time_elapsed    | 50375  |
|    total_timesteps | 491520 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 0.9           |
| time/                   |               |
|    total_timesteps      | 492000        |
| train/                  |               |
|    approx_kl            | 1.4499237e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0108       |
|    explained_variance   | 0.293         |
|    learning_rate        | 0.0003        |
|    loss                 | 8.5e-05       |
|    n_updates            | 2400          |
|    policy_gradient_loss | -2.04e-05     |
|    value_loss           | 0.00598       |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 493000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 241    |
|    time_elapsed    | 50580  |
|    total_timesteps | 493568 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 0.9           |
| time/                   |               |
|    total_timesteps      | 494000        |
| train/                  |               |
|    approx_kl            | 5.2532414e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0115       |
|    explained_variance   | 0.293         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0017        |
|    n_updates            | 2410          |
|    policy_gradient_loss | -1.11e-06     |
|    value_loss           | 0.00598       |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 495000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 242    |
|    time_elapsed    | 50785  |
|    total_timesteps | 495616 |
-------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+04         |
|    mean_reward          | 0.9           |
| time/                   |               |
|    total_timesteps      | 496000        |
| train/                  |               |
|    approx_kl            | 0.00054135243 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0224       |
|    explained_variance   | 0.294         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00645       |
|    n_updates            | 2420          |
|    policy_gradient_loss | -0.000214     |
|    value_loss           | 0.00593       |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 497000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 243    |
|    time_elapsed    | 50991  |
|    total_timesteps | 497664 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 0.9          |
| time/                   |              |
|    total_timesteps      | 498000       |
| train/                  |              |
|    approx_kl            | 3.700165e-05 |
|    clip_fraction        | 0.000732     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0245      |
|    explained_variance   | 0.293        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00152      |
|    n_updates            | 2430         |
|    policy_gradient_loss | 5.81e-05     |
|    value_loss           | 0.00595      |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 499000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 244    |
|    time_elapsed    | 51196  |
|    total_timesteps | 499712 |
-------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+04        |
|    mean_reward          | 0.9          |
| time/                   |              |
|    total_timesteps      | 500000       |
| train/                  |              |
|    approx_kl            | 4.653778e-05 |
|    clip_fraction        | 0.00127      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0176      |
|    explained_variance   | 0.293        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000186     |
|    n_updates            | 2440         |
|    policy_gradient_loss | -6.08e-05    |
|    value_loss           | 0.00596      |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+04    |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 501000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 9      |
|    iterations      | 245    |
|    time_elapsed    | 51401  |
|    total_timesteps | 501760 |
-------------------------------
