Logging to ../Logging/RPPO_BW_Then_FW_10_200
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 48.1                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 1                    |
|    time_elapsed    | -1683717609109071360 |
|    total_timesteps | 2048                 |
---------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 3000        |
| train/                  |             |
|    approx_kl            | 0.025164884 |
|    clip_fraction        | 0.286       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.53       |
|    explained_variance   | -0.548      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.068      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.062      |
|    value_loss           | 0.0315      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 48.5                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 2                    |
|    time_elapsed    | -1683717609109071104 |
|    total_timesteps | 4096                 |
---------------------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.09272584 |
|    clip_fraction        | 0.574      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.5       |
|    explained_variance   | 0.486      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0641    |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0425    |
|    value_loss           | 0.00787    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 50                   |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 3                    |
|    time_elapsed    | -1683717609109071104 |
|    total_timesteps | 6144                 |
---------------------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 7000       |
| train/                  |            |
|    approx_kl            | 0.16977215 |
|    clip_fraction        | 0.669      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.46      |
|    explained_variance   | 0.617      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0817    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0254    |
|    value_loss           | 0.00357    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 51.9                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 4                    |
|    time_elapsed    | -1683717609109070848 |
|    total_timesteps | 8192                 |
---------------------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 9000       |
| train/                  |            |
|    approx_kl            | 0.30839497 |
|    clip_fraction        | 0.753      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.44      |
|    explained_variance   | 0.611      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.067     |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0263    |
|    value_loss           | 0.0023     |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 57.4                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 5                    |
|    time_elapsed    | -1683717609109070848 |
|    total_timesteps | 10240                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+03     |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 11000     |
| train/                  |           |
|    approx_kl            | 0.4420262 |
|    clip_fraction        | 0.799     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.26     |
|    explained_variance   | 0.513     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0409   |
|    n_updates            | 100       |
|    policy_gradient_loss | -0.0153   |
|    value_loss           | 0.00383   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 57                   |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 6                    |
|    time_elapsed    | -1683717609109070592 |
|    total_timesteps | 12288                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+03     |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 13000     |
| train/                  |           |
|    approx_kl            | 0.7428458 |
|    clip_fraction        | 0.838     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.19     |
|    explained_variance   | 0.536     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.000656  |
|    n_updates            | 120       |
|    policy_gradient_loss | 0.00968   |
|    value_loss           | 0.00392   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 52                   |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 7                    |
|    time_elapsed    | -1683717609109070592 |
|    total_timesteps | 14336                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+03     |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 15000     |
| train/                  |           |
|    approx_kl            | 0.7066135 |
|    clip_fraction        | 0.867     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.23     |
|    explained_variance   | 0.494     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0169   |
|    n_updates            | 140       |
|    policy_gradient_loss | 0.0227    |
|    value_loss           | 0.00554   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 44.4                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 8                    |
|    time_elapsed    | -1683717609109070592 |
|    total_timesteps | 16384                |
---------------------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.8        |
| time/                   |            |
|    total_timesteps      | 17000      |
| train/                  |            |
|    approx_kl            | 0.79904246 |
|    clip_fraction        | 0.875      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.2       |
|    explained_variance   | 0.471      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00307   |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.00443   |
|    value_loss           | 0.00479    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 34.9                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 9                    |
|    time_elapsed    | -1683717609109070336 |
|    total_timesteps | 18432                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+03     |
|    mean_reward          | 0.8       |
| time/                   |           |
|    total_timesteps      | 19000     |
| train/                  |           |
|    approx_kl            | 1.0450271 |
|    clip_fraction        | 0.875     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.27     |
|    explained_variance   | 0.51      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0192   |
|    n_updates            | 180       |
|    policy_gradient_loss | -0.0105   |
|    value_loss           | 0.00287   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 27.8                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 10                   |
|    time_elapsed    | -1683717609109070336 |
|    total_timesteps | 20480                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+03     |
|    mean_reward          | 0.8       |
| time/                   |           |
|    total_timesteps      | 21000     |
| train/                  |           |
|    approx_kl            | 1.5424609 |
|    clip_fraction        | 0.874     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.23     |
|    explained_variance   | 0.507     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0399   |
|    n_updates            | 200       |
|    policy_gradient_loss | -0.03     |
|    value_loss           | 0.00193   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 26.1                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 11                   |
|    time_elapsed    | -1683717609109070080 |
|    total_timesteps | 22528                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+03     |
|    mean_reward          | 0.8       |
| time/                   |           |
|    total_timesteps      | 23000     |
| train/                  |           |
|    approx_kl            | 1.6253201 |
|    clip_fraction        | 0.892     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.17     |
|    explained_variance   | 0.343     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0614   |
|    n_updates            | 220       |
|    policy_gradient_loss | -0.0378   |
|    value_loss           | 0.00511   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 24.7                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 12                   |
|    time_elapsed    | -1683717609109070080 |
|    total_timesteps | 24576                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 25000     |
| train/                  |           |
|    approx_kl            | 2.1857834 |
|    clip_fraction        | 0.907     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.12     |
|    explained_variance   | 0.316     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0674   |
|    n_updates            | 240       |
|    policy_gradient_loss | -0.0226   |
|    value_loss           | 0.00424   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 20.2                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 13                   |
|    time_elapsed    | -1683717609109070080 |
|    total_timesteps | 26624                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 10       |
|    mean_reward          | 0.9      |
| time/                   |          |
|    total_timesteps      | 27000    |
| train/                  |          |
|    approx_kl            | 3.704001 |
|    clip_fraction        | 0.939    |
|    clip_range           | 0.2      |
|    entropy_loss         | -5.92    |
|    explained_variance   | 0.306    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.101   |
|    n_updates            | 260      |
|    policy_gradient_loss | -0.059   |
|    value_loss           | 0.00165  |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 21                   |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 14                   |
|    time_elapsed    | -1683717609109070080 |
|    total_timesteps | 28672                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 10       |
|    mean_reward          | 0.9      |
| time/                   |          |
|    total_timesteps      | 29000    |
| train/                  |          |
|    approx_kl            | 4.845934 |
|    clip_fraction        | 0.945    |
|    clip_range           | 0.2      |
|    entropy_loss         | -5.7     |
|    explained_variance   | 0.252    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.061   |
|    n_updates            | 280      |
|    policy_gradient_loss | -0.0371  |
|    value_loss           | 0.00197  |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 16.7                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 15                   |
|    time_elapsed    | -1683717609109069824 |
|    total_timesteps | 30720                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 10       |
|    mean_reward          | 0.9      |
| time/                   |          |
|    total_timesteps      | 31000    |
| train/                  |          |
|    approx_kl            | 9.636192 |
|    clip_fraction        | 0.951    |
|    clip_range           | 0.2      |
|    entropy_loss         | -5.55    |
|    explained_variance   | 0.263    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.0364   |
|    n_updates            | 300      |
|    policy_gradient_loss | -0.0334  |
|    value_loss           | 0.00517  |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 14.9                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 16                   |
|    time_elapsed    | -1683717609109069824 |
|    total_timesteps | 32768                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 33000     |
| train/                  |           |
|    approx_kl            | 17.366371 |
|    clip_fraction        | 0.938     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.28     |
|    explained_variance   | 0.114     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0813   |
|    n_updates            | 320       |
|    policy_gradient_loss | -0.0529   |
|    value_loss           | 0.00214   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 14.9                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 17                   |
|    time_elapsed    | -1683717609109069824 |
|    total_timesteps | 34816                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 10       |
|    mean_reward          | 0.9      |
| time/                   |          |
|    total_timesteps      | 35000    |
| train/                  |          |
|    approx_kl            | 20.57806 |
|    clip_fraction        | 0.955    |
|    clip_range           | 0.2      |
|    entropy_loss         | -4.93    |
|    explained_variance   | 0.117    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0911  |
|    n_updates            | 340      |
|    policy_gradient_loss | -0.0565  |
|    value_loss           | 0.00193  |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 13.6                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 18                   |
|    time_elapsed    | -1683717609109069824 |
|    total_timesteps | 36864                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 37000     |
| train/                  |           |
|    approx_kl            | 33.105377 |
|    clip_fraction        | 0.963     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.68     |
|    explained_variance   | 0.0769    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0859   |
|    n_updates            | 360       |
|    policy_gradient_loss | -0.045    |
|    value_loss           | 0.00376   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 13.1                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 19                   |
|    time_elapsed    | -1683717609109069824 |
|    total_timesteps | 38912                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 10       |
|    mean_reward          | 0.9      |
| time/                   |          |
|    total_timesteps      | 39000    |
| train/                  |          |
|    approx_kl            | 44.20391 |
|    clip_fraction        | 0.959    |
|    clip_range           | 0.2      |
|    entropy_loss         | -4.44    |
|    explained_variance   | 0.0879   |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0572  |
|    n_updates            | 380      |
|    policy_gradient_loss | 0.127    |
|    value_loss           | 0.00319  |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 13.3                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 20                   |
|    time_elapsed    | -1683717609109069568 |
|    total_timesteps | 40960                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 41000     |
| train/                  |           |
|    approx_kl            | 43.410378 |
|    clip_fraction        | 0.961     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.3      |
|    explained_variance   | 0.00775   |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0643   |
|    n_updates            | 400       |
|    policy_gradient_loss | -0.0423   |
|    value_loss           | 0.00153   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 12.1                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 21                   |
|    time_elapsed    | -1683717609109069568 |
|    total_timesteps | 43008                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 44000     |
| train/                  |           |
|    approx_kl            | 34.085556 |
|    clip_fraction        | 0.96      |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.15     |
|    explained_variance   | -0.00192  |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0955   |
|    n_updates            | 420       |
|    policy_gradient_loss | 0.00177   |
|    value_loss           | 0.000913  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 12.3                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 22                   |
|    time_elapsed    | -1683717609109069568 |
|    total_timesteps | 45056                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 46000     |
| train/                  |           |
|    approx_kl            | 61.870667 |
|    clip_fraction        | 0.95      |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.93     |
|    explained_variance   | -0.0307   |
|    learning_rate        | 0.0003    |
|    loss                 | 0.433     |
|    n_updates            | 440       |
|    policy_gradient_loss | -0.0142   |
|    value_loss           | 0.0033    |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 11.8                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 23                   |
|    time_elapsed    | -1683717609109069568 |
|    total_timesteps | 47104                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 48000     |
| train/                  |           |
|    approx_kl            | 120.60274 |
|    clip_fraction        | 0.968     |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.57     |
|    explained_variance   | -0.0897   |
|    learning_rate        | 0.0003    |
|    loss                 | -0.094    |
|    n_updates            | 460       |
|    policy_gradient_loss | -0.0845   |
|    value_loss           | 0.00127   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 11.5                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 24                   |
|    time_elapsed    | -1683717609109069568 |
|    total_timesteps | 49152                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 10       |
|    mean_reward          | 0.9      |
| time/                   |          |
|    total_timesteps      | 50000    |
| train/                  |          |
|    approx_kl            | 125.7617 |
|    clip_fraction        | 0.961    |
|    clip_range           | 0.2      |
|    entropy_loss         | -3.23    |
|    explained_variance   | -0.0264  |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0338  |
|    n_updates            | 480      |
|    policy_gradient_loss | -0.0628  |
|    value_loss           | 0.00246  |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 12.5                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 25                   |
|    time_elapsed    | -1683717609109069312 |
|    total_timesteps | 51200                |
---------------------------------------------
{'reward': [0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9], 'std': [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]}
{'reward': [0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929], 'std': [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}
