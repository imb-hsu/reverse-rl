Logging to ../Logging/RPPO_BW_Then_FW_5_100
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 15                   |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 1                    |
|    time_elapsed    | -1683715397353957120 |
|    total_timesteps | 2048                 |
---------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 3000        |
| train/                  |             |
|    approx_kl            | 0.028311143 |
|    clip_fraction        | 0.51        |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.12       |
|    explained_variance   | -0.724      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.12       |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.106      |
|    value_loss           | 0.00245     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 14.7                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 2                    |
|    time_elapsed    | -1683715397353957120 |
|    total_timesteps | 4096                 |
---------------------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50         |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.16844249 |
|    clip_fraction        | 0.659      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.08      |
|    explained_variance   | 0.00531    |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0693    |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.03      |
|    value_loss           | 0.000155   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 12.5                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 3                    |
|    time_elapsed    | -1683715397353957120 |
|    total_timesteps | 6144                 |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 7000      |
| train/                  |           |
|    approx_kl            | 0.3749257 |
|    clip_fraction        | 0.783     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.07     |
|    explained_variance   | 0.0365    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0819   |
|    n_updates            | 60        |
|    policy_gradient_loss | -0.0433   |
|    value_loss           | 6.33e-05  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 10.1                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 4                    |
|    time_elapsed    | -1683715397353956864 |
|    total_timesteps | 8192                 |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 9000      |
| train/                  |           |
|    approx_kl            | 0.6580776 |
|    clip_fraction        | 0.859     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.01     |
|    explained_variance   | -0.219    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0753   |
|    n_updates            | 80        |
|    policy_gradient_loss | -0.0529   |
|    value_loss           | 8.75e-05  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 7.87                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 5                    |
|    time_elapsed    | -1683715397353956864 |
|    total_timesteps | 10240                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 11000     |
| train/                  |           |
|    approx_kl            | 1.4474845 |
|    clip_fraction        | 0.908     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.91     |
|    explained_variance   | -0.307    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.122    |
|    n_updates            | 100       |
|    policy_gradient_loss | -0.0513   |
|    value_loss           | 0.000292  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 8.37                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 6                    |
|    time_elapsed    | -1683715397353956864 |
|    total_timesteps | 12288                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 13000     |
| train/                  |           |
|    approx_kl            | 4.0624814 |
|    clip_fraction        | 0.949     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.32     |
|    explained_variance   | -0.37     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0603   |
|    n_updates            | 120       |
|    policy_gradient_loss | -0.077    |
|    value_loss           | 0.000122  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 7.78                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 7                    |
|    time_elapsed    | -1683715397353956608 |
|    total_timesteps | 14336                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 15000     |
| train/                  |           |
|    approx_kl            | 15.163298 |
|    clip_fraction        | 0.967     |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.69     |
|    explained_variance   | -0.319    |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0106    |
|    n_updates            | 140       |
|    policy_gradient_loss | 0.00804   |
|    value_loss           | 0.000681  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 6.52                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 8                    |
|    time_elapsed    | -1683715397353956608 |
|    total_timesteps | 16384                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 17000     |
| train/                  |           |
|    approx_kl            | 43.512985 |
|    clip_fraction        | 0.965     |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.87     |
|    explained_variance   | -0.5      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.115    |
|    n_updates            | 160       |
|    policy_gradient_loss | -0.0955   |
|    value_loss           | 0.000132  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 6.26                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 9                    |
|    time_elapsed    | -1683715397353956608 |
|    total_timesteps | 18432                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 19000     |
| train/                  |           |
|    approx_kl            | 45.723404 |
|    clip_fraction        | 0.93      |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.1      |
|    explained_variance   | -0.584    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0975   |
|    n_updates            | 180       |
|    policy_gradient_loss | -0.0268   |
|    value_loss           | 0.00048   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 6.04                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 10                   |
|    time_elapsed    | -1683715397353956352 |
|    total_timesteps | 20480                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5        |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 21000    |
| train/                  |          |
|    approx_kl            | 35.87983 |
|    clip_fraction        | 0.841    |
|    clip_range           | 0.2      |
|    entropy_loss         | -1.57    |
|    explained_variance   | -0.626   |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0955  |
|    n_updates            | 200      |
|    policy_gradient_loss | -0.00856 |
|    value_loss           | 8.05e-05 |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.25                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 11                   |
|    time_elapsed    | -1683715397353956352 |
|    total_timesteps | 22528                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5        |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 23000    |
| train/                  |          |
|    approx_kl            | 18.10342 |
|    clip_fraction        | 0.703    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.98    |
|    explained_variance   | -0.956   |
|    learning_rate        | 0.0003   |
|    loss                 | -0.111   |
|    n_updates            | 220      |
|    policy_gradient_loss | 0.00199  |
|    value_loss           | 0.00037  |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.23                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 12                   |
|    time_elapsed    | -1683715397353956352 |
|    total_timesteps | 24576                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 25000     |
| train/                  |           |
|    approx_kl            | 24.758919 |
|    clip_fraction        | 0.463     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.441    |
|    explained_variance   | -1        |
|    learning_rate        | 0.0003    |
|    loss                 | -0.1      |
|    n_updates            | 240       |
|    policy_gradient_loss | -0.0896   |
|    value_loss           | 7.98e-05  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.38                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 13                   |
|    time_elapsed    | -1683715397353956096 |
|    total_timesteps | 26624                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 27000     |
| train/                  |           |
|    approx_kl            | 17.074791 |
|    clip_fraction        | 0.462     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.3      |
|    explained_variance   | -0.884    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.115    |
|    n_updates            | 260       |
|    policy_gradient_loss | -0.0879   |
|    value_loss           | 0.000129  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.07                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 14                   |
|    time_elapsed    | -1683715397353956096 |
|    total_timesteps | 28672                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 29000     |
| train/                  |           |
|    approx_kl            | 53.181953 |
|    clip_fraction        | 0.254     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.126    |
|    explained_variance   | -1.03     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.096    |
|    n_updates            | 280       |
|    policy_gradient_loss | -0.085    |
|    value_loss           | 8.07e-05  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.14                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 15                   |
|    time_elapsed    | -1683715397353956096 |
|    total_timesteps | 30720                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 31000     |
| train/                  |           |
|    approx_kl            | 15.506685 |
|    clip_fraction        | 0.234     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0785   |
|    explained_variance   | -1        |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0918   |
|    n_updates            | 300       |
|    policy_gradient_loss | -0.0775   |
|    value_loss           | 8.56e-05  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.07                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 16                   |
|    time_elapsed    | -1683715397353956096 |
|    total_timesteps | 32768                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 33000     |
| train/                  |           |
|    approx_kl            | 7.4374447 |
|    clip_fraction        | 0.216     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0569   |
|    explained_variance   | -1.01     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0816   |
|    n_updates            | 320       |
|    policy_gradient_loss | -0.0782   |
|    value_loss           | 0.000541  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.1                  |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 17                   |
|    time_elapsed    | -1683715397353955840 |
|    total_timesteps | 34816                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 14.754476 |
|    clip_fraction        | 0.217     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0434   |
|    explained_variance   | -0.996    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0753   |
|    n_updates            | 340       |
|    policy_gradient_loss | -0.0787   |
|    value_loss           | 0.000336  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.04                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 18                   |
|    time_elapsed    | -1683715397353955840 |
|    total_timesteps | 36864                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 37000     |
| train/                  |           |
|    approx_kl            | 15.324049 |
|    clip_fraction        | 0.209     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0351   |
|    explained_variance   | -0.981    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0729   |
|    n_updates            | 360       |
|    policy_gradient_loss | -0.077    |
|    value_loss           | 0.000426  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.08                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 19                   |
|    time_elapsed    | -1683715397353955840 |
|    total_timesteps | 38912                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 39000     |
| train/                  |           |
|    approx_kl            | 11.986017 |
|    clip_fraction        | 0.209     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0294   |
|    explained_variance   | -1.12     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0803   |
|    n_updates            | 380       |
|    policy_gradient_loss | -0.0777   |
|    value_loss           | 0.000484  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.05                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 20                   |
|    time_elapsed    | -1683715397353955584 |
|    total_timesteps | 40960                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 41000     |
| train/                  |           |
|    approx_kl            | 25.805052 |
|    clip_fraction        | 0.212     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0258   |
|    explained_variance   | -1.03     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0751   |
|    n_updates            | 400       |
|    policy_gradient_loss | -0.0772   |
|    value_loss           | 0.000106  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.03                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 21                   |
|    time_elapsed    | -1683715397353955584 |
|    total_timesteps | 43008                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 44000     |
| train/                  |           |
|    approx_kl            | 30.410503 |
|    clip_fraction        | 0.209     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0229   |
|    explained_variance   | -1.05     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0844   |
|    n_updates            | 420       |
|    policy_gradient_loss | -0.0787   |
|    value_loss           | 0.000555  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.07                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 22                   |
|    time_elapsed    | -1683715397353955584 |
|    total_timesteps | 45056                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5        |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 46000    |
| train/                  |          |
|    approx_kl            | 30.69709 |
|    clip_fraction        | 0.209    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0209  |
|    explained_variance   | -0.977   |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0723  |
|    n_updates            | 440      |
|    policy_gradient_loss | -0.0765  |
|    value_loss           | 0.000439 |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.07                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 23                   |
|    time_elapsed    | -1683715397353955584 |
|    total_timesteps | 47104                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 48000     |
| train/                  |           |
|    approx_kl            | 17.016882 |
|    clip_fraction        | 0.214     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0218   |
|    explained_variance   | -1.12     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0765   |
|    n_updates            | 460       |
|    policy_gradient_loss | -0.0787   |
|    value_loss           | 0.000258  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.08                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 24                   |
|    time_elapsed    | -1683715397353955328 |
|    total_timesteps | 49152                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 10.848774 |
|    clip_fraction        | 0.219     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0266   |
|    explained_variance   | -1.13     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0784   |
|    n_updates            | 480       |
|    policy_gradient_loss | -0.0783   |
|    value_loss           | 0.00056   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.07                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 25                   |
|    time_elapsed    | -1683715397353955328 |
|    total_timesteps | 51200                |
---------------------------------------------
{'reward': [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4], 'std': [5, 5, 5, 5, 5, 5, 5, 5, 5, 5]}
{'reward': [0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645], 'std': [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]}
