Logging to ../Logging/RPPO_BW_Then_FW_10_300
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 55                   |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 1                    |
|    time_elapsed    | -1683717609766311680 |
|    total_timesteps | 2048                 |
---------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 3000        |
| train/                  |             |
|    approx_kl            | 0.028047904 |
|    clip_fraction        | 0.314       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.53       |
|    explained_variance   | -1.51       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.096      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.07       |
|    value_loss           | 0.0365      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 50.2                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 2                    |
|    time_elapsed    | -1683717609766311424 |
|    total_timesteps | 4096                 |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+03     |
|    mean_reward          | 0         |
| time/                   |           |
|    total_timesteps      | 5000      |
| train/                  |           |
|    approx_kl            | 0.1144921 |
|    clip_fraction        | 0.546     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.46     |
|    explained_variance   | 0.302     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.052    |
|    n_updates            | 40        |
|    policy_gradient_loss | -0.0417   |
|    value_loss           | 0.0094    |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 51.6                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 3                    |
|    time_elapsed    | -1683717609766311424 |
|    total_timesteps | 6144                 |
---------------------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 7000       |
| train/                  |            |
|    approx_kl            | 0.17972936 |
|    clip_fraction        | 0.616      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.46      |
|    explained_variance   | 0.595      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0075    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0051    |
|    value_loss           | 0.0055     |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 46.1                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 4                    |
|    time_elapsed    | -1683717609766311168 |
|    total_timesteps | 8192                 |
---------------------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.3        |
| time/                   |            |
|    total_timesteps      | 9000       |
| train/                  |            |
|    approx_kl            | 0.32908693 |
|    clip_fraction        | 0.744      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.41      |
|    explained_variance   | 0.625      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0238    |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0229    |
|    value_loss           | 0.00382    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 38.1                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 5                    |
|    time_elapsed    | -1683717609766311168 |
|    total_timesteps | 10240                |
---------------------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 11000      |
| train/                  |            |
|    approx_kl            | 0.54069686 |
|    clip_fraction        | 0.814      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.35      |
|    explained_variance   | 0.556      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0526    |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0256    |
|    value_loss           | 0.00497    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 32.2                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 6                    |
|    time_elapsed    | -1683717609766310912 |
|    total_timesteps | 12288                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+03     |
|    mean_reward          | 0         |
| time/                   |           |
|    total_timesteps      | 13000     |
| train/                  |           |
|    approx_kl            | 0.7984247 |
|    clip_fraction        | 0.866     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.29     |
|    explained_variance   | 0.526     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0206   |
|    n_updates            | 120       |
|    policy_gradient_loss | -0.0343   |
|    value_loss           | 0.00107   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 29.6                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 7                    |
|    time_elapsed    | -1683717609766310912 |
|    total_timesteps | 14336                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+03     |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 15000     |
| train/                  |           |
|    approx_kl            | 1.4129913 |
|    clip_fraction        | 0.891     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.22     |
|    explained_variance   | 0.488     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0616   |
|    n_updates            | 140       |
|    policy_gradient_loss | -0.0346   |
|    value_loss           | 0.00217   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 26.7                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 8                    |
|    time_elapsed    | -1683717609766310912 |
|    total_timesteps | 16384                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 17000     |
| train/                  |           |
|    approx_kl            | 1.9795562 |
|    clip_fraction        | 0.903     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.16     |
|    explained_variance   | 0.415     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0545   |
|    n_updates            | 160       |
|    policy_gradient_loss | -0.0383   |
|    value_loss           | 0.00145   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 22.4                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 9                    |
|    time_elapsed    | -1683717609766310656 |
|    total_timesteps | 18432                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 1e+03    |
|    mean_reward          | 0.5      |
| time/                   |          |
|    total_timesteps      | 19000    |
| train/                  |          |
|    approx_kl            | 2.745906 |
|    clip_fraction        | 0.919    |
|    clip_range           | 0.2      |
|    entropy_loss         | -6.02    |
|    explained_variance   | 0.347    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0708  |
|    n_updates            | 180      |
|    policy_gradient_loss | -0.0303  |
|    value_loss           | 0.00412  |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 20.5                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 10                   |
|    time_elapsed    | -1683717609766310656 |
|    total_timesteps | 20480                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+03     |
|    mean_reward          | 0         |
| time/                   |           |
|    total_timesteps      | 21000     |
| train/                  |           |
|    approx_kl            | 6.1108828 |
|    clip_fraction        | 0.942     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.83     |
|    explained_variance   | 0.37      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0549   |
|    n_updates            | 200       |
|    policy_gradient_loss | -0.0445   |
|    value_loss           | 0.00128   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 19.8                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 11                   |
|    time_elapsed    | -1683717609766310656 |
|    total_timesteps | 22528                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 1e+03    |
|    mean_reward          | 0        |
| time/                   |          |
|    total_timesteps      | 23000    |
| train/                  |          |
|    approx_kl            | 7.524822 |
|    clip_fraction        | 0.951    |
|    clip_range           | 0.2      |
|    entropy_loss         | -5.65    |
|    explained_variance   | 0.32     |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0807  |
|    n_updates            | 220      |
|    policy_gradient_loss | -0.0561  |
|    value_loss           | 0.000235 |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 19.6                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 12                   |
|    time_elapsed    | -1683717609766310400 |
|    total_timesteps | 24576                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 25000     |
| train/                  |           |
|    approx_kl            | 10.386607 |
|    clip_fraction        | 0.947     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.54     |
|    explained_variance   | 0.273     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0245   |
|    n_updates            | 240       |
|    policy_gradient_loss | -0.0525   |
|    value_loss           | 0.00122   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 17.2                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 13                   |
|    time_elapsed    | -1683717609766310400 |
|    total_timesteps | 26624                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+03     |
|    mean_reward          | 0         |
| time/                   |           |
|    total_timesteps      | 27000     |
| train/                  |           |
|    approx_kl            | 13.381576 |
|    clip_fraction        | 0.949     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.19     |
|    explained_variance   | 0.226     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0292   |
|    n_updates            | 260       |
|    policy_gradient_loss | -0.058    |
|    value_loss           | 0.00161   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 17.8                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 14                   |
|    time_elapsed    | -1683717609766310400 |
|    total_timesteps | 28672                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+03     |
|    mean_reward          | 0         |
| time/                   |           |
|    total_timesteps      | 29000     |
| train/                  |           |
|    approx_kl            | 19.041368 |
|    clip_fraction        | 0.958     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.02     |
|    explained_variance   | 0.201     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0916   |
|    n_updates            | 280       |
|    policy_gradient_loss | -0.0582   |
|    value_loss           | 0.00342   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 16.2                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 15                   |
|    time_elapsed    | -1683717609766310144 |
|    total_timesteps | 30720                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 31000     |
| train/                  |           |
|    approx_kl            | 34.165474 |
|    clip_fraction        | 0.961     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.65     |
|    explained_variance   | 0.247     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0707   |
|    n_updates            | 300       |
|    policy_gradient_loss | -0.0631   |
|    value_loss           | 0.00193   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 14.1                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 16                   |
|    time_elapsed    | -1683717609766310144 |
|    total_timesteps | 32768                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 10       |
|    mean_reward          | 0.9      |
| time/                   |          |
|    total_timesteps      | 33000    |
| train/                  |          |
|    approx_kl            | 69.34976 |
|    clip_fraction        | 0.947    |
|    clip_range           | 0.2      |
|    entropy_loss         | -4.3     |
|    explained_variance   | 0.141    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0892  |
|    n_updates            | 320      |
|    policy_gradient_loss | -0.0516  |
|    value_loss           | 0.00371  |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 13                   |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 17                   |
|    time_elapsed    | -1683717609766310144 |
|    total_timesteps | 34816                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 10       |
|    mean_reward          | 0.9      |
| time/                   |          |
|    total_timesteps      | 35000    |
| train/                  |          |
|    approx_kl            | 75.54309 |
|    clip_fraction        | 0.958    |
|    clip_range           | 0.2      |
|    entropy_loss         | -4.09    |
|    explained_variance   | 0.066    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0862  |
|    n_updates            | 340      |
|    policy_gradient_loss | -0.0632  |
|    value_loss           | 0.00115  |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 12.5                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 18                   |
|    time_elapsed    | -1683717609766309888 |
|    total_timesteps | 36864                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 10       |
|    mean_reward          | 0.9      |
| time/                   |          |
|    total_timesteps      | 37000    |
| train/                  |          |
|    approx_kl            | 55.37876 |
|    clip_fraction        | 0.953    |
|    clip_range           | 0.2      |
|    entropy_loss         | -3.78    |
|    explained_variance   | 0.0109   |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0662  |
|    n_updates            | 360      |
|    policy_gradient_loss | -0.0718  |
|    value_loss           | 0.00246  |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 11.5                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 19                   |
|    time_elapsed    | -1683717609766309888 |
|    total_timesteps | 38912                |
---------------------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 10         |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 39000      |
| train/                  |            |
|    approx_kl            | 127.147804 |
|    clip_fraction        | 0.949      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.5       |
|    explained_variance   | 0.00516    |
|    learning_rate        | 0.0003     |
|    loss                 | -0.104     |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.0259    |
|    value_loss           | 0.00367    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 11.5                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 20                   |
|    time_elapsed    | -1683717609766309888 |
|    total_timesteps | 40960                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 41000     |
| train/                  |           |
|    approx_kl            | 162.62067 |
|    clip_fraction        | 0.905     |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.28     |
|    explained_variance   | 0.00868   |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0993   |
|    n_updates            | 400       |
|    policy_gradient_loss | 0.332     |
|    value_loss           | 0.00117   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 11.2                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 21                   |
|    time_elapsed    | -1683717609766309888 |
|    total_timesteps | 43008                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 10       |
|    mean_reward          | 0.9      |
| time/                   |          |
|    total_timesteps      | 44000    |
| train/                  |          |
|    approx_kl            | 76.99173 |
|    clip_fraction        | 0.953    |
|    clip_range           | 0.2      |
|    entropy_loss         | -3.08    |
|    explained_variance   | -0.0649  |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0794  |
|    n_updates            | 420      |
|    policy_gradient_loss | -0.0676  |
|    value_loss           | 0.00232  |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 11                   |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 22                   |
|    time_elapsed    | -1683717609766309888 |
|    total_timesteps | 45056                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 46000     |
| train/                  |           |
|    approx_kl            | 94.135666 |
|    clip_fraction        | 0.966     |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.82     |
|    explained_variance   | -0.135    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0938   |
|    n_updates            | 440       |
|    policy_gradient_loss | -0.0777   |
|    value_loss           | 0.0014    |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 11                   |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 23                   |
|    time_elapsed    | -1683717609766309888 |
|    total_timesteps | 47104                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 10       |
|    mean_reward          | 0.9      |
| time/                   |          |
|    total_timesteps      | 48000    |
| train/                  |          |
|    approx_kl            | 81.63109 |
|    clip_fraction        | 0.953    |
|    clip_range           | 0.2      |
|    entropy_loss         | -2.77    |
|    explained_variance   | -0.0787  |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0875  |
|    n_updates            | 460      |
|    policy_gradient_loss | -0.0715  |
|    value_loss           | 0.00387  |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 10.5                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 24                   |
|    time_elapsed    | -1683717609766309632 |
|    total_timesteps | 49152                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 10        |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 167.31497 |
|    clip_fraction        | 0.934     |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.64     |
|    explained_variance   | -0.069    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0866   |
|    n_updates            | 480       |
|    policy_gradient_loss | -0.0587   |
|    value_loss           | 0.00206   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 0.9      |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 10.8                 |
|    ep_rew_mean     | 0.9                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 25                   |
|    time_elapsed    | -1683717609766309632 |
|    total_timesteps | 51200                |
---------------------------------------------
{'reward': [0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9], 'std': [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]}
{'reward': [0.9000000134110451, 0.9000000134110451, 0.9000000134110451, 0.9000000134110451, 0.9000000134110451, 0.9000000134110451, 0.9000000134110451, 0.9000000134110451, 0.9000000134110451, 0.9000000134110451], 'std': [9, 9, 9, 9, 9, 9, 9, 9, 9, 9]}
