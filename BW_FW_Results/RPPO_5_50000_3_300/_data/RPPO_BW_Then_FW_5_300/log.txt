Logging to ../Logging/RPPO_BW_Then_FW_5_300
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 16                   |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 1                    |
|    time_elapsed    | -1683715398296753664 |
|    total_timesteps | 2048                 |
---------------------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 3000        |
| train/                  |             |
|    approx_kl            | 0.028623939 |
|    clip_fraction        | 0.422       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.12       |
|    explained_variance   | -4.12       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.114      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0933     |
|    value_loss           | 0.0231      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 13.8                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 2                    |
|    time_elapsed    | -1683715398296753664 |
|    total_timesteps | 4096                 |
---------------------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50         |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.13121952 |
|    clip_fraction        | 0.58       |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.07      |
|    explained_variance   | -0.655     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0277    |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0333    |
|    value_loss           | 0.00251    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 13.2                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 3                    |
|    time_elapsed    | -1683715398296753664 |
|    total_timesteps | 6144                 |
---------------------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50         |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 7000       |
| train/                  |            |
|    approx_kl            | 0.37486875 |
|    clip_fraction        | 0.731      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.01      |
|    explained_variance   | -0.0493    |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0549     |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0169    |
|    value_loss           | 0.000618   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 11.2                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 4                    |
|    time_elapsed    | -1683715398296753408 |
|    total_timesteps | 8192                 |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50        |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 9000      |
| train/                  |           |
|    approx_kl            | 1.0076233 |
|    clip_fraction        | 0.863     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.89     |
|    explained_variance   | -0.33     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.00381   |
|    n_updates            | 80        |
|    policy_gradient_loss | -0.0162   |
|    value_loss           | 0.000238  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 12.5                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 5                    |
|    time_elapsed    | -1683715398296753408 |
|    total_timesteps | 10240                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50        |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 11000     |
| train/                  |           |
|    approx_kl            | 1.5820422 |
|    clip_fraction        | 0.891     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.71     |
|    explained_variance   | -0.377    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0864   |
|    n_updates            | 100       |
|    policy_gradient_loss | -0.0106   |
|    value_loss           | 0.00079   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 9.35                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 6                    |
|    time_elapsed    | -1683715398296753152 |
|    total_timesteps | 12288                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50        |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 13000     |
| train/                  |           |
|    approx_kl            | 6.5526905 |
|    clip_fraction        | 0.944     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.11     |
|    explained_variance   | -0.296    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.068    |
|    n_updates            | 120       |
|    policy_gradient_loss | 0.021     |
|    value_loss           | 0.000614  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 8.93                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 7                    |
|    time_elapsed    | -1683715398296753152 |
|    total_timesteps | 14336                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 15000     |
| train/                  |           |
|    approx_kl            | 14.462588 |
|    clip_fraction        | 0.958     |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.41     |
|    explained_variance   | -0.559    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.082    |
|    n_updates            | 140       |
|    policy_gradient_loss | -0.0699   |
|    value_loss           | 0.000229  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 8.53                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 8                    |
|    time_elapsed    | -1683715398296753152 |
|    total_timesteps | 16384                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 17000     |
| train/                  |           |
|    approx_kl            | 15.585156 |
|    clip_fraction        | 0.944     |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.68     |
|    explained_variance   | -0.581    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0449   |
|    n_updates            | 160       |
|    policy_gradient_loss | -0.0509   |
|    value_loss           | 9.55e-05  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 7.16                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 9                    |
|    time_elapsed    | -1683715398296752896 |
|    total_timesteps | 18432                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 19000     |
| train/                  |           |
|    approx_kl            | 40.503357 |
|    clip_fraction        | 0.935     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.94     |
|    explained_variance   | -0.772    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.121    |
|    n_updates            | 180       |
|    policy_gradient_loss | 0.986     |
|    value_loss           | 0.000271  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 6.2                  |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 10                   |
|    time_elapsed    | -1683715398296752896 |
|    total_timesteps | 20480                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 21000     |
| train/                  |           |
|    approx_kl            | 123.71585 |
|    clip_fraction        | 0.912     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.63     |
|    explained_variance   | -0.8      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0831   |
|    n_updates            | 200       |
|    policy_gradient_loss | 3.89      |
|    value_loss           | 0.000731  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.74                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 11                   |
|    time_elapsed    | -1683715398296752896 |
|    total_timesteps | 22528                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 23000     |
| train/                  |           |
|    approx_kl            | 52.766644 |
|    clip_fraction        | 0.938     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.46     |
|    explained_variance   | -0.703    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.00378  |
|    n_updates            | 220       |
|    policy_gradient_loss | -0.0157   |
|    value_loss           | 0.000133  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.72                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 12                   |
|    time_elapsed    | -1683715398296752640 |
|    total_timesteps | 24576                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5        |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 25000    |
| train/                  |          |
|    approx_kl            | 72.07368 |
|    clip_fraction        | 0.947    |
|    clip_range           | 0.2      |
|    entropy_loss         | -1.33    |
|    explained_variance   | -1.02    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0531  |
|    n_updates            | 240      |
|    policy_gradient_loss | -0.046   |
|    value_loss           | 0.000186 |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.22                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 13                   |
|    time_elapsed    | -1683715398296752640 |
|    total_timesteps | 26624                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 27000     |
| train/                  |           |
|    approx_kl            | 306.33734 |
|    clip_fraction        | 0.941     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.45     |
|    explained_variance   | -1.05     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.127    |
|    n_updates            | 260       |
|    policy_gradient_loss | -0.103    |
|    value_loss           | 0.000112  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.08                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 14                   |
|    time_elapsed    | -1683715398296752640 |
|    total_timesteps | 28672                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 29000     |
| train/                  |           |
|    approx_kl            | 45.181255 |
|    clip_fraction        | 0.874     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.985    |
|    explained_variance   | -1.01     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0947   |
|    n_updates            | 280       |
|    policy_gradient_loss | -0.0961   |
|    value_loss           | 0.000726  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.1                  |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 15                   |
|    time_elapsed    | -1683715398296752640 |
|    total_timesteps | 30720                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5        |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 31000    |
| train/                  |          |
|    approx_kl            | 38.33322 |
|    clip_fraction        | 0.719    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.528   |
|    explained_variance   | -1.23    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.124   |
|    n_updates            | 300      |
|    policy_gradient_loss | -0.116   |
|    value_loss           | 0.000529 |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.07                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 16                   |
|    time_elapsed    | -1683715398296752384 |
|    total_timesteps | 32768                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 33000     |
| train/                  |           |
|    approx_kl            | 20.236294 |
|    clip_fraction        | 0.341     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.29     |
|    explained_variance   | -1.17     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0708   |
|    n_updates            | 320       |
|    policy_gradient_loss | -0.0867   |
|    value_loss           | 0.000357  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.15                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 17                   |
|    time_elapsed    | -1683715398296752384 |
|    total_timesteps | 34816                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 21.007946 |
|    clip_fraction        | 0.456     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.187    |
|    explained_variance   | -0.978    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.109    |
|    n_updates            | 340       |
|    policy_gradient_loss | -0.0846   |
|    value_loss           | 4.59e-05  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.07                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 18                   |
|    time_elapsed    | -1683715398296752384 |
|    total_timesteps | 36864                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5        |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 37000    |
| train/                  |          |
|    approx_kl            | 8.680003 |
|    clip_fraction        | 0.263    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.148   |
|    explained_variance   | -1.05    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0814  |
|    n_updates            | 360      |
|    policy_gradient_loss | -0.0815  |
|    value_loss           | 0.000245 |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.01                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 19                   |
|    time_elapsed    | -1683715398296752128 |
|    total_timesteps | 38912                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5        |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 39000    |
| train/                  |          |
|    approx_kl            | 20.22358 |
|    clip_fraction        | 0.28     |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.132   |
|    explained_variance   | -0.961   |
|    learning_rate        | 0.0003   |
|    loss                 | -0.102   |
|    n_updates            | 380      |
|    policy_gradient_loss | -0.0817  |
|    value_loss           | 0.000251 |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.01                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 20                   |
|    time_elapsed    | -1683715398296752128 |
|    total_timesteps | 40960                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 41000     |
| train/                  |           |
|    approx_kl            | 15.568615 |
|    clip_fraction        | 0.289     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.126    |
|    explained_variance   | -1.06     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0687   |
|    n_updates            | 400       |
|    policy_gradient_loss | -0.0636   |
|    value_loss           | 0.000207  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.04                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 21                   |
|    time_elapsed    | -1683715398296752128 |
|    total_timesteps | 43008                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5        |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 44000    |
| train/                  |          |
|    approx_kl            | 6.259774 |
|    clip_fraction        | 0.489    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.148   |
|    explained_variance   | -1.03    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0248  |
|    n_updates            | 420      |
|    policy_gradient_loss | -0.0583  |
|    value_loss           | 4.93e-05 |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.09                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 22                   |
|    time_elapsed    | -1683715398296751872 |
|    total_timesteps | 45056                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 46000     |
| train/                  |           |
|    approx_kl            | 17.260399 |
|    clip_fraction        | 0.581     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.133    |
|    explained_variance   | -1.05     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0217   |
|    n_updates            | 440       |
|    policy_gradient_loss | -0.0228   |
|    value_loss           | 1.62e-05  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.05                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 23                   |
|    time_elapsed    | -1683715398296751872 |
|    total_timesteps | 47104                |
---------------------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 5         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 48000     |
| train/                  |           |
|    approx_kl            | 14.724181 |
|    clip_fraction        | 0.569     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.113    |
|    explained_variance   | -1.05     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0492   |
|    n_updates            | 460       |
|    policy_gradient_loss | -0.0273   |
|    value_loss           | 0.00061   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.02                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 24                   |
|    time_elapsed    | -1683715398296751872 |
|    total_timesteps | 49152                |
---------------------------------------------
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 5        |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 50000    |
| train/                  |          |
|    approx_kl            | 6.985423 |
|    clip_fraction        | 0.379    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.157   |
|    explained_variance   | -0.95    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.0391  |
|    n_updates            | 480      |
|    policy_gradient_loss | -0.0316  |
|    value_loss           | 0.000235 |
--------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------------------
| rollout/           |                      |
|    ep_len_mean     | 5.07                 |
|    ep_rew_mean     | 0.4                  |
| time/              |                      |
|    fps             | 0                    |
|    iterations      | 25                   |
|    time_elapsed    | -1683715398296751872 |
|    total_timesteps | 51200                |
---------------------------------------------
{'reward': [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4], 'std': [5, 5, 5, 5, 5, 5, 5, 5, 5, 5]}
{'reward': [0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645], 'std': [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]}
