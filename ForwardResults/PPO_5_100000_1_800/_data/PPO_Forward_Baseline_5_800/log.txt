Logging to ../Logging/PPO_Forward_Baseline_5_800
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 19       |
|    iterations      | 1        |
|    time_elapsed    | 102      |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 3000        |
| train/                  |             |
|    approx_kl            | 0.014354633 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.16       |
|    explained_variance   | -0.387      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00603    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.0271      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.0975   |
| time/              |          |
|    fps             | 19       |
|    iterations      | 2        |
|    time_elapsed    | 208      |
|    total_timesteps | 4096     |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50         |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.01717367 |
|    clip_fraction        | 0.145      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.13      |
|    explained_variance   | -0.0873    |
|    learning_rate        | 0.0003     |
|    loss                 | -0.015     |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.0217    |
|    value_loss           | 0.0049     |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.105    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 3        |
|    time_elapsed    | 314      |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 7000        |
| train/                  |             |
|    approx_kl            | 0.015829448 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.11       |
|    explained_variance   | -0.188      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0102     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.00246     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.107    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 4        |
|    time_elapsed    | 420      |
|    total_timesteps | 8192     |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 9000        |
| train/                  |             |
|    approx_kl            | 0.011064421 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.09       |
|    explained_variance   | 0.147       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0142     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 0.00141     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.101    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 5        |
|    time_elapsed    | 526      |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 11000       |
| train/                  |             |
|    approx_kl            | 0.015891813 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.07       |
|    explained_variance   | 0.0545      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0206     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0204     |
|    value_loss           | 0.00125     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.112    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 6        |
|    time_elapsed    | 631      |
|    total_timesteps | 12288    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 13000       |
| train/                  |             |
|    approx_kl            | 0.011868277 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.05       |
|    explained_variance   | 0.139       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.014      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.00137     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.123    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 7        |
|    time_elapsed    | 736      |
|    total_timesteps | 14336    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.010397183 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.03       |
|    explained_variance   | 0.369       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0311     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 0.00132     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.1     |
|    ep_rew_mean     | 0.132    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 8        |
|    time_elapsed    | 842      |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 17000       |
| train/                  |             |
|    approx_kl            | 0.009937494 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.99       |
|    explained_variance   | 0.359       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0059      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0126     |
|    value_loss           | 0.00137     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 19       |
|    iterations      | 9        |
|    time_elapsed    | 947      |
|    total_timesteps | 18432    |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | 0.1          |
| time/                   |              |
|    total_timesteps      | 19000        |
| train/                  |              |
|    approx_kl            | 0.0142817525 |
|    clip_fraction        | 0.192        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.95        |
|    explained_variance   | 0.416        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0373      |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.0199      |
|    value_loss           | 0.00129      |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.152    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 10       |
|    time_elapsed    | 1052     |
|    total_timesteps | 20480    |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | 0.1          |
| time/                   |              |
|    total_timesteps      | 21000        |
| train/                  |              |
|    approx_kl            | 0.0113531435 |
|    clip_fraction        | 0.147        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.93        |
|    explained_variance   | 0.417        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0178      |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.0157      |
|    value_loss           | 0.00156      |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.165    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 11       |
|    time_elapsed    | 1158     |
|    total_timesteps | 22528    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 23000       |
| train/                  |             |
|    approx_kl            | 0.010601694 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.91       |
|    explained_variance   | 0.393       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0126      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 0.00162     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | 0.169    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 12       |
|    time_elapsed    | 1263     |
|    total_timesteps | 24576    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.012561001 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.84       |
|    explained_variance   | 0.376       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.056      |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.00187     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.4     |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 19       |
|    iterations      | 13       |
|    time_elapsed    | 1368     |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 27000       |
| train/                  |             |
|    approx_kl            | 0.012105508 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.85       |
|    explained_variance   | 0.31        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0364     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.019      |
|    value_loss           | 0.00214     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.2     |
|    ep_rew_mean     | 0.202    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 14       |
|    time_elapsed    | 1473     |
|    total_timesteps | 28672    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 29000       |
| train/                  |             |
|    approx_kl            | 0.014459068 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.8        |
|    explained_variance   | 0.588       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00938    |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.00154     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.9     |
|    ep_rew_mean     | 0.206    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 15       |
|    time_elapsed    | 1578     |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 31000       |
| train/                  |             |
|    approx_kl            | 0.013232748 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.73       |
|    explained_variance   | 0.626       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00163    |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 0.00141     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.9     |
|    ep_rew_mean     | 0.238    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 16       |
|    time_elapsed    | 1683     |
|    total_timesteps | 32768    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 33000       |
| train/                  |             |
|    approx_kl            | 0.012077216 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.69       |
|    explained_variance   | 0.581       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00305    |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.00203     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.6     |
|    ep_rew_mean     | 0.262    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 17       |
|    time_elapsed    | 1788     |
|    total_timesteps | 34816    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.013147432 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 0.557       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.048      |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.0024      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.4     |
|    ep_rew_mean     | 0.266    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 18       |
|    time_elapsed    | 1892     |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 37000       |
| train/                  |             |
|    approx_kl            | 0.012934631 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.605       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0236     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.00205     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.3     |
|    ep_rew_mean     | 0.293    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 19       |
|    time_elapsed    | 1996     |
|    total_timesteps | 38912    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 39000       |
| train/                  |             |
|    approx_kl            | 0.014948769 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.46       |
|    explained_variance   | 0.48        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0148      |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0189     |
|    value_loss           | 0.00277     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.7     |
|    ep_rew_mean     | 0.328    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 20       |
|    time_elapsed    | 2099     |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 4          |
|    mean_reward          | 0.4        |
| time/                   |            |
|    total_timesteps      | 41000      |
| train/                  |            |
|    approx_kl            | 0.01806501 |
|    clip_fraction        | 0.238      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.38      |
|    explained_variance   | 0.612      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0426    |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0245    |
|    value_loss           | 0.00221    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 40.4     |
|    ep_rew_mean     | 0.337    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 21       |
|    time_elapsed    | 2184     |
|    total_timesteps | 43008    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 44000       |
| train/                  |             |
|    approx_kl            | 0.014388364 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.601       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0471     |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.00283     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.4     |
|    ep_rew_mean     | 0.361    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 22       |
|    time_elapsed    | 2267     |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 46000       |
| train/                  |             |
|    approx_kl            | 0.019330114 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.658       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0306     |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0232     |
|    value_loss           | 0.00214     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 32.4     |
|    ep_rew_mean     | 0.378    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 23       |
|    time_elapsed    | 2350     |
|    total_timesteps | 47104    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 48000       |
| train/                  |             |
|    approx_kl            | 0.016149512 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.02       |
|    explained_variance   | 0.676       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.057      |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.00213     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29       |
|    ep_rew_mean     | 0.373    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 24       |
|    time_elapsed    | 2433     |
|    total_timesteps | 49152    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.015089979 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.539       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.018      |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.00294     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | 0.392    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 25       |
|    time_elapsed    | 2515     |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 52000       |
| train/                  |             |
|    approx_kl            | 0.020584404 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.73       |
|    explained_variance   | 0.728       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.028      |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0239     |
|    value_loss           | 0.0011      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19       |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 20       |
|    iterations      | 26       |
|    time_elapsed    | 2596     |
|    total_timesteps | 53248    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 54000       |
| train/                  |             |
|    approx_kl            | 0.015451096 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.53       |
|    explained_variance   | 0.907       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0425     |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0232     |
|    value_loss           | 0.000318    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.9     |
|    ep_rew_mean     | 0.403    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 27       |
|    time_elapsed    | 2678     |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 56000       |
| train/                  |             |
|    approx_kl            | 0.022062037 |
|    clip_fraction        | 0.287       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.36       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0482     |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0237     |
|    value_loss           | 0.000506    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | 0.403    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 28       |
|    time_elapsed    | 2777     |
|    total_timesteps | 57344    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 58000       |
| train/                  |             |
|    approx_kl            | 0.017718345 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.21       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0304     |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0193     |
|    value_loss           | 0.000209    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 13.5     |
|    ep_rew_mean     | 0.401    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 29       |
|    time_elapsed    | 2857     |
|    total_timesteps | 59392    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.020639082 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.04       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00492     |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0217     |
|    value_loss           | 0.000125    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 12.7     |
|    ep_rew_mean     | 0.402    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 30       |
|    time_elapsed    | 2936     |
|    total_timesteps | 61440    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 4          |
|    mean_reward          | 0.4        |
| time/                   |            |
|    total_timesteps      | 62000      |
| train/                  |            |
|    approx_kl            | 0.02476797 |
|    clip_fraction        | 0.357      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.82      |
|    explained_variance   | 0.972      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0358    |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.0331    |
|    value_loss           | 0.000271   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 10.6     |
|    ep_rew_mean     | 0.401    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 31       |
|    time_elapsed    | 3014     |
|    total_timesteps | 63488    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 64000       |
| train/                  |             |
|    approx_kl            | 0.031932168 |
|    clip_fraction        | 0.346       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.51       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.056      |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.0311     |
|    value_loss           | 0.000367    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 8.53     |
|    ep_rew_mean     | 0.402    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 32       |
|    time_elapsed    | 3092     |
|    total_timesteps | 65536    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 66000       |
| train/                  |             |
|    approx_kl            | 0.031104263 |
|    clip_fraction        | 0.421       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.09       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0845     |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0435     |
|    value_loss           | 0.000115    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 7.53     |
|    ep_rew_mean     | 0.403    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 33       |
|    time_elapsed    | 3167     |
|    total_timesteps | 67584    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 68000       |
| train/                  |             |
|    approx_kl            | 0.096199594 |
|    clip_fraction        | 0.418       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.044      |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.0412     |
|    value_loss           | 0.000187    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.49     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 21       |
|    iterations      | 34       |
|    time_elapsed    | 3239     |
|    total_timesteps | 69632    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.013878787 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00351     |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.0232     |
|    value_loss           | 0.00026     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.12     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 21       |
|    iterations      | 35       |
|    time_elapsed    | 3310     |
|    total_timesteps | 71680    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 4          |
|    mean_reward          | 0.4        |
| time/                   |            |
|    total_timesteps      | 72000      |
| train/                  |            |
|    approx_kl            | 0.02771496 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.853     |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0632    |
|    n_updates            | 350        |
|    policy_gradient_loss | -0.0395    |
|    value_loss           | 0.000182   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.74     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 21       |
|    iterations      | 36       |
|    time_elapsed    | 3380     |
|    total_timesteps | 73728    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 74000       |
| train/                  |             |
|    approx_kl            | 0.004683006 |
|    clip_fraction        | 0.0504      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.701      |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00816    |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.00788    |
|    value_loss           | 5.96e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.72     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 21       |
|    iterations      | 37       |
|    time_elapsed    | 3449     |
|    total_timesteps | 75776    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 4          |
|    mean_reward          | 0.4        |
| time/                   |            |
|    total_timesteps      | 76000      |
| train/                  |            |
|    approx_kl            | 0.07222945 |
|    clip_fraction        | 0.0929     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.473     |
|    explained_variance   | 0.995      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0986    |
|    n_updates            | 370        |
|    policy_gradient_loss | -0.0263    |
|    value_loss           | 2.18e-05   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.42     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 22       |
|    iterations      | 38       |
|    time_elapsed    | 3519     |
|    total_timesteps | 77824    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 4          |
|    mean_reward          | 0.4        |
| time/                   |            |
|    total_timesteps      | 78000      |
| train/                  |            |
|    approx_kl            | 0.01096753 |
|    clip_fraction        | 0.102      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.503     |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0503    |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.029     |
|    value_loss           | 0.000118   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.33     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 22       |
|    iterations      | 39       |
|    time_elapsed    | 3587     |
|    total_timesteps | 79872    |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 4            |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 80000        |
| train/                  |              |
|    approx_kl            | 0.0023974476 |
|    clip_fraction        | 0.0254       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.346       |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0176      |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.00555     |
|    value_loss           | 3.28e-05     |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.31     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 22       |
|    iterations      | 40       |
|    time_elapsed    | 3656     |
|    total_timesteps | 81920    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 4          |
|    mean_reward          | 0.4        |
| time/                   |            |
|    total_timesteps      | 82000      |
| train/                  |            |
|    approx_kl            | 0.09191626 |
|    clip_fraction        | 0.081      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.196     |
|    explained_variance   | 0.997      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0524    |
|    n_updates            | 400        |
|    policy_gradient_loss | -0.0373    |
|    value_loss           | 1.43e-05   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.13     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 22       |
|    iterations      | 41       |
|    time_elapsed    | 3724     |
|    total_timesteps | 83968    |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 4            |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 84000        |
| train/                  |              |
|    approx_kl            | 0.0029164979 |
|    clip_fraction        | 0.0276       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.146       |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0285      |
|    n_updates            | 410          |
|    policy_gradient_loss | -0.0174      |
|    value_loss           | 8.71e-06     |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.12     |
|    ep_rew_mean     | 0.402    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 42       |
|    time_elapsed    | 3792     |
|    total_timesteps | 86016    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 87000       |
| train/                  |             |
|    approx_kl            | 0.002478234 |
|    clip_fraction        | 0.0186      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.159      |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0181     |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0072     |
|    value_loss           | 0.00033     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.31     |
|    ep_rew_mean     | 0.403    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 43       |
|    time_elapsed    | 3859     |
|    total_timesteps | 88064    |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 4            |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 89000        |
| train/                  |              |
|    approx_kl            | 0.0017693007 |
|    clip_fraction        | 0.0152       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.136       |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00537     |
|    n_updates            | 430          |
|    policy_gradient_loss | -0.005       |
|    value_loss           | 0.000131     |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.54     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 22       |
|    iterations      | 44       |
|    time_elapsed    | 3927     |
|    total_timesteps | 90112    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 91000       |
| train/                  |             |
|    approx_kl            | 0.002301487 |
|    clip_fraction        | 0.0199      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.186      |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0274     |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.00598    |
|    value_loss           | 0.00042     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.09     |
|    ep_rew_mean     | 0.402    |
| time/              |          |
|    fps             | 23       |
|    iterations      | 45       |
|    time_elapsed    | 3994     |
|    total_timesteps | 92160    |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 4            |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 93000        |
| train/                  |              |
|    approx_kl            | 0.0006133255 |
|    clip_fraction        | 0.00708      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.127       |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000174     |
|    n_updates            | 450          |
|    policy_gradient_loss | -0.00205     |
|    value_loss           | 0.000161     |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.34     |
|    ep_rew_mean     | 0.403    |
| time/              |          |
|    fps             | 23       |
|    iterations      | 46       |
|    time_elapsed    | 4061     |
|    total_timesteps | 94208    |
---------------------------------
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 4             |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 95000         |
| train/                  |               |
|    approx_kl            | 0.00056980166 |
|    clip_fraction        | 0.00718       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.132        |
|    explained_variance   | 0.988         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.0012       |
|    n_updates            | 460           |
|    policy_gradient_loss | -0.00176      |
|    value_loss           | 9.8e-05       |
-------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.12     |
|    ep_rew_mean     | 0.402    |
| time/              |          |
|    fps             | 23       |
|    iterations      | 47       |
|    time_elapsed    | 4128     |
|    total_timesteps | 96256    |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 4            |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 97000        |
| train/                  |              |
|    approx_kl            | 0.0019545727 |
|    clip_fraction        | 0.00806      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0747      |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00357     |
|    n_updates            | 470          |
|    policy_gradient_loss | -0.0031      |
|    value_loss           | 0.000127     |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.04     |
|    ep_rew_mean     | 0.401    |
| time/              |          |
|    fps             | 23       |
|    iterations      | 48       |
|    time_elapsed    | 4195     |
|    total_timesteps | 98304    |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 4            |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 99000        |
| train/                  |              |
|    approx_kl            | 0.0002539909 |
|    clip_fraction        | 0.0041       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.061       |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00164     |
|    n_updates            | 480          |
|    policy_gradient_loss | -0.00116     |
|    value_loss           | 7.03e-05     |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.07     |
|    ep_rew_mean     | 0.402    |
| time/              |          |
|    fps             | 23       |
|    iterations      | 49       |
|    time_elapsed    | 4262     |
|    total_timesteps | 100352   |
---------------------------------
{'reward': [0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645], 'std': [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]}
