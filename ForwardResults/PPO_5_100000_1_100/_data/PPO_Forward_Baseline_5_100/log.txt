Logging to ../Logging/PPO_Forward_Baseline_5_100
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.115    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 1        |
|    time_elapsed    | 102      |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 3000        |
| train/                  |             |
|    approx_kl            | 0.015963929 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.15       |
|    explained_variance   | -0.373      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0395     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0186     |
|    value_loss           | 0.0089      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.101    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 2        |
|    time_elapsed    | 207      |
|    total_timesteps | 4096     |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50         |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.01310749 |
|    clip_fraction        | 0.147      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.13      |
|    explained_variance   | -0.266     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0599     |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.0154    |
|    value_loss           | 0.00365    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.102    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 3        |
|    time_elapsed    | 311      |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 7000        |
| train/                  |             |
|    approx_kl            | 0.013437867 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.11       |
|    explained_variance   | -0.263      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0186     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.00227     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.105    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 4        |
|    time_elapsed    | 415      |
|    total_timesteps | 8192     |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 9000        |
| train/                  |             |
|    approx_kl            | 0.014480833 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.07       |
|    explained_variance   | 0.102       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0259     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 0.00154     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.122    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 5        |
|    time_elapsed    | 519      |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 11000       |
| train/                  |             |
|    approx_kl            | 0.011085574 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.04       |
|    explained_variance   | 0.299       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0302     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.00142     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.132    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 6        |
|    time_elapsed    | 623      |
|    total_timesteps | 12288    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 13000       |
| train/                  |             |
|    approx_kl            | 0.009694628 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.01       |
|    explained_variance   | 0.382       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0098     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 0.00119     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.8     |
|    ep_rew_mean     | 0.142    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 7        |
|    time_elapsed    | 727      |
|    total_timesteps | 14336    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.013204062 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.97       |
|    explained_variance   | 0.496       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0308     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.00137     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.142    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 8        |
|    time_elapsed    | 831      |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 17000       |
| train/                  |             |
|    approx_kl            | 0.009371333 |
|    clip_fraction        | 0.0979      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.94       |
|    explained_variance   | 0.215       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0091      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 0.00161     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.135    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 9        |
|    time_elapsed    | 935      |
|    total_timesteps | 18432    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 19000       |
| train/                  |             |
|    approx_kl            | 0.011649704 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.92       |
|    explained_variance   | 0.317       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0411     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.00156     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.148    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 10       |
|    time_elapsed    | 1038     |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 21000       |
| train/                  |             |
|    approx_kl            | 0.014466746 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.87       |
|    explained_variance   | 0.413       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0332      |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0182     |
|    value_loss           | 0.00161     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.181    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 11       |
|    time_elapsed    | 1142     |
|    total_timesteps | 22528    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 23000       |
| train/                  |             |
|    approx_kl            | 0.014787607 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.85       |
|    explained_variance   | 0.481       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0134     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0189     |
|    value_loss           | 0.00175     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.207    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 12       |
|    time_elapsed    | 1246     |
|    total_timesteps | 24576    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.015290158 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.79       |
|    explained_variance   | 0.515       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0408     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0205     |
|    value_loss           | 0.00174     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.9     |
|    ep_rew_mean     | 0.226    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 13       |
|    time_elapsed    | 1350     |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 27000       |
| train/                  |             |
|    approx_kl            | 0.014675039 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.69       |
|    explained_variance   | 0.573       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0317     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0216     |
|    value_loss           | 0.00189     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.235    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 14       |
|    time_elapsed    | 1453     |
|    total_timesteps | 28672    |
---------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50        |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 29000     |
| train/                  |           |
|    approx_kl            | 0.0161235 |
|    clip_fraction        | 0.188     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.66     |
|    explained_variance   | 0.691     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0445   |
|    n_updates            | 140       |
|    policy_gradient_loss | -0.0191   |
|    value_loss           | 0.00151   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.4     |
|    ep_rew_mean     | 0.247    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 15       |
|    time_elapsed    | 1557     |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 31000       |
| train/                  |             |
|    approx_kl            | 0.013539621 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.58       |
|    explained_variance   | 0.73        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0394     |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.00143     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 0.263    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 16       |
|    time_elapsed    | 1661     |
|    total_timesteps | 32768    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 33000       |
| train/                  |             |
|    approx_kl            | 0.016551303 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.857       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.019      |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0236     |
|    value_loss           | 0.000549    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.2     |
|    ep_rew_mean     | 0.288    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 17       |
|    time_elapsed    | 1765     |
|    total_timesteps | 34816    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.017878255 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.52       |
|    explained_variance   | 0.907       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.024      |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0207     |
|    value_loss           | 0.000747    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.9     |
|    ep_rew_mean     | 0.303    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 18       |
|    time_elapsed    | 1870     |
|    total_timesteps | 36864    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50         |
|    mean_reward          | 0.3        |
| time/                   |            |
|    total_timesteps      | 37000      |
| train/                  |            |
|    approx_kl            | 0.01448206 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.49      |
|    explained_variance   | 0.904      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0198    |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0203    |
|    value_loss           | 0.000842   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | 0.313    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 19       |
|    time_elapsed    | 1974     |
|    total_timesteps | 38912    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 39000       |
| train/                  |             |
|    approx_kl            | 0.015610011 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.864       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00622     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.001       |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.6     |
|    ep_rew_mean     | 0.315    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 20       |
|    time_elapsed    | 2078     |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 41000       |
| train/                  |             |
|    approx_kl            | 0.015745733 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0106     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.000636    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.6     |
|    ep_rew_mean     | 0.324    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 21       |
|    time_elapsed    | 2191     |
|    total_timesteps | 43008    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 44000       |
| train/                  |             |
|    approx_kl            | 0.010547742 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | 0.888       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00252    |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00771    |
|    value_loss           | 0.00132     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48       |
|    ep_rew_mean     | 0.328    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 22       |
|    time_elapsed    | 2295     |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 46000       |
| train/                  |             |
|    approx_kl            | 0.012334201 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0093      |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 0.000581    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.6     |
|    ep_rew_mean     | 0.327    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 23       |
|    time_elapsed    | 2399     |
|    total_timesteps | 47104    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 48000       |
| train/                  |             |
|    approx_kl            | 0.011064317 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.49       |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.01       |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00839    |
|    value_loss           | 0.000705    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48       |
|    ep_rew_mean     | 0.327    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 24       |
|    time_elapsed    | 2503     |
|    total_timesteps | 49152    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.009967296 |
|    clip_fraction        | 0.099       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00702     |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0055     |
|    value_loss           | 0.00083     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.3     |
|    ep_rew_mean     | 0.343    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 25       |
|    time_elapsed    | 2606     |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 52000       |
| train/                  |             |
|    approx_kl            | 0.011931046 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.883       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00706    |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 0.00105     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.2     |
|    ep_rew_mean     | 0.339    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 26       |
|    time_elapsed    | 2710     |
|    total_timesteps | 53248    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50         |
|    mean_reward          | 0.3        |
| time/                   |            |
|    total_timesteps      | 54000      |
| train/                  |            |
|    approx_kl            | 0.01041981 |
|    clip_fraction        | 0.131      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.44      |
|    explained_variance   | 0.929      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0113    |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.00744   |
|    value_loss           | 0.00068    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 43.8     |
|    ep_rew_mean     | 0.341    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 27       |
|    time_elapsed    | 2814     |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 56000       |
| train/                  |             |
|    approx_kl            | 0.010257745 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 0.829       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0327     |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 0.00127     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 40.9     |
|    ep_rew_mean     | 0.353    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 28       |
|    time_elapsed    | 2917     |
|    total_timesteps | 57344    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 58000       |
| train/                  |             |
|    approx_kl            | 0.009508287 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.846       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00448     |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.00883    |
|    value_loss           | 0.00126     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.2     |
|    ep_rew_mean     | 0.367    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 29       |
|    time_elapsed    | 3020     |
|    total_timesteps | 59392    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50         |
|    mean_reward          | 0.3        |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.01368172 |
|    clip_fraction        | 0.163      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.37      |
|    explained_variance   | 0.863      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00867   |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0141    |
|    value_loss           | 0.00106    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 31.4     |
|    ep_rew_mean     | 0.381    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 30       |
|    time_elapsed    | 3123     |
|    total_timesteps | 61440    |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | 0.3          |
| time/                   |              |
|    total_timesteps      | 62000        |
| train/                  |              |
|    approx_kl            | 0.0143970195 |
|    clip_fraction        | 0.15         |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.865        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0146      |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.0136      |
|    value_loss           | 0.00113      |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.375    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 31       |
|    time_elapsed    | 3225     |
|    total_timesteps | 63488    |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | 0.3          |
| time/                   |              |
|    total_timesteps      | 64000        |
| train/                  |              |
|    approx_kl            | 0.0104105575 |
|    clip_fraction        | 0.162        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.27        |
|    explained_variance   | 0.846        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00653     |
|    n_updates            | 310          |
|    policy_gradient_loss | -0.0133      |
|    value_loss           | 0.00127      |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 0.381    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 32       |
|    time_elapsed    | 3327     |
|    total_timesteps | 65536    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 66000       |
| train/                  |             |
|    approx_kl            | 0.009938086 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.819       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0194     |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 0.00122     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.6     |
|    ep_rew_mean     | 0.391    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 33       |
|    time_elapsed    | 3410     |
|    total_timesteps | 67584    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 68000       |
| train/                  |             |
|    approx_kl            | 0.012105688 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.07       |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00929    |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 0.00108     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.5     |
|    ep_rew_mean     | 0.392    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 34       |
|    time_elapsed    | 3492     |
|    total_timesteps | 69632    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.007327425 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.96       |
|    explained_variance   | 0.856       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000389   |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.00759    |
|    value_loss           | 0.00127     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.2     |
|    ep_rew_mean     | 0.399    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 35       |
|    time_elapsed    | 3574     |
|    total_timesteps | 71680    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 72000       |
| train/                  |             |
|    approx_kl            | 0.011089664 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.88       |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00466    |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.00944    |
|    value_loss           | 0.00091     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.2     |
|    ep_rew_mean     | 0.402    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 36       |
|    time_elapsed    | 3655     |
|    total_timesteps | 73728    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 4          |
|    mean_reward          | 0.4        |
| time/                   |            |
|    total_timesteps      | 74000      |
| train/                  |            |
|    approx_kl            | 0.02028476 |
|    clip_fraction        | 0.247      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.67      |
|    explained_variance   | 0.922      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0118    |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.0212    |
|    value_loss           | 0.000775   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | 0.398    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 37       |
|    time_elapsed    | 3736     |
|    total_timesteps | 75776    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 76000       |
| train/                  |             |
|    approx_kl            | 0.014403129 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.52       |
|    explained_variance   | 0.935       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00409    |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.0145     |
|    value_loss           | 0.000561    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 13.7     |
|    ep_rew_mean     | 0.403    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 38       |
|    time_elapsed    | 3816     |
|    total_timesteps | 77824    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 78000       |
| train/                  |             |
|    approx_kl            | 0.009915333 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.39       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0132     |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.00916    |
|    value_loss           | 0.000753    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 12.5     |
|    ep_rew_mean     | 0.403    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 39       |
|    time_elapsed    | 3895     |
|    total_timesteps | 79872    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.025148135 |
|    clip_fraction        | 0.417       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3          |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0338     |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.0293     |
|    value_loss           | 0.00039     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.71     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 20       |
|    iterations      | 40       |
|    time_elapsed    | 3972     |
|    total_timesteps | 81920    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 82000       |
| train/                  |             |
|    approx_kl            | 0.010942322 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.66       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.031      |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.021      |
|    value_loss           | 8.05e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 8.06     |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 20       |
|    iterations      | 41       |
|    time_elapsed    | 4048     |
|    total_timesteps | 83968    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 4          |
|    mean_reward          | 0.4        |
| time/                   |            |
|    total_timesteps      | 84000      |
| train/                  |            |
|    approx_kl            | 0.01739792 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.19      |
|    explained_variance   | 0.997      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0283    |
|    n_updates            | 410        |
|    policy_gradient_loss | -0.0335    |
|    value_loss           | 2.93e-05   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 7.09     |
|    ep_rew_mean     | 0.403    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 42       |
|    time_elapsed    | 4124     |
|    total_timesteps | 86016    |
---------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 4         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 87000     |
| train/                  |           |
|    approx_kl            | 0.1701926 |
|    clip_fraction        | 0.444     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.45     |
|    explained_variance   | 0.967     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0702   |
|    n_updates            | 420       |
|    policy_gradient_loss | -0.0359   |
|    value_loss           | 0.00047   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.82     |
|    ep_rew_mean     | 0.401    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 43       |
|    time_elapsed    | 4194     |
|    total_timesteps | 88064    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 89000       |
| train/                  |             |
|    approx_kl            | 0.023034105 |
|    clip_fraction        | 0.357       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.872      |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0812     |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.066      |
|    value_loss           | 1.93e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.08     |
|    ep_rew_mean     | 0.401    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 44       |
|    time_elapsed    | 4264     |
|    total_timesteps | 90112    |
---------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 4         |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 91000     |
| train/                  |           |
|    approx_kl            | 0.0662266 |
|    clip_fraction        | 0.331     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.689    |
|    explained_variance   | 0.998     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.122    |
|    n_updates            | 440       |
|    policy_gradient_loss | -0.0638   |
|    value_loss           | 2.5e-05   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.14     |
|    ep_rew_mean     | 0.403    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 45       |
|    time_elapsed    | 4335     |
|    total_timesteps | 92160    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 4          |
|    mean_reward          | 0.4        |
| time/                   |            |
|    total_timesteps      | 93000      |
| train/                  |            |
|    approx_kl            | 0.04979682 |
|    clip_fraction        | 0.33       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.675     |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0596    |
|    n_updates            | 450        |
|    policy_gradient_loss | -0.0378    |
|    value_loss           | 0.000245   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.7      |
|    ep_rew_mean     | 0.403    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 46       |
|    time_elapsed    | 4403     |
|    total_timesteps | 94208    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 95000       |
| train/                  |             |
|    approx_kl            | 0.018177513 |
|    clip_fraction        | 0.27        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.419      |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0442      |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.0191     |
|    value_loss           | 0.000224    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.47     |
|    ep_rew_mean     | 0.403    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 47       |
|    time_elapsed    | 4471     |
|    total_timesteps | 96256    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 97000       |
| train/                  |             |
|    approx_kl            | 0.015247923 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.525      |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0742     |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.0338     |
|    value_loss           | 0.000179    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.74     |
|    ep_rew_mean     | 0.402    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 48       |
|    time_elapsed    | 4540     |
|    total_timesteps | 98304    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 4           |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 99000       |
| train/                  |             |
|    approx_kl            | 0.023270523 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.471      |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0168     |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.0202     |
|    value_loss           | 0.000213    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 4        |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.67     |
|    ep_rew_mean     | 0.401    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 49       |
|    time_elapsed    | 4608     |
|    total_timesteps | 100352   |
---------------------------------
{'reward': [0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645, 0.4000000059604645], 'std': [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]}
