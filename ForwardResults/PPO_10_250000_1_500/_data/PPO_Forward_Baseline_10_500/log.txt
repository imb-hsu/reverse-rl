Logging to ../Logging/PPO_Forward_Baseline_10_500
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 835      |
|    ep_rew_mean     | 0.75     |
| time/              |          |
|    fps             | 26       |
|    iterations      | 1        |
|    time_elapsed    | 78       |
|    total_timesteps | 2048     |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 3000       |
| train/                  |            |
|    approx_kl            | 0.01517215 |
|    clip_fraction        | 0.147      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.54      |
|    explained_variance   | 0.674      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0241    |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.0279    |
|    value_loss           | 0.019      |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 990      |
|    ep_rew_mean     | 0.9      |
| time/              |          |
|    fps             | 25       |
|    iterations      | 2        |
|    time_elapsed    | 160      |
|    total_timesteps | 4096     |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.01825938 |
|    clip_fraction        | 0.182      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.51      |
|    explained_variance   | 0.894      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0562    |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.0251    |
|    value_loss           | 0.00456    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.817    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 3        |
|    time_elapsed    | 242      |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 7000        |
| train/                  |             |
|    approx_kl            | 0.017228538 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.51       |
|    explained_variance   | 0.697       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0409     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0297     |
|    value_loss           | 0.00816     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.75     |
| time/              |          |
|    fps             | 25       |
|    iterations      | 4        |
|    time_elapsed    | 325      |
|    total_timesteps | 8192     |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 9000        |
| train/                  |             |
|    approx_kl            | 0.011991855 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.5        |
|    explained_variance   | 0.899       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00727     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 0.00362     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.76     |
| time/              |          |
|    fps             | 25       |
|    iterations      | 5        |
|    time_elapsed    | 406      |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 11000       |
| train/                  |             |
|    approx_kl            | 0.016259572 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.5        |
|    explained_variance   | 0.694       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0123     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0234     |
|    value_loss           | 0.00336     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.8      |
| time/              |          |
|    fps             | 25       |
|    iterations      | 6        |
|    time_elapsed    | 487      |
|    total_timesteps | 12288    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 13000       |
| train/                  |             |
|    approx_kl            | 0.014724098 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.48       |
|    explained_variance   | 0.771       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0347     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0275     |
|    value_loss           | 0.00282     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.843    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 7        |
|    time_elapsed    | 568      |
|    total_timesteps | 14336    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.013875114 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.48       |
|    explained_variance   | 0.712       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0419     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0309     |
|    value_loss           | 0.00314     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.837    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 8        |
|    time_elapsed    | 649      |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 17000       |
| train/                  |             |
|    approx_kl            | 0.016499076 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.46       |
|    explained_variance   | 0.838       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0307     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0273     |
|    value_loss           | 0.00246     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.861    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 9        |
|    time_elapsed    | 730      |
|    total_timesteps | 18432    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 19000       |
| train/                  |             |
|    approx_kl            | 0.017306833 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.45       |
|    explained_variance   | 0.757       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0411     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0279     |
|    value_loss           | 0.00291     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.84     |
| time/              |          |
|    fps             | 25       |
|    iterations      | 10       |
|    time_elapsed    | 811      |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 21000       |
| train/                  |             |
|    approx_kl            | 0.016799496 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.44       |
|    explained_variance   | 0.789       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0585     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0279     |
|    value_loss           | 0.00132     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.841    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 11       |
|    time_elapsed    | 892      |
|    total_timesteps | 22528    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 23000       |
| train/                  |             |
|    approx_kl            | 0.014908725 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.44       |
|    explained_variance   | 0.811       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0398     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.026      |
|    value_loss           | 0.00208     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.817    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 12       |
|    time_elapsed    | 973      |
|    total_timesteps | 24576    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.01742739 |
|    clip_fraction        | 0.192      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.43      |
|    explained_variance   | 0.746      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.039     |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0273    |
|    value_loss           | 0.00183    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.808    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 13       |
|    time_elapsed    | 1054     |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 27000       |
| train/                  |             |
|    approx_kl            | 0.019204814 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.4        |
|    explained_variance   | 0.838       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0285      |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0266     |
|    value_loss           | 0.00175     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.8      |
| time/              |          |
|    fps             | 25       |
|    iterations      | 14       |
|    time_elapsed    | 1135     |
|    total_timesteps | 28672    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 29000       |
| train/                  |             |
|    approx_kl            | 0.016323319 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.4        |
|    explained_variance   | 0.672       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0342     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0287     |
|    value_loss           | 0.00215     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.8      |
| time/              |          |
|    fps             | 25       |
|    iterations      | 15       |
|    time_elapsed    | 1217     |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 31000       |
| train/                  |             |
|    approx_kl            | 0.016121227 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.39       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0322     |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0302     |
|    value_loss           | 0.000343    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 995      |
|    ep_rew_mean     | 0.797    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 16       |
|    time_elapsed    | 1298     |
|    total_timesteps | 32768    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 33000       |
| train/                  |             |
|    approx_kl            | 0.014255166 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.32       |
|    explained_variance   | 0.655       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0312     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0235     |
|    value_loss           | 0.00159     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 992      |
|    ep_rew_mean     | 0.815    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 17       |
|    time_elapsed    | 1379     |
|    total_timesteps | 34816    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.016074937 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.34       |
|    explained_variance   | 0.843       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0186     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.027      |
|    value_loss           | 0.00153     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.825    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 18       |
|    time_elapsed    | 1460     |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 37000       |
| train/                  |             |
|    approx_kl            | 0.015146805 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.34       |
|    explained_variance   | 0.725       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0166     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0251     |
|    value_loss           | 0.00117     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.824    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 19       |
|    time_elapsed    | 1541     |
|    total_timesteps | 38912    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 39000       |
| train/                  |             |
|    approx_kl            | 0.018557265 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.23       |
|    explained_variance   | 0.907       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0102     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0287     |
|    value_loss           | 0.000774    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.82     |
| time/              |          |
|    fps             | 25       |
|    iterations      | 20       |
|    time_elapsed    | 1622     |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 41000       |
| train/                  |             |
|    approx_kl            | 0.019395586 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.21       |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0649     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0314     |
|    value_loss           | 0.00036     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.805    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 21       |
|    time_elapsed    | 1730     |
|    total_timesteps | 43008    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 44000       |
| train/                  |             |
|    approx_kl            | 0.013939612 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.18       |
|    explained_variance   | 0.821       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.031      |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0189     |
|    value_loss           | 0.000892    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.804    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 22       |
|    time_elapsed    | 1811     |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 46000       |
| train/                  |             |
|    approx_kl            | 0.021403976 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.22       |
|    explained_variance   | 0.71        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0266     |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0278     |
|    value_loss           | 0.00104     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.806    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 23       |
|    time_elapsed    | 1893     |
|    total_timesteps | 47104    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 48000       |
| train/                  |             |
|    approx_kl            | 0.015680749 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.3        |
|    explained_variance   | 0.848       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0243     |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0226     |
|    value_loss           | 0.000593    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.8      |
| time/              |          |
|    fps             | 24       |
|    iterations      | 24       |
|    time_elapsed    | 1974     |
|    total_timesteps | 49152    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.015363779 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.2        |
|    explained_variance   | 0.884       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0569     |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0266     |
|    value_loss           | 0.000684    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.794    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 25       |
|    time_elapsed    | 2055     |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 52000       |
| train/                  |             |
|    approx_kl            | 0.014293888 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.16       |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00288     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0212     |
|    value_loss           | 0.000434    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.794    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 26       |
|    time_elapsed    | 2136     |
|    total_timesteps | 53248    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 54000       |
| train/                  |             |
|    approx_kl            | 0.017222308 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.09       |
|    explained_variance   | 0.904       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0316     |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0289     |
|    value_loss           | 0.00114     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 990      |
|    ep_rew_mean     | 0.791    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 27       |
|    time_elapsed    | 2218     |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 56000       |
| train/                  |             |
|    approx_kl            | 0.016039811 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.26       |
|    explained_variance   | 0.835       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0254     |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0275     |
|    value_loss           | 0.000989    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.798    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 28       |
|    time_elapsed    | 2300     |
|    total_timesteps | 57344    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 58000       |
| train/                  |             |
|    approx_kl            | 0.011574503 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.19       |
|    explained_variance   | 0.785       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0221     |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0255     |
|    value_loss           | 0.00134     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.797    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 29       |
|    time_elapsed    | 2382     |
|    total_timesteps | 59392    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.01670259 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.14      |
|    explained_variance   | 0.885      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0303    |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0256    |
|    value_loss           | 0.000908   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.807    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 30       |
|    time_elapsed    | 2464     |
|    total_timesteps | 61440    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 62000       |
| train/                  |             |
|    approx_kl            | 0.018335667 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.22       |
|    explained_variance   | 0.858       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0369     |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.027      |
|    value_loss           | 0.000769    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.805    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 31       |
|    time_elapsed    | 2545     |
|    total_timesteps | 63488    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 64000       |
| train/                  |             |
|    approx_kl            | 0.026286244 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.19       |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0553     |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.0376     |
|    value_loss           | 0.000358    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.797    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 32       |
|    time_elapsed    | 2626     |
|    total_timesteps | 65536    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 66000      |
| train/                  |            |
|    approx_kl            | 0.01821007 |
|    clip_fraction        | 0.184      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.22      |
|    explained_variance   | 0.927      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0224    |
|    n_updates            | 320        |
|    policy_gradient_loss | -0.025     |
|    value_loss           | 0.000431   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 996      |
|    ep_rew_mean     | 0.804    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 33       |
|    time_elapsed    | 2707     |
|    total_timesteps | 67584    |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | 0.2          |
| time/                   |              |
|    total_timesteps      | 68000        |
| train/                  |              |
|    approx_kl            | 0.0150812445 |
|    clip_fraction        | 0.177        |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.21        |
|    explained_variance   | 0.804        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0432      |
|    n_updates            | 330          |
|    policy_gradient_loss | -0.0245      |
|    value_loss           | 0.000602     |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.807    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 34       |
|    time_elapsed    | 2788     |
|    total_timesteps | 69632    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 70000      |
| train/                  |            |
|    approx_kl            | 0.01524772 |
|    clip_fraction        | 0.177      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.18      |
|    explained_variance   | 0.921      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.061     |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.027     |
|    value_loss           | 0.000595   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 993      |
|    ep_rew_mean     | 0.808    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 35       |
|    time_elapsed    | 2869     |
|    total_timesteps | 71680    |
---------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+03     |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 72000     |
| train/                  |           |
|    approx_kl            | 0.0172012 |
|    clip_fraction        | 0.189     |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.07     |
|    explained_variance   | 0.898     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0298   |
|    n_updates            | 350       |
|    policy_gradient_loss | -0.0311   |
|    value_loss           | 0.000833  |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.815    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 36       |
|    time_elapsed    | 2950     |
|    total_timesteps | 73728    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 74000      |
| train/                  |            |
|    approx_kl            | 0.01701075 |
|    clip_fraction        | 0.202      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.09      |
|    explained_variance   | 0.777      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.044     |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.0254    |
|    value_loss           | 0.000518   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 997      |
|    ep_rew_mean     | 0.814    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 37       |
|    time_elapsed    | 3031     |
|    total_timesteps | 75776    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 76000       |
| train/                  |             |
|    approx_kl            | 0.014235245 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.01       |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0272     |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.0222     |
|    value_loss           | 0.000354    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.821    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 38       |
|    time_elapsed    | 3113     |
|    total_timesteps | 77824    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 78000       |
| train/                  |             |
|    approx_kl            | 0.018110424 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.1        |
|    explained_variance   | 0.901       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0413     |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.0267     |
|    value_loss           | 0.000423    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.816    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 39       |
|    time_elapsed    | 3194     |
|    total_timesteps | 79872    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.018217051 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.01       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0506     |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.0258     |
|    value_loss           | 0.000279    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.816    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 40       |
|    time_elapsed    | 3276     |
|    total_timesteps | 81920    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 82000      |
| train/                  |            |
|    approx_kl            | 0.01792889 |
|    clip_fraction        | 0.174      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.06      |
|    explained_variance   | 0.93       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0359    |
|    n_updates            | 400        |
|    policy_gradient_loss | -0.0291    |
|    value_loss           | 0.000367   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.814    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 41       |
|    time_elapsed    | 3357     |
|    total_timesteps | 83968    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 84000       |
| train/                  |             |
|    approx_kl            | 0.016309435 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6          |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0484     |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.0262     |
|    value_loss           | 0.000224    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.822    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 42       |
|    time_elapsed    | 3465     |
|    total_timesteps | 86016    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 87000       |
| train/                  |             |
|    approx_kl            | 0.015705787 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.97       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0667     |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0206     |
|    value_loss           | 0.000808    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 996      |
|    ep_rew_mean     | 0.83     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 43       |
|    time_elapsed    | 3546     |
|    total_timesteps | 88064    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 89000       |
| train/                  |             |
|    approx_kl            | 0.016768515 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.89       |
|    explained_variance   | 0.777       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.039      |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.0259     |
|    value_loss           | 0.00117     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.836    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 44       |
|    time_elapsed    | 3628     |
|    total_timesteps | 90112    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 91000       |
| train/                  |             |
|    approx_kl            | 0.016252447 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.86       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0384     |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.0231     |
|    value_loss           | 0.000389    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.835    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 45       |
|    time_elapsed    | 3709     |
|    total_timesteps | 92160    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 93000       |
| train/                  |             |
|    approx_kl            | 0.016351258 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.8        |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0396     |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.0304     |
|    value_loss           | 0.000164    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.84     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 46       |
|    time_elapsed    | 3790     |
|    total_timesteps | 94208    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 95000       |
| train/                  |             |
|    approx_kl            | 0.016799893 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.94       |
|    explained_variance   | 0.867       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0314     |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.0265     |
|    value_loss           | 0.000696    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.839    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 47       |
|    time_elapsed    | 3872     |
|    total_timesteps | 96256    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 97000       |
| train/                  |             |
|    approx_kl            | 0.013235171 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.79       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | -7.25e-05   |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.0262     |
|    value_loss           | 0.000203    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.837    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 48       |
|    time_elapsed    | 3953     |
|    total_timesteps | 98304    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 99000       |
| train/                  |             |
|    approx_kl            | 0.016127761 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.79       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0416     |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.0228     |
|    value_loss           | 0.00023     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.834    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 49       |
|    time_elapsed    | 4034     |
|    total_timesteps | 100352   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 101000      |
| train/                  |             |
|    approx_kl            | 0.015753774 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.78       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00343    |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.0277     |
|    value_loss           | 0.000193    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.834    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 50       |
|    time_elapsed    | 4115     |
|    total_timesteps | 102400   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 103000      |
| train/                  |             |
|    approx_kl            | 0.016870452 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.73       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.038      |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.0252     |
|    value_loss           | 0.000128    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 995      |
|    ep_rew_mean     | 0.83     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 51       |
|    time_elapsed    | 4196     |
|    total_timesteps | 104448   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 105000      |
| train/                  |             |
|    approx_kl            | 0.021614883 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.67       |
|    explained_variance   | 0.907       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0123     |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.0241     |
|    value_loss           | 0.000789    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 993      |
|    ep_rew_mean     | 0.84     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 52       |
|    time_elapsed    | 4277     |
|    total_timesteps | 106496   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 107000      |
| train/                  |             |
|    approx_kl            | 0.013882105 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.64       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0245     |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.024      |
|    value_loss           | 0.000266    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 985      |
|    ep_rew_mean     | 0.849    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 53       |
|    time_elapsed    | 4358     |
|    total_timesteps | 108544   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 109000      |
| train/                  |             |
|    approx_kl            | 0.018157486 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.55       |
|    explained_variance   | 0.862       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0208     |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.0228     |
|    value_loss           | 0.00144     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 994      |
|    ep_rew_mean     | 0.854    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 54       |
|    time_elapsed    | 4440     |
|    total_timesteps | 110592   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 111000      |
| train/                  |             |
|    approx_kl            | 0.010362322 |
|    clip_fraction        | 0.0925      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.55       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00909    |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.022      |
|    value_loss           | 5.03e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 983      |
|    ep_rew_mean     | 0.854    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 55       |
|    time_elapsed    | 4522     |
|    total_timesteps | 112640   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.4        |
| time/                   |            |
|    total_timesteps      | 113000     |
| train/                  |            |
|    approx_kl            | 0.01606486 |
|    clip_fraction        | 0.194      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.54      |
|    explained_variance   | 0.886      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0263    |
|    n_updates            | 550        |
|    policy_gradient_loss | -0.0179    |
|    value_loss           | 0.0011     |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 983      |
|    ep_rew_mean     | 0.862    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 56       |
|    time_elapsed    | 4604     |
|    total_timesteps | 114688   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 115000      |
| train/                  |             |
|    approx_kl            | 0.022118172 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.56       |
|    explained_variance   | 0.778       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0371     |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.0213     |
|    value_loss           | 0.00175     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 994      |
|    ep_rew_mean     | 0.868    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 57       |
|    time_elapsed    | 4686     |
|    total_timesteps | 116736   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 117000      |
| train/                  |             |
|    approx_kl            | 0.012745602 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.55       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.04       |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.0181     |
|    value_loss           | 0.000108    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 987      |
|    ep_rew_mean     | 0.866    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 58       |
|    time_elapsed    | 4768     |
|    total_timesteps | 118784   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 119000      |
| train/                  |             |
|    approx_kl            | 0.016593933 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.57       |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00452    |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.0198     |
|    value_loss           | 0.000762    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 977      |
|    ep_rew_mean     | 0.893    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 59       |
|    time_elapsed    | 4850     |
|    total_timesteps | 120832   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 121000      |
| train/                  |             |
|    approx_kl            | 0.016828645 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.49       |
|    explained_variance   | 0.82        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0411     |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.00199     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 973      |
|    ep_rew_mean     | 0.904    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 60       |
|    time_elapsed    | 4932     |
|    total_timesteps | 122880   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 123000      |
| train/                  |             |
|    approx_kl            | 0.011753695 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.46       |
|    explained_variance   | 0.91        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0285     |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.0126     |
|    value_loss           | 0.000872    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 973      |
|    ep_rew_mean     | 0.924    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 61       |
|    time_elapsed    | 5014     |
|    total_timesteps | 124928   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 125000      |
| train/                  |             |
|    approx_kl            | 0.020255525 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.5        |
|    explained_variance   | 0.75        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0214     |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.0186     |
|    value_loss           | 0.00247     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 956      |
|    ep_rew_mean     | 0.943    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 62       |
|    time_elapsed    | 5096     |
|    total_timesteps | 126976   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 127000      |
| train/                  |             |
|    approx_kl            | 0.015940778 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.48       |
|    explained_variance   | 0.763       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0237     |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 0.00221     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 956      |
|    ep_rew_mean     | 0.956    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 63       |
|    time_elapsed    | 5205     |
|    total_timesteps | 129024   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 130000      |
| train/                  |             |
|    approx_kl            | 0.015850127 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.44       |
|    explained_variance   | 0.643       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000856   |
|    n_updates            | 630         |
|    policy_gradient_loss | -0.0145     |
|    value_loss           | 0.00394     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 949      |
|    ep_rew_mean     | 0.969    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 64       |
|    time_elapsed    | 5287     |
|    total_timesteps | 131072   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.5        |
| time/                   |            |
|    total_timesteps      | 132000     |
| train/                  |            |
|    approx_kl            | 0.01618214 |
|    clip_fraction        | 0.217      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.4       |
|    explained_variance   | 0.562      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0141    |
|    n_updates            | 640        |
|    policy_gradient_loss | -0.0206    |
|    value_loss           | 0.00418    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 939      |
|    ep_rew_mean     | 0.986    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 65       |
|    time_elapsed    | 5369     |
|    total_timesteps | 133120   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.6        |
| time/                   |            |
|    total_timesteps      | 134000     |
| train/                  |            |
|    approx_kl            | 0.02189966 |
|    clip_fraction        | 0.242      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.4       |
|    explained_variance   | 0.562      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0115     |
|    n_updates            | 650        |
|    policy_gradient_loss | -0.0187    |
|    value_loss           | 0.00374    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 887      |
|    ep_rew_mean     | 1.01     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 66       |
|    time_elapsed    | 5452     |
|    total_timesteps | 135168   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.6        |
| time/                   |            |
|    total_timesteps      | 136000     |
| train/                  |            |
|    approx_kl            | 0.01520269 |
|    clip_fraction        | 0.211      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.32      |
|    explained_variance   | 0.718      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0406    |
|    n_updates            | 660        |
|    policy_gradient_loss | -0.0178    |
|    value_loss           | 0.00307    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 838      |
|    ep_rew_mean     | 1.04     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 67       |
|    time_elapsed    | 5534     |
|    total_timesteps | 137216   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 138000      |
| train/                  |             |
|    approx_kl            | 0.015707934 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.28       |
|    explained_variance   | 0.746       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00186    |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.0145     |
|    value_loss           | 0.00296     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 802      |
|    ep_rew_mean     | 1.07     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 68       |
|    time_elapsed    | 5616     |
|    total_timesteps | 139264   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.6        |
| time/                   |            |
|    total_timesteps      | 140000     |
| train/                  |            |
|    approx_kl            | 0.01504693 |
|    clip_fraction        | 0.169      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.33      |
|    explained_variance   | 0.694      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0306    |
|    n_updates            | 680        |
|    policy_gradient_loss | -0.0135    |
|    value_loss           | 0.00294    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 793      |
|    ep_rew_mean     | 1.08     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 69       |
|    time_elapsed    | 5698     |
|    total_timesteps | 141312   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 142000      |
| train/                  |             |
|    approx_kl            | 0.020237152 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.37       |
|    explained_variance   | 0.661       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00128     |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 0.00276     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 786      |
|    ep_rew_mean     | 1.09     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 70       |
|    time_elapsed    | 5780     |
|    total_timesteps | 143360   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 144000      |
| train/                  |             |
|    approx_kl            | 0.015475809 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.42       |
|    explained_variance   | 0.646       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00138    |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 0.00284     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 748      |
|    ep_rew_mean     | 1.11     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 71       |
|    time_elapsed    | 5862     |
|    total_timesteps | 145408   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.6        |
| time/                   |            |
|    total_timesteps      | 146000     |
| train/                  |            |
|    approx_kl            | 0.01798778 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.3       |
|    explained_variance   | 0.738      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0188    |
|    n_updates            | 710        |
|    policy_gradient_loss | -0.014     |
|    value_loss           | 0.00297    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 734      |
|    ep_rew_mean     | 1.12     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 72       |
|    time_elapsed    | 5944     |
|    total_timesteps | 147456   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 148000      |
| train/                  |             |
|    approx_kl            | 0.022813251 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.35       |
|    explained_variance   | 0.78        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0246     |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 0.00202     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 734      |
|    ep_rew_mean     | 1.14     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 73       |
|    time_elapsed    | 6026     |
|    total_timesteps | 149504   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.016363256 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.42       |
|    explained_variance   | 0.593       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0072      |
|    n_updates            | 730         |
|    policy_gradient_loss | -0.00868    |
|    value_loss           | 0.00369     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 151000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 721      |
|    ep_rew_mean     | 1.16     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 74       |
|    time_elapsed    | 6108     |
|    total_timesteps | 151552   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 152000      |
| train/                  |             |
|    approx_kl            | 0.013089903 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.4        |
|    explained_variance   | 0.664       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0158     |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.00551    |
|    value_loss           | 0.00353     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 153000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 701      |
|    ep_rew_mean     | 1.18     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 75       |
|    time_elapsed    | 6190     |
|    total_timesteps | 153600   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 154000      |
| train/                  |             |
|    approx_kl            | 0.013031213 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.28       |
|    explained_variance   | 0.659       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0267     |
|    n_updates            | 750         |
|    policy_gradient_loss | -0.00795    |
|    value_loss           | 0.0033      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 676      |
|    ep_rew_mean     | 1.18     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 76       |
|    time_elapsed    | 6272     |
|    total_timesteps | 155648   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 156000      |
| train/                  |             |
|    approx_kl            | 0.012702174 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.41       |
|    explained_variance   | 0.751       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0272     |
|    n_updates            | 760         |
|    policy_gradient_loss | -0.0143     |
|    value_loss           | 0.0026      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 634      |
|    ep_rew_mean     | 1.19     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 77       |
|    time_elapsed    | 6354     |
|    total_timesteps | 157696   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 158000      |
| train/                  |             |
|    approx_kl            | 0.015194086 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.32       |
|    explained_variance   | 0.677       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0123     |
|    n_updates            | 770         |
|    policy_gradient_loss | -0.00991    |
|    value_loss           | 0.00411     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 159000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 626      |
|    ep_rew_mean     | 1.2      |
| time/              |          |
|    fps             | 24       |
|    iterations      | 78       |
|    time_elapsed    | 6436     |
|    total_timesteps | 159744   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 160000      |
| train/                  |             |
|    approx_kl            | 0.017003406 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.18       |
|    explained_variance   | 0.862       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0392     |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.024      |
|    value_loss           | 0.000974    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 161000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 605      |
|    ep_rew_mean     | 1.21     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 79       |
|    time_elapsed    | 6518     |
|    total_timesteps | 161792   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 162000      |
| train/                  |             |
|    approx_kl            | 0.013721787 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.31       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00483    |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 0.000206    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 163000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | 1.22     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 80       |
|    time_elapsed    | 6600     |
|    total_timesteps | 163840   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 164000      |
| train/                  |             |
|    approx_kl            | 0.012307784 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.36       |
|    explained_variance   | 0.685       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00987     |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.00679    |
|    value_loss           | 0.00215     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 165000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 588      |
|    ep_rew_mean     | 1.22     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 81       |
|    time_elapsed    | 6683     |
|    total_timesteps | 165888   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 166000      |
| train/                  |             |
|    approx_kl            | 0.012232472 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.35       |
|    explained_variance   | 0.713       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0119      |
|    n_updates            | 810         |
|    policy_gradient_loss | -0.00679    |
|    value_loss           | 0.00214     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 167000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 597      |
|    ep_rew_mean     | 1.23     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 82       |
|    time_elapsed    | 6767     |
|    total_timesteps | 167936   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 168000      |
| train/                  |             |
|    approx_kl            | 0.012790101 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.32       |
|    explained_variance   | 0.514       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00156     |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.00952    |
|    value_loss           | 0.00414     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 169000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 576      |
|    ep_rew_mean     | 1.23     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 83       |
|    time_elapsed    | 6850     |
|    total_timesteps | 169984   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.8        |
| time/                   |            |
|    total_timesteps      | 170000     |
| train/                  |            |
|    approx_kl            | 0.01308378 |
|    clip_fraction        | 0.211      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.22      |
|    explained_variance   | 0.773      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0034     |
|    n_updates            | 830        |
|    policy_gradient_loss | -0.00965   |
|    value_loss           | 0.00217    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 538      |
|    ep_rew_mean     | 1.24     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 84       |
|    time_elapsed    | 6964     |
|    total_timesteps | 172032   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 173000      |
| train/                  |             |
|    approx_kl            | 0.014283739 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.12       |
|    explained_variance   | 0.613       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0316     |
|    n_updates            | 840         |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 0.00553     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 533      |
|    ep_rew_mean     | 1.24     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 85       |
|    time_elapsed    | 7049     |
|    total_timesteps | 174080   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 175000      |
| train/                  |             |
|    approx_kl            | 0.011127843 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.15       |
|    explained_variance   | 0.58        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0215     |
|    n_updates            | 850         |
|    policy_gradient_loss | -0.00435    |
|    value_loss           | 0.00454     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 553      |
|    ep_rew_mean     | 1.25     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 86       |
|    time_elapsed    | 7134     |
|    total_timesteps | 176128   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 177000      |
| train/                  |             |
|    approx_kl            | 0.015890982 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.02       |
|    explained_variance   | 0.609       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000169    |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.00847    |
|    value_loss           | 0.00227     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 542      |
|    ep_rew_mean     | 1.24     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 87       |
|    time_elapsed    | 7218     |
|    total_timesteps | 178176   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 179000      |
| train/                  |             |
|    approx_kl            | 0.012396261 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.96       |
|    explained_variance   | 0.554       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0117      |
|    n_updates            | 870         |
|    policy_gradient_loss | -0.00442    |
|    value_loss           | 0.00452     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 545      |
|    ep_rew_mean     | 1.24     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 88       |
|    time_elapsed    | 7302     |
|    total_timesteps | 180224   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 181000      |
| train/                  |             |
|    approx_kl            | 0.016989835 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.99       |
|    explained_variance   | 0.66        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0239     |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.0099     |
|    value_loss           | 0.00228     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 1.22     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 89       |
|    time_elapsed    | 7387     |
|    total_timesteps | 182272   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.7        |
| time/                   |            |
|    total_timesteps      | 183000     |
| train/                  |            |
|    approx_kl            | 0.01078036 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.85      |
|    explained_variance   | 0.673      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0146     |
|    n_updates            | 890        |
|    policy_gradient_loss | -0.00635   |
|    value_loss           | 0.00432    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 526      |
|    ep_rew_mean     | 1.22     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 90       |
|    time_elapsed    | 7470     |
|    total_timesteps | 184320   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 185000      |
| train/                  |             |
|    approx_kl            | 0.013575515 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.91       |
|    explained_variance   | 0.619       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0111      |
|    n_updates            | 900         |
|    policy_gradient_loss | -0.00497    |
|    value_loss           | 0.00211     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 530      |
|    ep_rew_mean     | 1.21     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 91       |
|    time_elapsed    | 7554     |
|    total_timesteps | 186368   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 187000      |
| train/                  |             |
|    approx_kl            | 0.013133695 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.93       |
|    explained_variance   | 0.717       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0286      |
|    n_updates            | 910         |
|    policy_gradient_loss | -0.00283    |
|    value_loss           | 0.00216     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 536      |
|    ep_rew_mean     | 1.22     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 92       |
|    time_elapsed    | 7638     |
|    total_timesteps | 188416   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 189000      |
| train/                  |             |
|    approx_kl            | 0.012196616 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.8        |
|    explained_variance   | 0.678       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0182     |
|    n_updates            | 920         |
|    policy_gradient_loss | -0.00497    |
|    value_loss           | 0.00445     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 541      |
|    ep_rew_mean     | 1.22     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 93       |
|    time_elapsed    | 7714     |
|    total_timesteps | 190464   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 191000      |
| train/                  |             |
|    approx_kl            | 0.011649065 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.78       |
|    explained_variance   | 0.679       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0342     |
|    n_updates            | 930         |
|    policy_gradient_loss | -0.00907    |
|    value_loss           | 0.00412     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 541      |
|    ep_rew_mean     | 1.21     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 94       |
|    time_elapsed    | 7788     |
|    total_timesteps | 192512   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 193000      |
| train/                  |             |
|    approx_kl            | 0.018173475 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.79       |
|    explained_variance   | 0.725       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000367    |
|    n_updates            | 940         |
|    policy_gradient_loss | -0.00523    |
|    value_loss           | 0.00246     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 549      |
|    ep_rew_mean     | 1.22     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 95       |
|    time_elapsed    | 7863     |
|    total_timesteps | 194560   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 195000      |
| train/                  |             |
|    approx_kl            | 0.013607632 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.71       |
|    explained_variance   | 0.587       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0235      |
|    n_updates            | 950         |
|    policy_gradient_loss | -0.00411    |
|    value_loss           | 0.00446     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 549      |
|    ep_rew_mean     | 1.22     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 96       |
|    time_elapsed    | 7924     |
|    total_timesteps | 196608   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 197000      |
| train/                  |             |
|    approx_kl            | 0.011556499 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.71       |
|    explained_variance   | 0.682       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00351    |
|    n_updates            | 960         |
|    policy_gradient_loss | -0.00462    |
|    value_loss           | 0.00364     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 545      |
|    ep_rew_mean     | 1.21     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 97       |
|    time_elapsed    | 7978     |
|    total_timesteps | 198656   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 199000      |
| train/                  |             |
|    approx_kl            | 0.009906119 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.72       |
|    explained_variance   | 0.573       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0203     |
|    n_updates            | 970         |
|    policy_gradient_loss | -0.00375    |
|    value_loss           | 0.00469     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 560      |
|    ep_rew_mean     | 1.22     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 98       |
|    time_elapsed    | 8033     |
|    total_timesteps | 200704   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 201000      |
| train/                  |             |
|    approx_kl            | 0.012732292 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.8        |
|    explained_variance   | 0.658       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00318     |
|    n_updates            | 980         |
|    policy_gradient_loss | -0.00335    |
|    value_loss           | 0.0024      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 553      |
|    ep_rew_mean     | 1.21     |
| time/              |          |
|    fps             | 25       |
|    iterations      | 99       |
|    time_elapsed    | 8088     |
|    total_timesteps | 202752   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.8        |
| time/                   |            |
|    total_timesteps      | 203000     |
| train/                  |            |
|    approx_kl            | 0.01220483 |
|    clip_fraction        | 0.188      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.85      |
|    explained_variance   | 0.81       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0263    |
|    n_updates            | 990        |
|    policy_gradient_loss | -0.0113    |
|    value_loss           | 0.00145    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 536      |
|    ep_rew_mean     | 1.2      |
| time/              |          |
|    fps             | 25       |
|    iterations      | 100      |
|    time_elapsed    | 8143     |
|    total_timesteps | 204800   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 205000      |
| train/                  |             |
|    approx_kl            | 0.020154182 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.83       |
|    explained_variance   | 0.634       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000802    |
|    n_updates            | 1000        |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 0.00542     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 517      |
|    ep_rew_mean     | 1.19     |
| time/              |          |
|    fps             | 25       |
|    iterations      | 101      |
|    time_elapsed    | 8197     |
|    total_timesteps | 206848   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 207000      |
| train/                  |             |
|    approx_kl            | 0.013739407 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.77       |
|    explained_variance   | 0.625       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00719    |
|    n_updates            | 1010        |
|    policy_gradient_loss | -0.00713    |
|    value_loss           | 0.00475     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 530      |
|    ep_rew_mean     | 1.2      |
| time/              |          |
|    fps             | 25       |
|    iterations      | 102      |
|    time_elapsed    | 8252     |
|    total_timesteps | 208896   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 209000      |
| train/                  |             |
|    approx_kl            | 0.014570907 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.7        |
|    explained_variance   | 0.578       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0157     |
|    n_updates            | 1020        |
|    policy_gradient_loss | -0.00907    |
|    value_loss           | 0.00457     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 535      |
|    ep_rew_mean     | 1.2      |
| time/              |          |
|    fps             | 25       |
|    iterations      | 103      |
|    time_elapsed    | 8308     |
|    total_timesteps | 210944   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 211000      |
| train/                  |             |
|    approx_kl            | 0.013008699 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.62       |
|    explained_variance   | 0.601       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0458     |
|    n_updates            | 1030        |
|    policy_gradient_loss | -0.00398    |
|    value_loss           | 0.00443     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | 1.22     |
| time/              |          |
|    fps             | 25       |
|    iterations      | 104      |
|    time_elapsed    | 8363     |
|    total_timesteps | 212992   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 213000      |
| train/                  |             |
|    approx_kl            | 0.014237032 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.55       |
|    explained_variance   | 0.705       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0118     |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.00784    |
|    value_loss           | 0.00438     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 499      |
|    ep_rew_mean     | 1.21     |
| time/              |          |
|    fps             | 25       |
|    iterations      | 105      |
|    time_elapsed    | 8437     |
|    total_timesteps | 215040   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 216000      |
| train/                  |             |
|    approx_kl            | 0.012100108 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.836       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0405     |
|    n_updates            | 1050        |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 0.00176     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 400      |
|    ep_rew_mean     | 1.15     |
| time/              |          |
|    fps             | 25       |
|    iterations      | 106      |
|    time_elapsed    | 8491     |
|    total_timesteps | 217088   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 218000      |
| train/                  |             |
|    approx_kl            | 0.015328443 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 0.806       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.029      |
|    n_updates            | 1060        |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 0.0047      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 361      |
|    ep_rew_mean     | 1.14     |
| time/              |          |
|    fps             | 25       |
|    iterations      | 107      |
|    time_elapsed    | 8546     |
|    total_timesteps | 219136   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 220000      |
| train/                  |             |
|    approx_kl            | 0.011424804 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.775       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0124     |
|    n_updates            | 1070        |
|    policy_gradient_loss | -0.00526    |
|    value_loss           | 0.00452     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 331      |
|    ep_rew_mean     | 1.12     |
| time/              |          |
|    fps             | 25       |
|    iterations      | 108      |
|    time_elapsed    | 8600     |
|    total_timesteps | 221184   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.8        |
| time/                   |            |
|    total_timesteps      | 222000     |
| train/                  |            |
|    approx_kl            | 0.01149548 |
|    clip_fraction        | 0.151      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.44      |
|    explained_variance   | 0.81       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00617    |
|    n_updates            | 1080       |
|    policy_gradient_loss | -0.00504   |
|    value_loss           | 0.00437    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 1.11     |
| time/              |          |
|    fps             | 25       |
|    iterations      | 109      |
|    time_elapsed    | 8655     |
|    total_timesteps | 223232   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.8        |
| time/                   |            |
|    total_timesteps      | 224000     |
| train/                  |            |
|    approx_kl            | 0.01158466 |
|    clip_fraction        | 0.142      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.46      |
|    explained_variance   | 0.755      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.012     |
|    n_updates            | 1090       |
|    policy_gradient_loss | -0.00535   |
|    value_loss           | 0.00445    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 221      |
|    ep_rew_mean     | 1.06     |
| time/              |          |
|    fps             | 25       |
|    iterations      | 110      |
|    time_elapsed    | 8710     |
|    total_timesteps | 225280   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 226000      |
| train/                  |             |
|    approx_kl            | 0.008086044 |
|    clip_fraction        | 0.0965      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.858       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0236      |
|    n_updates            | 1100        |
|    policy_gradient_loss | -0.00677    |
|    value_loss           | 0.00377     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 168      |
|    ep_rew_mean     | 1.02     |
| time/              |          |
|    fps             | 25       |
|    iterations      | 111      |
|    time_elapsed    | 8765     |
|    total_timesteps | 227328   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 228000      |
| train/                  |             |
|    approx_kl            | 0.011526825 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.31       |
|    explained_variance   | 0.804       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0133      |
|    n_updates            | 1110        |
|    policy_gradient_loss | -0.00938    |
|    value_loss           | 0.0048      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 140      |
|    ep_rew_mean     | 1.01     |
| time/              |          |
|    fps             | 26       |
|    iterations      | 112      |
|    time_elapsed    | 8820     |
|    total_timesteps | 229376   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 230000      |
| train/                  |             |
|    approx_kl            | 0.010811313 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.29       |
|    explained_variance   | 0.799       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0345      |
|    n_updates            | 1120        |
|    policy_gradient_loss | -0.00702    |
|    value_loss           | 0.00463     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | 1.01     |
| time/              |          |
|    fps             | 26       |
|    iterations      | 113      |
|    time_elapsed    | 8875     |
|    total_timesteps | 231424   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 232000      |
| train/                  |             |
|    approx_kl            | 0.009146907 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.823       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0129      |
|    n_updates            | 1130        |
|    policy_gradient_loss | -0.00322    |
|    value_loss           | 0.00456     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 0.995    |
| time/              |          |
|    fps             | 26       |
|    iterations      | 114      |
|    time_elapsed    | 8930     |
|    total_timesteps | 233472   |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | 0.8          |
| time/                   |              |
|    total_timesteps      | 234000       |
| train/                  |              |
|    approx_kl            | 0.0077742753 |
|    clip_fraction        | 0.104        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.14        |
|    explained_variance   | 0.835        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000111    |
|    n_updates            | 1140         |
|    policy_gradient_loss | -0.00338     |
|    value_loss           | 0.00445      |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 0.979    |
| time/              |          |
|    fps             | 26       |
|    iterations      | 115      |
|    time_elapsed    | 8985     |
|    total_timesteps | 235520   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 236000      |
| train/                  |             |
|    approx_kl            | 0.017722469 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.862       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00159    |
|    n_updates            | 1150        |
|    policy_gradient_loss | -0.00882    |
|    value_loss           | 0.00268     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 237000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 0.976    |
| time/              |          |
|    fps             | 26       |
|    iterations      | 116      |
|    time_elapsed    | 9037     |
|    total_timesteps | 237568   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.8        |
| time/                   |            |
|    total_timesteps      | 238000     |
| train/                  |            |
|    approx_kl            | 0.01687774 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.27      |
|    explained_variance   | 0.889      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0255     |
|    n_updates            | 1160       |
|    policy_gradient_loss | -0.00984   |
|    value_loss           | 0.00245    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 239000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 101      |
|    ep_rew_mean     | 0.968    |
| time/              |          |
|    fps             | 26       |
|    iterations      | 117      |
|    time_elapsed    | 9084     |
|    total_timesteps | 239616   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 0.015697677 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0163     |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.00869    |
|    value_loss           | 0.00464     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 241000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.2     |
|    ep_rew_mean     | 0.96     |
| time/              |          |
|    fps             | 26       |
|    iterations      | 118      |
|    time_elapsed    | 9131     |
|    total_timesteps | 241664   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 242000      |
| train/                  |             |
|    approx_kl            | 0.010719968 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.81       |
|    explained_variance   | 0.867       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0162     |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.00664    |
|    value_loss           | 0.0047      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 243000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67.1     |
|    ep_rew_mean     | 0.956    |
| time/              |          |
|    fps             | 26       |
|    iterations      | 119      |
|    time_elapsed    | 9177     |
|    total_timesteps | 243712   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 244000      |
| train/                  |             |
|    approx_kl            | 0.012406422 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.68       |
|    explained_variance   | 0.892       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00117    |
|    n_updates            | 1190        |
|    policy_gradient_loss | -0.00929    |
|    value_loss           | 0.00429     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 58.7     |
|    ep_rew_mean     | 0.948    |
| time/              |          |
|    fps             | 26       |
|    iterations      | 120      |
|    time_elapsed    | 9224     |
|    total_timesteps | 245760   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 246000      |
| train/                  |             |
|    approx_kl            | 0.011865003 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.59       |
|    explained_variance   | 0.891       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00256    |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.0069     |
|    value_loss           | 0.0045      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 247000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.6     |
|    ep_rew_mean     | 0.932    |
| time/              |          |
|    fps             | 26       |
|    iterations      | 121      |
|    time_elapsed    | 9271     |
|    total_timesteps | 247808   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.8        |
| time/                   |            |
|    total_timesteps      | 248000     |
| train/                  |            |
|    approx_kl            | 0.02538526 |
|    clip_fraction        | 0.182      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.36      |
|    explained_variance   | 0.91       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00912   |
|    n_updates            | 1210       |
|    policy_gradient_loss | -0.0111    |
|    value_loss           | 0.0044     |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 43.2     |
|    ep_rew_mean     | 0.93     |
| time/              |          |
|    fps             | 26       |
|    iterations      | 122      |
|    time_elapsed    | 9317     |
|    total_timesteps | 249856   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.015995432 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.36       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0343     |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.00902    |
|    value_loss           | 0.00349     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 43.7     |
|    ep_rew_mean     | 0.93     |
| time/              |          |
|    fps             | 26       |
|    iterations      | 123      |
|    time_elapsed    | 9363     |
|    total_timesteps | 251904   |
---------------------------------
{'reward': [0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929], 'std': [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}
