Logging to ../Logging/PPO_Forward_Baseline_10_600
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.9      |
| time/              |          |
|    fps             | 26       |
|    iterations      | 1        |
|    time_elapsed    | 78       |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 3000        |
| train/                  |             |
|    approx_kl            | 0.015808385 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.54       |
|    explained_variance   | 0.8         |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00773    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0278     |
|    value_loss           | 0.0285      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.725    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 2        |
|    time_elapsed    | 160      |
|    total_timesteps | 4096     |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.017768053 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.53       |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0178      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0215     |
|    value_loss           | 0.00796     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 999      |
|    ep_rew_mean     | 0.733    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 3        |
|    time_elapsed    | 242      |
|    total_timesteps | 6144     |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 7000       |
| train/                  |            |
|    approx_kl            | 0.01786175 |
|    clip_fraction        | 0.15       |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.51      |
|    explained_variance   | 0.855      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00849   |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.026     |
|    value_loss           | 0.00753    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.75     |
| time/              |          |
|    fps             | 25       |
|    iterations      | 4        |
|    time_elapsed    | 325      |
|    total_timesteps | 8192     |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 9000        |
| train/                  |             |
|    approx_kl            | 0.016119935 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.5        |
|    explained_variance   | 0.852       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0447     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0294     |
|    value_loss           | 0.00788     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.8      |
| time/              |          |
|    fps             | 25       |
|    iterations      | 5        |
|    time_elapsed    | 406      |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 11000       |
| train/                  |             |
|    approx_kl            | 0.015179725 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.48       |
|    explained_variance   | 0.856       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0254      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0254     |
|    value_loss           | 0.00666     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.783    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 6        |
|    time_elapsed    | 487      |
|    total_timesteps | 12288    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 13000       |
| train/                  |             |
|    approx_kl            | 0.019350454 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.48       |
|    explained_variance   | 0.903       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.048      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0237     |
|    value_loss           | 0.00184     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.764    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 7        |
|    time_elapsed    | 568      |
|    total_timesteps | 14336    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.017188206 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.48       |
|    explained_variance   | 0.851       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0536     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0311     |
|    value_loss           | 0.00182     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.738    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 8        |
|    time_elapsed    | 649      |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 17000       |
| train/                  |             |
|    approx_kl            | 0.015207112 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.43       |
|    explained_variance   | 0.857       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0118     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0213     |
|    value_loss           | 0.000925    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.728    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 9        |
|    time_elapsed    | 730      |
|    total_timesteps | 18432    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 19000       |
| train/                  |             |
|    approx_kl            | 0.015107175 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.39       |
|    explained_variance   | 0.903       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0231     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0232     |
|    value_loss           | 0.0012      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.76     |
| time/              |          |
|    fps             | 25       |
|    iterations      | 10       |
|    time_elapsed    | 810      |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 21000       |
| train/                  |             |
|    approx_kl            | 0.016456854 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.43       |
|    explained_variance   | 0.836       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0508     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0286     |
|    value_loss           | 0.00177     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 987      |
|    ep_rew_mean     | 0.764    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 11       |
|    time_elapsed    | 891      |
|    total_timesteps | 22528    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 23000       |
| train/                  |             |
|    approx_kl            | 0.016679179 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.42       |
|    explained_variance   | 0.726       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0277     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0249     |
|    value_loss           | 0.0019      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.771    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 12       |
|    time_elapsed    | 973      |
|    total_timesteps | 24576    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.015932847 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.4        |
|    explained_variance   | 0.883       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0465     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0268     |
|    value_loss           | 0.000703    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.758    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 13       |
|    time_elapsed    | 1053     |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 27000      |
| train/                  |            |
|    approx_kl            | 0.02084776 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.41      |
|    explained_variance   | 0.892      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0647    |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0256    |
|    value_loss           | 0.00109    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.754    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 14       |
|    time_elapsed    | 1134     |
|    total_timesteps | 28672    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 29000       |
| train/                  |             |
|    approx_kl            | 0.020640181 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.38       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0283     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0277     |
|    value_loss           | 0.00121     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.747    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 15       |
|    time_elapsed    | 1215     |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 31000       |
| train/                  |             |
|    approx_kl            | 0.012668154 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.41       |
|    explained_variance   | 0.781       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0547     |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0244     |
|    value_loss           | 0.000979    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.747    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 16       |
|    time_elapsed    | 1297     |
|    total_timesteps | 32768    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 33000       |
| train/                  |             |
|    approx_kl            | 0.017762199 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.39       |
|    explained_variance   | 0.888       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0646     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0289     |
|    value_loss           | 0.000627    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.738    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 17       |
|    time_elapsed    | 1378     |
|    total_timesteps | 34816    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.019734345 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.35       |
|    explained_variance   | 0.821       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0451     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.027      |
|    value_loss           | 0.00345     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.728    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 18       |
|    time_elapsed    | 1459     |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 37000       |
| train/                  |             |
|    approx_kl            | 0.016632317 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.35       |
|    explained_variance   | 0.812       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0152      |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0233     |
|    value_loss           | 0.00101     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.724    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 19       |
|    time_elapsed    | 1540     |
|    total_timesteps | 38912    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 39000       |
| train/                  |             |
|    approx_kl            | 0.015544433 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.36       |
|    explained_variance   | 0.76        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0395     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0255     |
|    value_loss           | 0.00112     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.725    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 20       |
|    time_elapsed    | 1621     |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 41000       |
| train/                  |             |
|    approx_kl            | 0.016588952 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.33       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.067      |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0277     |
|    value_loss           | 0.00145     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.73     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 21       |
|    time_elapsed    | 1729     |
|    total_timesteps | 43008    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 44000      |
| train/                  |            |
|    approx_kl            | 0.01891042 |
|    clip_fraction        | 0.177      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.34      |
|    explained_variance   | 0.683      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0165    |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.0292    |
|    value_loss           | 0.000861   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.727    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 22       |
|    time_elapsed    | 1810     |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 46000       |
| train/                  |             |
|    approx_kl            | 0.016800549 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.32       |
|    explained_variance   | 0.836       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0288     |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0221     |
|    value_loss           | 0.00207     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.717    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 23       |
|    time_elapsed    | 1891     |
|    total_timesteps | 47104    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 48000       |
| train/                  |             |
|    approx_kl            | 0.020499919 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.31       |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.046      |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0275     |
|    value_loss           | 0.000789    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.708    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 24       |
|    time_elapsed    | 1972     |
|    total_timesteps | 49152    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.02003903 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.29      |
|    explained_variance   | 0.309      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0227    |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0263    |
|    value_loss           | 0.00146    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.708    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 25       |
|    time_elapsed    | 2053     |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 52000       |
| train/                  |             |
|    approx_kl            | 0.016622033 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.31       |
|    explained_variance   | 0.65        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.037      |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0232     |
|    value_loss           | 0.000818    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.706    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 26       |
|    time_elapsed    | 2134     |
|    total_timesteps | 53248    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 54000       |
| train/                  |             |
|    approx_kl            | 0.020192116 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.3        |
|    explained_variance   | 0.847       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0394     |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0301     |
|    value_loss           | 0.000703    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.715    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 27       |
|    time_elapsed    | 2215     |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 56000       |
| train/                  |             |
|    approx_kl            | 0.016889881 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.27       |
|    explained_variance   | 0.705       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0549     |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0233     |
|    value_loss           | 0.00431     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.714    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 28       |
|    time_elapsed    | 2296     |
|    total_timesteps | 57344    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 58000       |
| train/                  |             |
|    approx_kl            | 0.018401321 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.3        |
|    explained_variance   | 0.85        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0299     |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0247     |
|    value_loss           | 0.00238     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 993      |
|    ep_rew_mean     | 0.715    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 29       |
|    time_elapsed    | 2377     |
|    total_timesteps | 59392    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.020781405 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.29       |
|    explained_variance   | 0.649       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0323     |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0296     |
|    value_loss           | 0.0015      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.726    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 30       |
|    time_elapsed    | 2458     |
|    total_timesteps | 61440    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 62000       |
| train/                  |             |
|    approx_kl            | 0.015725046 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.27       |
|    explained_variance   | 0.865       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0274     |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.0234     |
|    value_loss           | 0.000885    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.724    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 31       |
|    time_elapsed    | 2539     |
|    total_timesteps | 63488    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 64000       |
| train/                  |             |
|    approx_kl            | 0.015275988 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.3        |
|    explained_variance   | 0.797       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0261     |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.0248     |
|    value_loss           | 0.00189     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 998      |
|    ep_rew_mean     | 0.728    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 32       |
|    time_elapsed    | 2620     |
|    total_timesteps | 65536    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 66000       |
| train/                  |             |
|    approx_kl            | 0.019416207 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.22       |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.015      |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0287     |
|    value_loss           | 0.000583    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.73     |
| time/              |          |
|    fps             | 25       |
|    iterations      | 33       |
|    time_elapsed    | 2701     |
|    total_timesteps | 67584    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 68000       |
| train/                  |             |
|    approx_kl            | 0.016514078 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.24       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0214     |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.0204     |
|    value_loss           | 0.000689    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.726    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 34       |
|    time_elapsed    | 2782     |
|    total_timesteps | 69632    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.020942176 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.19       |
|    explained_variance   | 0.815       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0401     |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.0256     |
|    value_loss           | 0.000583    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.724    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 35       |
|    time_elapsed    | 2863     |
|    total_timesteps | 71680    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 72000       |
| train/                  |             |
|    approx_kl            | 0.020065501 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.2        |
|    explained_variance   | 0.892       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0522     |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.032      |
|    value_loss           | 0.000476    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.721    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 36       |
|    time_elapsed    | 2944     |
|    total_timesteps | 73728    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 74000       |
| train/                  |             |
|    approx_kl            | 0.016808504 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.17       |
|    explained_variance   | 0.904       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0267     |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.024      |
|    value_loss           | 0.000384    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.721    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 37       |
|    time_elapsed    | 3024     |
|    total_timesteps | 75776    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 76000       |
| train/                  |             |
|    approx_kl            | 0.017093439 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.16       |
|    explained_variance   | 0.767       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0326     |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.0234     |
|    value_loss           | 0.000605    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.722    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 38       |
|    time_elapsed    | 3105     |
|    total_timesteps | 77824    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 78000       |
| train/                  |             |
|    approx_kl            | 0.015234854 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.2        |
|    explained_variance   | 0.85        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0559     |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.0199     |
|    value_loss           | 0.000438    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.722    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 39       |
|    time_elapsed    | 3186     |
|    total_timesteps | 79872    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.014884169 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.12       |
|    explained_variance   | 0.867       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0379     |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.0221     |
|    value_loss           | 0.000358    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 996      |
|    ep_rew_mean     | 0.722    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 40       |
|    time_elapsed    | 3267     |
|    total_timesteps | 81920    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 82000       |
| train/                  |             |
|    approx_kl            | 0.018920956 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.09       |
|    explained_variance   | 0.814       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0273     |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.0267     |
|    value_loss           | 0.000781    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.729    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 41       |
|    time_elapsed    | 3348     |
|    total_timesteps | 83968    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 84000       |
| train/                  |             |
|    approx_kl            | 0.013742177 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.18       |
|    explained_variance   | 0.742       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0454     |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.0225     |
|    value_loss           | 0.000664    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.729    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 42       |
|    time_elapsed    | 3456     |
|    total_timesteps | 86016    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 87000       |
| train/                  |             |
|    approx_kl            | 0.017839614 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.13       |
|    explained_variance   | 0.797       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0127     |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0194     |
|    value_loss           | 0.000687    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 998      |
|    ep_rew_mean     | 0.732    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 43       |
|    time_elapsed    | 3537     |
|    total_timesteps | 88064    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 89000       |
| train/                  |             |
|    approx_kl            | 0.018174022 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.13       |
|    explained_variance   | 0.857       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0377     |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.0253     |
|    value_loss           | 0.000775    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.732    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 44       |
|    time_elapsed    | 3618     |
|    total_timesteps | 90112    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 91000       |
| train/                  |             |
|    approx_kl            | 0.022892889 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.09       |
|    explained_variance   | 0.855       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.04       |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.0316     |
|    value_loss           | 0.000491    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.734    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 45       |
|    time_elapsed    | 3699     |
|    total_timesteps | 92160    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 93000      |
| train/                  |            |
|    approx_kl            | 0.01480849 |
|    clip_fraction        | 0.17       |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.04      |
|    explained_variance   | 0.818      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0291    |
|    n_updates            | 450        |
|    policy_gradient_loss | -0.0227    |
|    value_loss           | 0.000553   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.744    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 46       |
|    time_elapsed    | 3780     |
|    total_timesteps | 94208    |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 95000      |
| train/                  |            |
|    approx_kl            | 0.01698803 |
|    clip_fraction        | 0.192      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.02      |
|    explained_variance   | 0.705      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0259    |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.0278    |
|    value_loss           | 0.000774   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.745    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 47       |
|    time_elapsed    | 3861     |
|    total_timesteps | 96256    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 97000       |
| train/                  |             |
|    approx_kl            | 0.023952529 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.14       |
|    explained_variance   | 0.786       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0563     |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.0245     |
|    value_loss           | 0.000392    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.752    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 48       |
|    time_elapsed    | 3942     |
|    total_timesteps | 98304    |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 99000       |
| train/                  |             |
|    approx_kl            | 0.016298704 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6          |
|    explained_variance   | 0.804       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0387     |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.0278     |
|    value_loss           | 0.00114     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.753    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 49       |
|    time_elapsed    | 4023     |
|    total_timesteps | 100352   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 101000     |
| train/                  |            |
|    approx_kl            | 0.01678304 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.06      |
|    explained_variance   | 0.503      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0334    |
|    n_updates            | 490        |
|    policy_gradient_loss | -0.0226    |
|    value_loss           | 0.000945   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.755    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 50       |
|    time_elapsed    | 4103     |
|    total_timesteps | 102400   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 103000     |
| train/                  |            |
|    approx_kl            | 0.02075494 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.14      |
|    explained_variance   | 0.763      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0654    |
|    n_updates            | 500        |
|    policy_gradient_loss | -0.0246    |
|    value_loss           | 0.000813   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.76     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 51       |
|    time_elapsed    | 4184     |
|    total_timesteps | 104448   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 105000     |
| train/                  |            |
|    approx_kl            | 0.01929082 |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.1       |
|    explained_variance   | 0.911      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.000312   |
|    n_updates            | 510        |
|    policy_gradient_loss | -0.0182    |
|    value_loss           | 0.000275   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 990      |
|    ep_rew_mean     | 0.763    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 52       |
|    time_elapsed    | 4265     |
|    total_timesteps | 106496   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 107000     |
| train/                  |            |
|    approx_kl            | 0.01838252 |
|    clip_fraction        | 0.208      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.99      |
|    explained_variance   | 0.802      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0527    |
|    n_updates            | 520        |
|    policy_gradient_loss | -0.0285    |
|    value_loss           | 0.00116    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 988      |
|    ep_rew_mean     | 0.767    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 53       |
|    time_elapsed    | 4346     |
|    total_timesteps | 108544   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 109000      |
| train/                  |             |
|    approx_kl            | 0.014939503 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.99       |
|    explained_variance   | 0.777       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0165     |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.0293     |
|    value_loss           | 0.000711    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 983      |
|    ep_rew_mean     | 0.773    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 54       |
|    time_elapsed    | 4426     |
|    total_timesteps | 110592   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 111000      |
| train/                  |             |
|    approx_kl            | 0.017207166 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.01       |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0626     |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.0306     |
|    value_loss           | 0.000541    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 980      |
|    ep_rew_mean     | 0.779    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 55       |
|    time_elapsed    | 4507     |
|    total_timesteps | 112640   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 113000      |
| train/                  |             |
|    approx_kl            | 0.018362924 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.01       |
|    explained_variance   | 0.902       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.045      |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.0313     |
|    value_loss           | 0.000619    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 980      |
|    ep_rew_mean     | 0.784    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 56       |
|    time_elapsed    | 4589     |
|    total_timesteps | 114688   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.1        |
| time/                   |            |
|    total_timesteps      | 115000     |
| train/                  |            |
|    approx_kl            | 0.01946298 |
|    clip_fraction        | 0.222      |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.07      |
|    explained_variance   | 0.915      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0212    |
|    n_updates            | 560        |
|    policy_gradient_loss | -0.0226    |
|    value_loss           | 0.000311   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 980      |
|    ep_rew_mean     | 0.787    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 57       |
|    time_elapsed    | 4670     |
|    total_timesteps | 116736   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 117000      |
| train/                  |             |
|    approx_kl            | 0.017911874 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.05       |
|    explained_variance   | 0.899       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0446     |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.0272     |
|    value_loss           | 0.000384    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 966      |
|    ep_rew_mean     | 0.791    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 58       |
|    time_elapsed    | 4751     |
|    total_timesteps | 118784   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 119000      |
| train/                  |             |
|    approx_kl            | 0.016351145 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.85       |
|    explained_variance   | 0.78        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0429     |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.0257     |
|    value_loss           | 0.00111     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 970      |
|    ep_rew_mean     | 0.796    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 59       |
|    time_elapsed    | 4832     |
|    total_timesteps | 120832   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 121000     |
| train/                  |            |
|    approx_kl            | 0.01736983 |
|    clip_fraction        | 0.227      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.97      |
|    explained_variance   | 0.84       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00785    |
|    n_updates            | 590        |
|    policy_gradient_loss | -0.019     |
|    value_loss           | 0.000845   |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 964      |
|    ep_rew_mean     | 0.81     |
| time/              |          |
|    fps             | 25       |
|    iterations      | 60       |
|    time_elapsed    | 4914     |
|    total_timesteps | 122880   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 123000      |
| train/                  |             |
|    approx_kl            | 0.016226161 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.95       |
|    explained_variance   | 0.774       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0281     |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.0231     |
|    value_loss           | 0.00102     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 964      |
|    ep_rew_mean     | 0.827    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 61       |
|    time_elapsed    | 4994     |
|    total_timesteps | 124928   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 125000      |
| train/                  |             |
|    approx_kl            | 0.014451257 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.93       |
|    explained_variance   | 0.772       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0507     |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.0197     |
|    value_loss           | 0.00121     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 966      |
|    ep_rew_mean     | 0.843    |
| time/              |          |
|    fps             | 25       |
|    iterations      | 62       |
|    time_elapsed    | 5076     |
|    total_timesteps | 126976   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 127000      |
| train/                  |             |
|    approx_kl            | 0.014294859 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.8        |
|    explained_variance   | 0.743       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0448     |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.0253     |
|    value_loss           | 0.000833    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 970      |
|    ep_rew_mean     | 0.859    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 63       |
|    time_elapsed    | 5184     |
|    total_timesteps | 129024   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 130000      |
| train/                  |             |
|    approx_kl            | 0.016325254 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.75       |
|    explained_variance   | 0.597       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0393     |
|    n_updates            | 630         |
|    policy_gradient_loss | -0.0236     |
|    value_loss           | 0.000867    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 970      |
|    ep_rew_mean     | 0.861    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 64       |
|    time_elapsed    | 5266     |
|    total_timesteps | 131072   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 132000     |
| train/                  |            |
|    approx_kl            | 0.01924501 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.9       |
|    explained_variance   | 0.776      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0315    |
|    n_updates            | 640        |
|    policy_gradient_loss | -0.0253    |
|    value_loss           | 0.00044    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 968      |
|    ep_rew_mean     | 0.875    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 65       |
|    time_elapsed    | 5346     |
|    total_timesteps | 133120   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 134000     |
| train/                  |            |
|    approx_kl            | 0.01708367 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.6       |
|    explained_variance   | 0.662      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.034     |
|    n_updates            | 650        |
|    policy_gradient_loss | -0.0225    |
|    value_loss           | 0.0013     |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 970      |
|    ep_rew_mean     | 0.895    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 66       |
|    time_elapsed    | 5428     |
|    total_timesteps | 135168   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 136000      |
| train/                  |             |
|    approx_kl            | 0.022914305 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.72       |
|    explained_variance   | 0.736       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0442     |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.0273     |
|    value_loss           | 0.00106     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 964      |
|    ep_rew_mean     | 0.897    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 67       |
|    time_elapsed    | 5509     |
|    total_timesteps | 137216   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 138000      |
| train/                  |             |
|    approx_kl            | 0.020233355 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.79       |
|    explained_variance   | 0.819       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0298     |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.0261     |
|    value_loss           | 0.00091     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 949      |
|    ep_rew_mean     | 0.918    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 68       |
|    time_elapsed    | 5590     |
|    total_timesteps | 139264   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 140000      |
| train/                  |             |
|    approx_kl            | 0.018901251 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.46       |
|    explained_variance   | 0.746       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0435     |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.0255     |
|    value_loss           | 0.00162     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 942      |
|    ep_rew_mean     | 0.941    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 69       |
|    time_elapsed    | 5672     |
|    total_timesteps | 141312   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 142000      |
| train/                  |             |
|    approx_kl            | 0.016484383 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.45       |
|    explained_variance   | 0.812       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0552     |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.0208     |
|    value_loss           | 0.00109     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 945      |
|    ep_rew_mean     | 0.954    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 70       |
|    time_elapsed    | 5753     |
|    total_timesteps | 143360   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 144000     |
| train/                  |            |
|    approx_kl            | 0.01675466 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.35      |
|    explained_variance   | 0.774      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0245    |
|    n_updates            | 700        |
|    policy_gradient_loss | -0.0168    |
|    value_loss           | 0.00203    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 942      |
|    ep_rew_mean     | 0.976    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 71       |
|    time_elapsed    | 5835     |
|    total_timesteps | 145408   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 146000      |
| train/                  |             |
|    approx_kl            | 0.017809467 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.26       |
|    explained_variance   | 0.871       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0317     |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.0187     |
|    value_loss           | 0.00126     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 942      |
|    ep_rew_mean     | 0.987    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 72       |
|    time_elapsed    | 5916     |
|    total_timesteps | 147456   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 148000      |
| train/                  |             |
|    approx_kl            | 0.015210807 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.19       |
|    explained_variance   | 0.848       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0339     |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.02       |
|    value_loss           | 0.00112     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 937      |
|    ep_rew_mean     | 0.994    |
| time/              |          |
|    fps             | 24       |
|    iterations      | 73       |
|    time_elapsed    | 5998     |
|    total_timesteps | 149504   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.017557833 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.09       |
|    explained_variance   | 0.826       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.031      |
|    n_updates            | 730         |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.00184     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 151000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 932      |
|    ep_rew_mean     | 1        |
| time/              |          |
|    fps             | 24       |
|    iterations      | 74       |
|    time_elapsed    | 6079     |
|    total_timesteps | 151552   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 152000      |
| train/                  |             |
|    approx_kl            | 0.012856359 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.03       |
|    explained_variance   | 0.823       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00103    |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.0136     |
|    value_loss           | 0.00108     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 153000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 935      |
|    ep_rew_mean     | 1.01     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 75       |
|    time_elapsed    | 6161     |
|    total_timesteps | 153600   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 154000      |
| train/                  |             |
|    approx_kl            | 0.016908582 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5          |
|    explained_variance   | 0.85        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0297     |
|    n_updates            | 750         |
|    policy_gradient_loss | -0.0154     |
|    value_loss           | 0.00128     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 934      |
|    ep_rew_mean     | 1.03     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 76       |
|    time_elapsed    | 6242     |
|    total_timesteps | 155648   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 156000      |
| train/                  |             |
|    approx_kl            | 0.015689436 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.97       |
|    explained_variance   | 0.862       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0149     |
|    n_updates            | 760         |
|    policy_gradient_loss | -0.0137     |
|    value_loss           | 0.00149     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 928      |
|    ep_rew_mean     | 1.05     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 77       |
|    time_elapsed    | 6324     |
|    total_timesteps | 157696   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 158000      |
| train/                  |             |
|    approx_kl            | 0.014190203 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.85       |
|    explained_variance   | 0.804       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0163     |
|    n_updates            | 770         |
|    policy_gradient_loss | -0.0087     |
|    value_loss           | 0.00302     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 159000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 921      |
|    ep_rew_mean     | 1.07     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 78       |
|    time_elapsed    | 6406     |
|    total_timesteps | 159744   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 160000      |
| train/                  |             |
|    approx_kl            | 0.014934099 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.84       |
|    explained_variance   | 0.796       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0135     |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.00947    |
|    value_loss           | 0.00313     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 161000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 898      |
|    ep_rew_mean     | 1.09     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 79       |
|    time_elapsed    | 6488     |
|    total_timesteps | 161792   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 162000      |
| train/                  |             |
|    approx_kl            | 0.012707885 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.75       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0121     |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.00989    |
|    value_loss           | 0.00159     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 163000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 877      |
|    ep_rew_mean     | 1.1      |
| time/              |          |
|    fps             | 24       |
|    iterations      | 80       |
|    time_elapsed    | 6570     |
|    total_timesteps | 163840   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 164000      |
| train/                  |             |
|    approx_kl            | 0.018917048 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.69       |
|    explained_variance   | 0.681       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00339    |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.00873    |
|    value_loss           | 0.00372     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 165000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 868      |
|    ep_rew_mean     | 1.11     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 81       |
|    time_elapsed    | 6653     |
|    total_timesteps | 165888   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 166000      |
| train/                  |             |
|    approx_kl            | 0.012947457 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.797       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00323    |
|    n_updates            | 810         |
|    policy_gradient_loss | -0.00765    |
|    value_loss           | 0.00181     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 167000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 861      |
|    ep_rew_mean     | 1.14     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 82       |
|    time_elapsed    | 6736     |
|    total_timesteps | 167936   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 168000      |
| train/                  |             |
|    approx_kl            | 0.018510133 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.771       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0282      |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.00855    |
|    value_loss           | 0.00207     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 169000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 839      |
|    ep_rew_mean     | 1.15     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 83       |
|    time_elapsed    | 6820     |
|    total_timesteps | 169984   |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | 0.8          |
| time/                   |              |
|    total_timesteps      | 170000       |
| train/                  |              |
|    approx_kl            | 0.0101440875 |
|    clip_fraction        | 0.14         |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.33        |
|    explained_variance   | 0.733        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0116       |
|    n_updates            | 830          |
|    policy_gradient_loss | -0.00405     |
|    value_loss           | 0.0043       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 810      |
|    ep_rew_mean     | 1.17     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 84       |
|    time_elapsed    | 6933     |
|    total_timesteps | 172032   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.8        |
| time/                   |            |
|    total_timesteps      | 173000     |
| train/                  |            |
|    approx_kl            | 0.01602189 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.23      |
|    explained_variance   | 0.667      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0122    |
|    n_updates            | 840        |
|    policy_gradient_loss | -0.00918   |
|    value_loss           | 0.00617    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 778      |
|    ep_rew_mean     | 1.19     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 85       |
|    time_elapsed    | 7018     |
|    total_timesteps | 174080   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 175000      |
| train/                  |             |
|    approx_kl            | 0.013090686 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.688       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0301      |
|    n_updates            | 850         |
|    policy_gradient_loss | -0.00436    |
|    value_loss           | 0.00433     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 738      |
|    ep_rew_mean     | 1.2      |
| time/              |          |
|    fps             | 24       |
|    iterations      | 86       |
|    time_elapsed    | 7102     |
|    total_timesteps | 176128   |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | 0.8          |
| time/                   |              |
|    total_timesteps      | 177000       |
| train/                  |              |
|    approx_kl            | 0.0122647295 |
|    clip_fraction        | 0.201        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.97        |
|    explained_variance   | 0.716        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0152      |
|    n_updates            | 860          |
|    policy_gradient_loss | -0.00678     |
|    value_loss           | 0.00419      |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 723      |
|    ep_rew_mean     | 1.21     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 87       |
|    time_elapsed    | 7187     |
|    total_timesteps | 178176   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 179000      |
| train/                  |             |
|    approx_kl            | 0.019062497 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.05       |
|    explained_variance   | 0.739       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0145     |
|    n_updates            | 870         |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 0.00239     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 724      |
|    ep_rew_mean     | 1.23     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 88       |
|    time_elapsed    | 7271     |
|    total_timesteps | 180224   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 181000      |
| train/                  |             |
|    approx_kl            | 0.012250472 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.607       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00575     |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.00574    |
|    value_loss           | 0.00457     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 720      |
|    ep_rew_mean     | 1.24     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 89       |
|    time_elapsed    | 7356     |
|    total_timesteps | 182272   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.8        |
| time/                   |            |
|    total_timesteps      | 183000     |
| train/                  |            |
|    approx_kl            | 0.01064156 |
|    clip_fraction        | 0.139      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.91      |
|    explained_variance   | 0.581      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0048     |
|    n_updates            | 890        |
|    policy_gradient_loss | -0.00446   |
|    value_loss           | 0.00428    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 721      |
|    ep_rew_mean     | 1.25     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 90       |
|    time_elapsed    | 7440     |
|    total_timesteps | 184320   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 185000      |
| train/                  |             |
|    approx_kl            | 0.015235757 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.9        |
|    explained_variance   | 0.719       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0122     |
|    n_updates            | 900         |
|    policy_gradient_loss | -0.00759    |
|    value_loss           | 0.00226     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 687      |
|    ep_rew_mean     | 1.26     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 91       |
|    time_elapsed    | 7523     |
|    total_timesteps | 186368   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 187000      |
| train/                  |             |
|    approx_kl            | 0.013140192 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.79       |
|    explained_variance   | 0.624       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0262     |
|    n_updates            | 910         |
|    policy_gradient_loss | -0.00203    |
|    value_loss           | 0.00458     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 660      |
|    ep_rew_mean     | 1.27     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 92       |
|    time_elapsed    | 7607     |
|    total_timesteps | 188416   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 189000      |
| train/                  |             |
|    approx_kl            | 0.007455458 |
|    clip_fraction        | 0.0945      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.71       |
|    explained_variance   | 0.67        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0147     |
|    n_updates            | 920         |
|    policy_gradient_loss | -0.00185    |
|    value_loss           | 0.00469     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 589      |
|    ep_rew_mean     | 1.25     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 93       |
|    time_elapsed    | 7687     |
|    total_timesteps | 190464   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.8        |
| time/                   |            |
|    total_timesteps      | 191000     |
| train/                  |            |
|    approx_kl            | 0.01069659 |
|    clip_fraction        | 0.144      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.64      |
|    explained_variance   | 0.727      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0137     |
|    n_updates            | 930        |
|    policy_gradient_loss | -0.00335   |
|    value_loss           | 0.00458    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 548      |
|    ep_rew_mean     | 1.24     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 94       |
|    time_elapsed    | 7761     |
|    total_timesteps | 192512   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 193000      |
| train/                  |             |
|    approx_kl            | 0.008680279 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.64       |
|    explained_variance   | 0.669       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0116      |
|    n_updates            | 940         |
|    policy_gradient_loss | -0.00477    |
|    value_loss           | 0.00465     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 502      |
|    ep_rew_mean     | 1.22     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 95       |
|    time_elapsed    | 7836     |
|    total_timesteps | 194560   |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | 0.8          |
| time/                   |              |
|    total_timesteps      | 195000       |
| train/                  |              |
|    approx_kl            | 0.0096217245 |
|    clip_fraction        | 0.119        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.59        |
|    explained_variance   | 0.704        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00459     |
|    n_updates            | 950          |
|    policy_gradient_loss | -0.00457     |
|    value_loss           | 0.0048       |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 446      |
|    ep_rew_mean     | 1.2      |
| time/              |          |
|    fps             | 24       |
|    iterations      | 96       |
|    time_elapsed    | 7904     |
|    total_timesteps | 196608   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 197000      |
| train/                  |             |
|    approx_kl            | 0.009411775 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.56       |
|    explained_variance   | 0.745       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0233     |
|    n_updates            | 960         |
|    policy_gradient_loss | -0.00623    |
|    value_loss           | 0.00446     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 422      |
|    ep_rew_mean     | 1.19     |
| time/              |          |
|    fps             | 24       |
|    iterations      | 97       |
|    time_elapsed    | 7959     |
|    total_timesteps | 198656   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 199000      |
| train/                  |             |
|    approx_kl            | 0.013305876 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.5        |
|    explained_variance   | 0.617       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0362     |
|    n_updates            | 970         |
|    policy_gradient_loss | -0.00459    |
|    value_loss           | 0.0048      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 388      |
|    ep_rew_mean     | 1.17     |
| time/              |          |
|    fps             | 25       |
|    iterations      | 98       |
|    time_elapsed    | 8013     |
|    total_timesteps | 200704   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 201000      |
| train/                  |             |
|    approx_kl            | 0.010465275 |
|    clip_fraction        | 0.0996      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.46       |
|    explained_variance   | 0.778       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000348   |
|    n_updates            | 980         |
|    policy_gradient_loss | -0.00702    |
|    value_loss           | 0.00252     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 342      |
|    ep_rew_mean     | 1.15     |
| time/              |          |
|    fps             | 25       |
|    iterations      | 99       |
|    time_elapsed    | 8068     |
|    total_timesteps | 202752   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 203000      |
| train/                  |             |
|    approx_kl            | 0.009758748 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.33       |
|    explained_variance   | 0.765       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00356     |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.00383    |
|    value_loss           | 0.00467     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 317      |
|    ep_rew_mean     | 1.14     |
| time/              |          |
|    fps             | 25       |
|    iterations      | 100      |
|    time_elapsed    | 8122     |
|    total_timesteps | 204800   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 205000      |
| train/                  |             |
|    approx_kl            | 0.010795736 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.3        |
|    explained_variance   | 0.714       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.011      |
|    n_updates            | 1000        |
|    policy_gradient_loss | -0.00239    |
|    value_loss           | 0.00473     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 324      |
|    ep_rew_mean     | 1.13     |
| time/              |          |
|    fps             | 25       |
|    iterations      | 101      |
|    time_elapsed    | 8176     |
|    total_timesteps | 206848   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 207000      |
| train/                  |             |
|    approx_kl            | 0.012590107 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.25       |
|    explained_variance   | 0.726       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0224     |
|    n_updates            | 1010        |
|    policy_gradient_loss | -0.0054     |
|    value_loss           | 0.00252     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 291      |
|    ep_rew_mean     | 1.11     |
| time/              |          |
|    fps             | 25       |
|    iterations      | 102      |
|    time_elapsed    | 8232     |
|    total_timesteps | 208896   |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | 0.8          |
| time/                   |              |
|    total_timesteps      | 209000       |
| train/                  |              |
|    approx_kl            | 0.0071311314 |
|    clip_fraction        | 0.08         |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.13        |
|    explained_variance   | 0.711        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00188      |
|    n_updates            | 1020         |
|    policy_gradient_loss | -0.00103     |
|    value_loss           | 0.00475      |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 249      |
|    ep_rew_mean     | 1.09     |
| time/              |          |
|    fps             | 25       |
|    iterations      | 103      |
|    time_elapsed    | 8287     |
|    total_timesteps | 210944   |
---------------------------------
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | 0.8        |
| time/                   |            |
|    total_timesteps      | 211000     |
| train/                  |            |
|    approx_kl            | 0.00742409 |
|    clip_fraction        | 0.0801     |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.05      |
|    explained_variance   | 0.729      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0212     |
|    n_updates            | 1030       |
|    policy_gradient_loss | -0.0024    |
|    value_loss           | 0.00487    |
----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 243      |
|    ep_rew_mean     | 1.08     |
| time/              |          |
|    fps             | 25       |
|    iterations      | 104      |
|    time_elapsed    | 8343     |
|    total_timesteps | 212992   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 213000      |
| train/                  |             |
|    approx_kl            | 0.009350443 |
|    clip_fraction        | 0.0986      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.01       |
|    explained_variance   | 0.672       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000621    |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.00335    |
|    value_loss           | 0.00464     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 239      |
|    ep_rew_mean     | 1.07     |
| time/              |          |
|    fps             | 25       |
|    iterations      | 105      |
|    time_elapsed    | 8417     |
|    total_timesteps | 215040   |
---------------------------------
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | 0.8          |
| time/                   |              |
|    total_timesteps      | 216000       |
| train/                  |              |
|    approx_kl            | 0.0092956675 |
|    clip_fraction        | 0.0832       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.92        |
|    explained_variance   | 0.677        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00454      |
|    n_updates            | 1050         |
|    policy_gradient_loss | -0.00126     |
|    value_loss           | 0.00615      |
------------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 235      |
|    ep_rew_mean     | 1.07     |
| time/              |          |
|    fps             | 25       |
|    iterations      | 106      |
|    time_elapsed    | 8472     |
|    total_timesteps | 217088   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 218000      |
| train/                  |             |
|    approx_kl            | 0.008219369 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.85       |
|    explained_variance   | 0.666       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0214     |
|    n_updates            | 1060        |
|    policy_gradient_loss | -0.00182    |
|    value_loss           | 0.0048      |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 240      |
|    ep_rew_mean     | 1.08     |
| time/              |          |
|    fps             | 25       |
|    iterations      | 107      |
|    time_elapsed    | 8527     |
|    total_timesteps | 219136   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 220000      |
| train/                  |             |
|    approx_kl            | 0.017190766 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.79       |
|    explained_variance   | 0.635       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0236     |
|    n_updates            | 1070        |
|    policy_gradient_loss | -0.00553    |
|    value_loss           | 0.00474     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 245      |
|    ep_rew_mean     | 1.09     |
| time/              |          |
|    fps             | 25       |
|    iterations      | 108      |
|    time_elapsed    | 8580     |
|    total_timesteps | 221184   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 222000      |
| train/                  |             |
|    approx_kl            | 0.013961898 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.72       |
|    explained_variance   | 0.643       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0134      |
|    n_updates            | 1080        |
|    policy_gradient_loss | -0.00277    |
|    value_loss           | 0.00464     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 261      |
|    ep_rew_mean     | 1.1      |
| time/              |          |
|    fps             | 25       |
|    iterations      | 109      |
|    time_elapsed    | 8634     |
|    total_timesteps | 223232   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 224000      |
| train/                  |             |
|    approx_kl            | 0.014535216 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.63       |
|    explained_variance   | 0.396       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0215      |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.00419    |
|    value_loss           | 0.00464     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 262      |
|    ep_rew_mean     | 1.11     |
| time/              |          |
|    fps             | 25       |
|    iterations      | 110      |
|    time_elapsed    | 8688     |
|    total_timesteps | 225280   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 226000      |
| train/                  |             |
|    approx_kl            | 0.011304811 |
|    clip_fraction        | 0.0599      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.49       |
|    explained_variance   | 0.652       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00564     |
|    n_updates            | 1100        |
|    policy_gradient_loss | -0.000232   |
|    value_loss           | 0.00468     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 269      |
|    ep_rew_mean     | 1.11     |
| time/              |          |
|    fps             | 26       |
|    iterations      | 111      |
|    time_elapsed    | 8742     |
|    total_timesteps | 227328   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 228000      |
| train/                  |             |
|    approx_kl            | 0.008590108 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.39       |
|    explained_variance   | 0.562       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.011      |
|    n_updates            | 1110        |
|    policy_gradient_loss | -0.00249    |
|    value_loss           | 0.00479     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 282      |
|    ep_rew_mean     | 1.12     |
| time/              |          |
|    fps             | 26       |
|    iterations      | 112      |
|    time_elapsed    | 8797     |
|    total_timesteps | 229376   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 230000      |
| train/                  |             |
|    approx_kl            | 0.015684618 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.25       |
|    explained_variance   | 0.542       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00634     |
|    n_updates            | 1120        |
|    policy_gradient_loss | -0.00343    |
|    value_loss           | 0.00243     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 1.12     |
| time/              |          |
|    fps             | 26       |
|    iterations      | 113      |
|    time_elapsed    | 8851     |
|    total_timesteps | 231424   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 232000      |
| train/                  |             |
|    approx_kl            | 0.015162343 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.08       |
|    explained_variance   | 0.68        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00936    |
|    n_updates            | 1130        |
|    policy_gradient_loss | -0.00116    |
|    value_loss           | 0.00245     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 316      |
|    ep_rew_mean     | 1.12     |
| time/              |          |
|    fps             | 26       |
|    iterations      | 114      |
|    time_elapsed    | 8906     |
|    total_timesteps | 233472   |
---------------------------------
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+03     |
|    mean_reward          | 0.8       |
| time/                   |           |
|    total_timesteps      | 234000    |
| train/                  |           |
|    approx_kl            | 0.0212837 |
|    clip_fraction        | 0.274     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.9      |
|    explained_variance   | 0.456     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.00535   |
|    n_updates            | 1140      |
|    policy_gradient_loss | -0.00562  |
|    value_loss           | 0.00244   |
---------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 329      |
|    ep_rew_mean     | 1.12     |
| time/              |          |
|    fps             | 26       |
|    iterations      | 115      |
|    time_elapsed    | 8961     |
|    total_timesteps | 235520   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 236000      |
| train/                  |             |
|    approx_kl            | 0.009318788 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.83       |
|    explained_variance   | 0.572       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00433    |
|    n_updates            | 1150        |
|    policy_gradient_loss | -0.00423    |
|    value_loss           | 0.00238     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 237000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 347      |
|    ep_rew_mean     | 1.12     |
| time/              |          |
|    fps             | 26       |
|    iterations      | 116      |
|    time_elapsed    | 9015     |
|    total_timesteps | 237568   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 238000      |
| train/                  |             |
|    approx_kl            | 0.030245159 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.02       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0182     |
|    n_updates            | 1160        |
|    policy_gradient_loss | 0.00191     |
|    value_loss           | 1.76e-05    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 239000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 366      |
|    ep_rew_mean     | 1.12     |
| time/              |          |
|    fps             | 26       |
|    iterations      | 117      |
|    time_elapsed    | 9062     |
|    total_timesteps | 239616   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 0.014135167 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.92       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0267     |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.00408    |
|    value_loss           | 3e-05       |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 241000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 380      |
|    ep_rew_mean     | 1.13     |
| time/              |          |
|    fps             | 26       |
|    iterations      | 118      |
|    time_elapsed    | 9108     |
|    total_timesteps | 241664   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 242000      |
| train/                  |             |
|    approx_kl            | 0.025920842 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.473       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0163     |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.00428    |
|    value_loss           | 0.00252     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 243000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 399      |
|    ep_rew_mean     | 1.13     |
| time/              |          |
|    fps             | 26       |
|    iterations      | 119      |
|    time_elapsed    | 9155     |
|    total_timesteps | 243712   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 244000      |
| train/                  |             |
|    approx_kl            | 0.012207285 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.54        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0097     |
|    n_updates            | 1190        |
|    policy_gradient_loss | 0.000179    |
|    value_loss           | 0.00248     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 415      |
|    ep_rew_mean     | 1.12     |
| time/              |          |
|    fps             | 26       |
|    iterations      | 120      |
|    time_elapsed    | 9201     |
|    total_timesteps | 245760   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 246000      |
| train/                  |             |
|    approx_kl            | 0.046314754 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00116     |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.00256    |
|    value_loss           | 4.41e-06    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 247000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 431      |
|    ep_rew_mean     | 1.12     |
| time/              |          |
|    fps             | 26       |
|    iterations      | 121      |
|    time_elapsed    | 9248     |
|    total_timesteps | 247808   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 248000      |
| train/                  |             |
|    approx_kl            | 0.030886345 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2          |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.123       |
|    n_updates            | 1210        |
|    policy_gradient_loss | 0.000584    |
|    value_loss           | 2.79e-06    |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 443      |
|    ep_rew_mean     | 1.12     |
| time/              |          |
|    fps             | 26       |
|    iterations      | 122      |
|    time_elapsed    | 9295     |
|    total_timesteps | 249856   |
---------------------------------
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.011680666 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.02       |
|    explained_variance   | 0.636       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00677    |
|    n_updates            | 1220        |
|    policy_gradient_loss | 0.000232    |
|    value_loss           | 0.00252     |
-----------------------------------------
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 453      |
|    ep_rew_mean     | 1.13     |
| time/              |          |
|    fps             | 26       |
|    iterations      | 123      |
|    time_elapsed    | 9341     |
|    total_timesteps | 251904   |
---------------------------------
{'reward': [0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929], 'std': [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}
